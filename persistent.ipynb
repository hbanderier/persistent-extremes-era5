{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c2be01-ea74-4586-825d-b599b067739f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://climcal4.giub.unibe.ch:8000/user/hugo/?token=a71c03fa9afc41be95bbad3a0ba9d739\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as feat\n",
    "import xrft\n",
    "import pickle as pkl\n",
    "from scipy import constants as co\n",
    "from scipy.stats import gaussian_kde, norm\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.cluster import KMeans\n",
    "# import kmedoids\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as mpl\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.widgets import CheckButtons, Slider, Button, RadioButtons\n",
    "from matplotlib.legend_handler import HandlerBase\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap, Normalize, CenteredNorm\n",
    "import matplotlib.path as mpath\n",
    "import IPython.display as disp\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from metpy import calc as mcalc\n",
    "from metpy import interpolate as minterpolate\n",
    "from metpy.units import units\n",
    "import pandas as pd\n",
    "import time\n",
    "from ipywidgets import IntProgress, HTML\n",
    "import shutil\n",
    "from cdo import Cdo\n",
    "from nco import Nco\n",
    "import hvplot.xarray # noqa\n",
    "import panel.widgets as pnw\n",
    "import panel as pn\n",
    "from bokeh.resources import INLINE\n",
    "# pn.extension(comms=\"vscode\")\n",
    "# from definitions import *\n",
    "\n",
    "plt.rcParams['animation.ffmpeg_path'] = '/home/hugo/mambaforge-pypy3/envs/env/bin/ffmpeg'\n",
    "# cdo = Cdo()\n",
    "# nco = Nco()\n",
    "pn.extension(comms=\"default\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803b5d83",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7903ee2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Utilities, platform specifics and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013f6889",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import platform\n",
    "pf = platform.platform()\n",
    "if pf.find(\"cray\") >= 0:\n",
    "    NODE = \"daint\"\n",
    "elif platform.node()[:4] == \"clim\":\n",
    "    NODE = \"CLIM\"\n",
    "else: # find better later\n",
    "    NODE = \"UBELIX\"\n",
    "DATADIR = \"/scratch/snx3000/hbanderi/data/persistent\" if NODE == \"daint\" else \"/scratch2/hugo\"\n",
    "CLIMSTOR = \"/mnt/climstor/ecmwf/era5/raw\"\n",
    "\n",
    "def filenamescm(y, m, d):  # Naming conventions of the files on climstor (why are they so different?)\n",
    "    return [f\"{CLIMSTOR}/ML/data/{str(y)}/P{str(y)}{str(m).zfill(2)}{str(d).zfill(2)}_{str(h).zfill(2)}\" for h in range(0, 24, 6)]\n",
    "def filenamecp(y, m, d):\n",
    "    return [f\"{CLIMSTOR}/PL/data/an_pl_ERA5_{str(y)}-{str(m).zfill(2)}-{str(d).zfill(2)}.nc\"]  # returns iterable to have same call signature as filenamescl(y, m, d)\n",
    "def filenamegeneric(y, m, folder):\n",
    "    return [f\"{DATADIR}/{folder}/{y}{str(m).zfill(2)}.nc\"]\n",
    "\n",
    "def _fn(date, which):\n",
    "    if which == \"ML\":\n",
    "        return filenamescm(date.year, date.month, date.day)\n",
    "    elif which == \"PL\":\n",
    "        return filenamecp(date.year, date.month, date.day)\n",
    "    else:\n",
    "        return filenamegeneric(date.year, date.month, which)\n",
    "    \n",
    "def fn(date, which):  # instead takes pandas.timestamp (or iterable of _) as input\n",
    "    if isinstance(date, (list, np.ndarray, pd.DatedayIndex)):\n",
    "        filenames = []\n",
    "        for d in date:\n",
    "            filenames.extend(_fn(d, which))\n",
    "        return filenames\n",
    "    elif isinstance(date, pd.daystamp):\n",
    "        return _fn(date, which)\n",
    "    else:\n",
    "        raise RundayError(f\"Invalid type : {type(date)}\")\n",
    "\n",
    "RADIUS = 6.371e6  # m\n",
    "OMEGA = 7.2921e-5  # rad.s-1\n",
    "KAPPA = 0.2854\n",
    "R_SPECIFIC_AIR = 287.0500676\n",
    "\n",
    "def degcos(x):\n",
    "    return np.cos(x / 180 * np.pi)\n",
    "def degsin(x):\n",
    "    return np.sin(x / 180 * np.pi)\n",
    "\n",
    "DATERANGEPL = pd.date_range(\"19590101\", \"20211231\")\n",
    "YEARSPL = np.unique(DATERANGEPL.year)\n",
    "DATERANGEML = pd.date_range(\"19770101\", \"20211231\")\n",
    "WINDBINS = np.arange(0, 25, 2)\n",
    "LATBINS = np.arange(15, 75, 2.5)\n",
    "LONBINS = np.arange(-90, 30, 3)\n",
    "DEPBINS = np.arange(-25, 26, 1.5)\n",
    "\n",
    "COLORS5 = [     # https://coolors.co/palette/ef476f-ffd166-06d6a0-118ab2-073b4c\n",
    "    \"#ef476f\", # pinky red\n",
    "    \"#ffd166\", # yellow\n",
    "    \"#06d6a0\", # cyany green\n",
    "    \"#118ab2\", # light blue\n",
    "    \"#073b4c\", # dark blue\n",
    "]\n",
    "\n",
    "COLORS10 = [     # https://coolors.co/palette/ef476f-ffd166-06d6a0-118ab2-073b4c\n",
    "    \"#F94144\", # Vermilion\n",
    "    \"#F3722C\", # Orange\n",
    "    \"#F8961E\", # Atoll\n",
    "    \"#F9844A\", # Cadmium orange\n",
    "    \"#F9C74F\", # Caramel\n",
    "    \"#90BE6D\", # Lettuce green\n",
    "    \"#43AA8B\", # Bright Parrot Green\n",
    "    \"#4D908E\", # Abyss Green\n",
    "    \"#577590\", # Night Blue\n",
    "    \"#277DA1\", # Night Blue\n",
    "]\n",
    "\n",
    "ZOO = ['Lat', 'Int', 'Shar', 'Lats', 'Latn', 'Tilt', 'Lon', 'Lonw', 'Lone', 'Dep', \"Mea\"]\n",
    "\n",
    "COASTLINE = feat.NaturalEarthFeature(\n",
    "    \"physical\", \"coastline\", \"10m\", edgecolor=\"black\", facecolor=\"none\"\n",
    ")\n",
    "BORDERS = feat.NaturalEarthFeature(\n",
    "    \"cultural\",\n",
    "    \"admin_0_boundary_lines_land\",\n",
    "    \"10m\",\n",
    "    edgecolor=\"grey\",\n",
    "    facecolor=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9785c1b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Zoo / JLI computation and plotting functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3512fa5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cdf(timeseries):\n",
    "    idxs = np.argsort(timeseries).values\n",
    "    y = np.cumsum(idxs) / np.sum(idxs)\n",
    "    x = timeseries.values[idxs]\n",
    "    return x, y\n",
    "\n",
    "### Create histogram\n",
    "def compute_hist(timeseries, season, bins):\n",
    "    if season is not None and season != \"Annual\":\n",
    "        timeseries = timeseries.isel(time=timeseries.time.dt.season==season)\n",
    "    return np.histogram(timeseries, bins=bins)\n",
    "\n",
    "    \n",
    "def histogram(timeseries, ax, season=None, bins=LATBINS, **kwargs):\n",
    "    hist = compute_hist(timeseries, season, bins)\n",
    "    midpoints = (hist[1][1:] + hist[1][:-1]) / 2\n",
    "    bars = ax.bar(midpoints, hist[0], width=hist[1][1] - hist[1][0], **kwargs)\n",
    "    return hist\n",
    "\n",
    "\n",
    "def kde(timeseries, season=None, bins=LATBINS, scaled=False, return_x=False, **kwargs):\n",
    "    hist = compute_hist(timeseries, season, bins)\n",
    "    midpoints = (hist[1][1:] + hist[1][:-1]) / 2\n",
    "    norm = (hist[1][1] - hist[1][0]) * np.sum(hist[0])\n",
    "    kde = gaussian_kde(midpoints, weights=hist[0], **kwargs).evaluate(midpoints)\n",
    "    if scaled:\n",
    "        kde *= norm\n",
    "    if return_x:\n",
    "        return midpoints, kde\n",
    "    return kde\n",
    "\n",
    "\n",
    "def compute_anomaly(ds, return_clim=False, smooth_kmax=None):\n",
    "    # needed to workaround xarray's check with zero dimensions\n",
    "    # https://github.com/pydata/xarray/issues/3575\n",
    "    if len(ds['time']) == 0:\n",
    "        return ds\n",
    "    gb = ds.groupby(\"time.dayofyear\")\n",
    "    clim = gb.mean(dim='time')\n",
    "    if smooth_kmax:\n",
    "        ft = xrft.fft(clim, dim=\"dayofyear\")\n",
    "        ft[:int(len(ft) / 2) - smooth_kmax] = 0\n",
    "        ft[int(len(ft) / 2) + smooth_kmax:] = 0\n",
    "        clim = xrft.ifft(\n",
    "            ft, dim=\"freq_dayofyear\", true_phase=True, true_amplitude=True\n",
    "        ).real.assign_coords(dayofyear=clim.dayofyear)\n",
    "    anom = (gb - clim).reset_coords(\"dayofyear\", drop=True)\n",
    "    if return_clim:\n",
    "        return anom, clim\n",
    "    return anom\n",
    "\n",
    "\n",
    "### Lat and Int\n",
    "def compute_JLI(da_Lat):\n",
    "    LatI = da_Lat.argmax(dim=\"lat\", skipna=True)\n",
    "    Lat = xr.DataArray(da_Lat.lat[LatI.values.flatten()].values, coords={\"time\": da_Lat.time}).rename(\"Lat\")\n",
    "    Lat.attrs[\"units\"] = \"degree_north\"\n",
    "    Int = da_Lat.isel(lat=LatI).reset_coords(\"lat\", drop=True).rename(\"Int\")\n",
    "    Int.attrs[\"units\"] = \"m/s\"\n",
    "    return Lat, Int\n",
    "    \n",
    "### Shar, Latn, Lats, \n",
    "def compute_shar(da_Lat, Int, Lat):\n",
    "    Shar = (Int - da_Lat.mean(dim=\"lat\")).rename(\"Shar\")\n",
    "    Shar.attrs[\"units\"] = Int.attrs[\"units\"]\n",
    "    this = da_Lat - Shar / 2\n",
    "    ouais = np.where(this.values[:, 1:] * this.values[:, :-1] < 0)\n",
    "    hist = np.histogram(ouais[0], bins=np.arange(len(da_Lat.time) + 1))[0]\n",
    "    cumsumhist = np.append([0], np.cumsum(hist)[:-1])\n",
    "    Lats = xr.DataArray(da_Lat.lat.values[ouais[1][cumsumhist]], coords={\"time\": da_Lat.time}, name=\"Lats\")\n",
    "    Latn = xr.DataArray(da_Lat.lat.values[ouais[1][cumsumhist + hist - 1]], coords={\"time\": da_Lat.time}, name=\"Latn\")\n",
    "    Latn[Latn < Lat] = da_Lat.lat[-1]\n",
    "    Lats[Lats > Lat] = da_Lat.lat[0]\n",
    "    Latn.attrs[\"units\"] = \"degree_north\"\n",
    "    Lats.attrs[\"units\"] = \"degree_north\"\n",
    "    return Shar, Lats, Latn\n",
    "\n",
    "### Tilt\n",
    "def compute_Tilt(da, Lat):\n",
    "    trackedLats = da.isel(lat=0).copy(data=np.zeros(da.shape[::2])).reset_coords(\"lat\", drop=True).rename(\"Tracked Latitudes\")\n",
    "    trackedLats.attrs[\"units\"] = \"degree_north\"\n",
    "    lats = da.lat.values\n",
    "    twodelta = lats[2] - lats[0]\n",
    "    midpoint = int(len(da.lon) / 2)\n",
    "    trackedLats[:, midpoint] = Lat\n",
    "    iterator = zip(reversed(range(midpoint)), range(midpoint + 1, len(da.lon)))\n",
    "    for lonw, lone in iterator:\n",
    "        for k, thislon in enumerate((lonw, lone)):\n",
    "            otherlon = thislon - (2 * k - 1) # previous step in the iterator for either east (k=1, otherlon=thislon-1) or west (k=0, otherlon=thislon+1)\n",
    "            mask = np.abs(trackedLats[:, otherlon].values[:, None] - lats[None, :]) > twodelta \n",
    "            # mask = where not to look for a maximum. The next step (forward for east or backward for west) needs to be within twodelta of the previous (otherlon)\n",
    "            here = np.ma.argmax(np.ma.array(da.isel(lon=thislon).values, mask=mask), axis=1)\n",
    "            trackedLats[:, thislon] = lats[here]\n",
    "    Tilt = trackedLats.polyfit(dim=\"lon\", deg=1).sel(degree=1)[\"polyfit_coefficients\"].reset_coords(\"degree\", drop=True).rename(\"Tilt\")\n",
    "    Tilt.attrs[\"units\"] = \"degree_north/degree_east\"\n",
    "    return trackedLats, Tilt\n",
    "\n",
    "### Lon\n",
    "def compute_Lon(da, trackedLats):\n",
    "    Intlambda = da.sel(lat=trackedLats).reset_coords(\"lat\", drop=True)\n",
    "    Intlambdasq = Intlambda * Intlambda\n",
    "    lons = xr.DataArray(da.lon.values[None, :] * np.ones(len(da.time))[:, None], coords={\"time\": da.time, \"lon\": da.lon})\n",
    "    Lon = (lons * Intlambdasq).sum(dim=\"lon\") / Intlambdasq.sum(dim=\"lon\")\n",
    "    Lon.attrs[\"units\"] = \"degree_east\"\n",
    "    return Intlambda, Lon.rename(\"Lon\")\n",
    "\n",
    "### Lonw, Lone\n",
    "def compute_Lonew(da, Intlambda, Lon):\n",
    "    Intlambda = Intlambda.values\n",
    "    Mean = np.mean(Intlambda, axis=1)\n",
    "    lon = da.lon.values\n",
    "    iLon = np.argmax(lon[None, :] - Lon.values[:, None] > 0, axis=1)\n",
    "    basearray = Intlambda - Mean[:, None] < 0\n",
    "    iLonw = np.ma.argmin(np.ma.array(basearray, mask=lon[None, :] > Lon.values[:, None]), axis=1) - 1\n",
    "    iLone = np.ma.argmax(np.ma.array(basearray, mask=lon[None, :] <= Lon.values[:, None]), axis=1) - 1\n",
    "    Lonw = xr.DataArray(lon[iLonw], coords={\"time\": da.time}, name=\"Lonw\")\n",
    "    Lone = xr.DataArray(lon[iLone], coords={\"day\": da.day}, name=\"Lone\")\n",
    "    Lonw.attrs[\"units\"] = \"degree_east\"\n",
    "    Lone.attrs[\"units\"] = \"degree_east\"\n",
    "    return Lonw, Lone\n",
    "    \n",
    "### Dep\n",
    "def compute_Dep(da, trackedLats):\n",
    "    phistarl = xr.DataArray(da.lat.values[da.argmax(dim=\"lat\").values], coords={\"time\": da.time.values, \"lon\": da.lon.values})\n",
    "    Dep = np.sqrt((phistarl - trackedLats) ** 2).sum(dim=\"lon\").rename(\"Dep\")\n",
    "    Dep.attrs[\"units\"] = \"degree_north\"\n",
    "    return Dep\n",
    "\n",
    "\n",
    "def make_boundary_path(minlon,maxlon,minlat,maxlat,n=50):\n",
    "    '''\n",
    "    return a matplotlib Path whose points are a lon-lat box given by\n",
    "    the input parameters\n",
    "    '''\n",
    "\n",
    "    boundary_path = []\n",
    "    #North (E->W)\n",
    "    edge = [np.linspace(minlon,maxlon,n), np.full(n,maxlat)]\n",
    "    boundary_path += [[i,j] for i,j in zip(*edge)]\n",
    "\n",
    "    #West (N->S)\n",
    "    edge = [np.full(n,maxlon),np.linspace(maxlat,minlat,n)]\n",
    "    boundary_path += [[i,j] for i,j in zip(*edge)]\n",
    "\n",
    "    #South (W->E)\n",
    "    edge = [np.linspace(maxlon,minlon,n), np.full(n,minlat)]\n",
    "    boundary_path += [[i,j] for i,j in zip(*edge)]\n",
    "\n",
    "    #East (S->N)\n",
    "    edge = [np.full(n,minlon),np.linspace(minlat,maxlat,n)]\n",
    "    boundary_path += [[i,j] for i,j in zip(*edge)]\n",
    "\n",
    "    boundary_path = mpath.Path(boundary_path)\n",
    "\n",
    "    return boundary_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5059eb95",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Barriopedro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03271c75-aefb-4a8b-a4fb-30434ae47c52",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6910ccb-3f07-4531-9d46-c8a134e972b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"ERA5\" # dataset = \"NCEP\"\n",
    "datadir = f\"{DATADIR}/{dataset}/packaged/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385fd3d4-fae6-4cbc-be91-e9585aed8536",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = xr.open_dataset(f\"{datadir}/BarriopedroRaw.nc\")[\"u\"]\n",
    "if dataset == \"ERA5\":\n",
    "    da = da.rename({\"longitude\": \"lon\", \"latitude\": \"lat\"})\n",
    "da2 = da.rolling(lon=60 if dataset == \"ERA5\" else 24, center=True).mean().sel(lon=np.arange(-60, 0.1, 0.5))\n",
    "da_fft = xrft.fft(da2, dim=\"time\")\n",
    "da_fft[np.abs(da_fft.freq_time) > 1 / 10 / 24 / 3600] = 0\n",
    "da3 = xrft.ifft(da_fft, dim=\"freq_time\", true_phase=True, true_amplitude=True).real.assign_coords(time=da.time).rename(\"u\")\n",
    "da2.attrs[\"unit\"] = \"m/s\"\n",
    "da3.attrs[\"unit\"] = \"m/s\"\n",
    "da3.to_netcdf(f\"{datadir}/Barriopedro.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0909052",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4306588e-f8ae-47cf-b686-6ecbda576c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = xr.open_dataset(f\"{datadir}/Barriopedro.nc\")[\"u\"]\n",
    "da_Lat = da.sel(lon=-30.).reset_coords(\"lon\", drop=True)\n",
    "Lat, Int, Latmean, Latsmooth = compute_JLI(da_Lat)\n",
    "Shar, Lats, Latn = compute_shar(da_Lat, Int, Lat)\n",
    "trackedLats, Tilt = compute_Tilt(da, Lat)\n",
    "Intlambda, Lon = compute_Lon(da, trackedLats)\n",
    "Lonw, Lone = compute_Lonew(da, Intlambda, Lon)\n",
    "Dep = compute_Dep(da, trackedLats)\n",
    "Zoo = xr.Dataset({\n",
    "    \"Lat\": Lat, \n",
    "    \"Int\" : Int, \n",
    "    \"Shar\": Shar, \n",
    "    \"Lats\": Lats, \n",
    "    \"Latn\" : Latn, \n",
    "    \"Tilt\" : Tilt, \n",
    "    \"Lon\" : Lon, \n",
    "    \"Lonw\" : Lonw, \n",
    "    \"Lone\" : Lone, \n",
    "    \"Dep\" : Dep,\n",
    "})\n",
    "Zoo.to_netcdf(f\"{DATADIR}/{dataset}/processed/BarriopedroZoo.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a127f4bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260405c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# timeseries, ax, season=None, bins=LATBINS, **kwargs\n",
    "dataset = \"ERA5\"\n",
    "Zoo = xr.open_dataset(f\"{DATADIR}/{dataset}/processed/BarriopedroZooDetrended.nc\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 14))\n",
    "axes = axes.flatten()\n",
    "mapping = [[\"Int\", \"Shar\"], [\"Lat\", \"Lats\", \"Latn\"], [\"Lon\", \"Lone\", \"Lonw\"], [\"Tilt\", \"Dep\"]]\n",
    "bins = [WINDBINS, LATBINS, LONBINS, DEPBINS]\n",
    "for i, group in enumerate(mapping):\n",
    "    ax = axes[i]\n",
    "    for j, key in enumerate(group):\n",
    "        midpoints, gkde = kde(Zoo[key], \"DJF\", bins[i], scaled=True, return_x=True)\n",
    "        ax.plot(\n",
    "            midpoints, gkde, color=COLORS5[j], label=key\n",
    "        )\n",
    "    ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84eec78-1df3-4672-8a93-49cf0a2fac22",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Meandering Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53737e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"ERA5\"\n",
    "da = xr.open_mfdataset(\n",
    "    f\"{DATADIR}/{dataset}/Geopotential/dailymean/*.nc\"\n",
    ").rename(\n",
    "    {\"longitude\": \"lon\", \"latitude\": \"lat\"}\n",
    ").isel(lat=np.arange(180, 361))[\"z\"].load() / co.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91a4da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contourpy\n",
    "from joblib import delayed, Parallel, dump, load\n",
    "\n",
    "def meandering(lines):\n",
    "    m = 0\n",
    "    for line in lines:\n",
    "        m += np.sum(np.sqrt(np.sum(np.diff(line, axis=0)**2, axis=1))) / 360\n",
    "    return m\n",
    "\n",
    "def one_ts(lon, lat, da):\n",
    "    m = []\n",
    "    gen = contourpy.contour_generator(x=lon, y=lat, z=da)\n",
    "    for lev in range(4900, 6205, 5):\n",
    "        m.append(meandering(gen.lines(lev)))\n",
    "    return np.amax(m)\n",
    "\n",
    "lon = da.lon.values\n",
    "lat = da.lat.values\n",
    "M = Parallel(\n",
    "    n_jobs=32, backend=\"loky\", max_nbytes=1e5\n",
    ")(\n",
    "    delayed(one_ts)(lon, lat, da.sel(time=t).values) for t in da.time[:]\n",
    ")\n",
    "daM = xr.DataArray(M, coords={\"time\":da.time})\n",
    "daM.to_netcdf(f\"{DATADIR}/{dataset}/processed/Meandering.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc0f72b-f760-4baa-bfd5-37827cdb2150",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plot like Dicapua et al. 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf2b11d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "daM = xr.open_dataarray(f\"{DATADIR}/{dataset}/processed/Meandering.nc\")\n",
    "daM_anom = compute_anomaly(daM)\n",
    "fig, axes = plt.subplots(1, 5, figsize=(18, 5), sharey=True)\n",
    "early = daM.isel(time=(daM.time.dt.year < 2000))\n",
    "recent = daM.isel(time=(daM.time.dt.year >= 2000))\n",
    "fig.subplots_adjust(wspace=0.04)\n",
    "for i, season in enumerate([\"DJF\", \"MAM\", \"JJA\", \"SON\", \"Annual\"]):\n",
    "    \n",
    "    kde_early = kde(early, season, 50, scaled=False, return_x=True)\n",
    "    kde_recent = kde(recent, season, 50, scaled=False, return_x=True)\n",
    "    axes[i].plot(*kde_early, label=\"pre 2000\", color=COLORS5[3], lw=2.5)\n",
    "    axes[i].plot(*kde_recent, label=\"post 2000\", color=COLORS5[0], lw=2.5)\n",
    "    axes[i].fill_between(kde_early[0], 0, kde_early[1], color=COLORS5[3], alpha=0.5)\n",
    "    axes[i].fill_between(kde_recent[0], 0, kde_recent[1], color=COLORS5[0], alpha=0.5)\n",
    "    axes[i].set_title(season)\n",
    "    axes[i].set_xlabel(\"Meandering Index\")\n",
    "    axes[i].set_yticks([0, 0.5, 1, 1.5, 2.0])\n",
    "    axes[i].set_xlim([1, 3.3])\n",
    "    axes[i].set_ylim([0, 2])\n",
    "axes[0].set_ylabel(\"Density\")\n",
    "axes[-1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cd3f4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Combine, detrend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6cfb86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# timeseries, ax, season=None, bins=LATBINS, **kwargs\n",
    "dataset = \"ERA5\"\n",
    "Zoo = xr.open_dataset(f\"{DATADIR}/{dataset}/processed/BarriopedroZoo.nc\")\n",
    "daM = xr.open_dataarray(f\"{DATADIR}/{dataset}/processed/Meandering.nc\")\n",
    "Zoo[\"Mea\"] = daM\n",
    "for key, value in Zoo.data_vars.items():\n",
    "    noseason, Zoo[f\"{key}_climatology\"] = compute_anomaly(value, return_clim=True, smooth_kmax=3)\n",
    "    Zoo[f\"{key}_anomaly\"] = xrft.detrend(noseason, dim=\"time\", detrend_type=\"linear\")\n",
    "Zoo.to_netcdf(f\"{DATADIR}/{dataset}/processed/BarriopedroZooDetrended.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e734435e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Zoo[\"Lat_anomaly\"].plot.hist(bins=np.arange(-30,31,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f030f183-fa24-4175-b5c7-7e3c18fefdac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# EDG (see Barriopedro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263e98a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Create EDG filtered datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbbce96-7dcd-4b25-974f-499777e2b268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa932f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EDG\n",
    "ds_EDG = xr.open_dataset(f\"{datadir}/EDG.nc\").isel(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c91b3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for varname in [\"u\", \"v\"]:\n",
    "    da_fft_bgrnd = xrft.fft(ds_EDG[f\"{varname}wnd\"], dim=\"time\")\n",
    "    da_fft_trans = da_fft_bgrnd.copy()\n",
    "    freq = np.abs(da_fft.freq_time)\n",
    "    da_fft_bgrnd[freq > 1 / 10 / 24 / 3600] = 0\n",
    "    da_fft_trans[np.logical_or(freq > 1 / 2 / 24 / 3600, freq < 1 / 6 / 24 / 3600)] = 0\n",
    "    ds_EDG[f\"{varname}bgrnd\"] = xrft.ifft(da_fft_bgrnd, dim=\"freq_time\", true_phase=True, true_amplitude=True).real.assign_coords(time=ds_EDG.time).rename(f\"{varname}bgrnd\")\n",
    "    ds_EDG[f\"{varname}trans\"] = xrft.ifft(da_fft_trans, dim=\"freq_time\", true_phase=True, true_amplitude=True).real.assign_coords(time=ds_EDG.time).rename(f\"{varname}trans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66ef0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_EDG.to_netcdf(f\"{datadir}/EDG3.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aa962f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### EDG Computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3913d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_EDG = xr.open_dataset(f\"{DATADIR}/EDG3.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea2595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_EDG[\"E1\"] = (ds_EDG[\"vtrans\"] ** 2 - ds_EDG[\"utrans\"] ** 2) / 2\n",
    "ds_EDG[\"E2\"] = - ds_EDG[\"utrans\"] * ds_EDG[\"vtrans\"]\n",
    "### D vector in spherical coordinates, see Obsidian page for this\n",
    "ds_EDG[\"D1\"] = ds_EDG[\"ubgrnd\"].differentiate(\"lon\") / RADIUS \\\n",
    "               - 1 / degsin(ds_EDG[\"lat\"]) / RADIUS * ds_EDG[\"vbgrnd\"].differentiate(\"lat\") \\\n",
    "               - ds_EDG[\"ubgrnd\"] * degcos(ds_EDG[\"lat\"]) / degsin(ds_EDG[\"lat\"]) / RADIUS\n",
    "ds_EDG[\"D2\"] = 0.5 * (degsin(ds_EDG[\"lat\"]) / RADIUS * (ds_EDG[\"vbgrnd\"] / degsin(ds_EDG[\"lat\"])).differentiate(\"lon\") \\\n",
    "                    + 1 / degsin(ds_EDG[\"lat\"]) / RADIUS * ds_EDG[\"ubgrnd\"].differentiate(\"lat\"))\n",
    "### Generation rate\n",
    "ds_EDG[\"G\"] = ds_EDG[\"E1\"] * ds_EDG[\"D1\"] + ds_EDG[\"E2\"] * ds_EDG[\"D2\"]\n",
    "### while we're at it, let's compute the vorticity\n",
    "ds_EDG[\"omega\"] = 1 / RADIUS / degcos(ds_EDG[\"lat\"]) * (ds_EDG[\"vwnd\"].differentiate(\"lon\") - \\\n",
    "                    (ds_EDG[\"uwnd\"] * degcos(ds_EDG[\"lat\"])).differentiate(\"lat\"))\n",
    "ds_EDG[\"EKE\"] = 0.5 * (ds_EDG[\"utrans\"] ** 2 + ds_EDG[\"vtrans\"] ** 2)\n",
    "for key in ds_EDG.data_vars:\n",
    "    ds_EDG[key].to_netcdf(f\"{DATADIR}/NCEP/processed/{key}.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47a72c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ds_EDG.data_vars:\n",
    "    ds_EDG[key].to_netcdf(f\"{DATADIR}/NCEP/processed/{key}.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f859da9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcb653f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Zoo Autocorrelation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b09db8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "howmany = 50\n",
    "f1 = IntProgress(value=0, max=len(Zoo.data_vars))\n",
    "f2 = IntProgress(value=0, max=howmany)\n",
    "display(f1, f2)\n",
    "autocorrs = {}\n",
    "\n",
    "for i, varname in enumerate(Zoo):\n",
    "    if varname.split('_')[-1] == \"climatology\":\n",
    "        continue\n",
    "    f2.value = 0\n",
    "    autocorrs[varname] = (\"lag\", np.empty(howmany))\n",
    "    for j in range(howmany):\n",
    "        autocorrs[varname][1][j] = xr.corr(Zoo[varname], Zoo[varname].shift(time=j)).values\n",
    "        f2.value = j + 1\n",
    "    f1.value = i + 1\n",
    "autocorrsda = xr.Dataset(autocorrs, coords={\"lag\": np.arange(howmany)})\n",
    "autocorrsda.to_netcdf(f\"{DATADIR}/{dataset}/processed/Zoo_autocorrs.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e234d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 4, figsize=(20, 16), tight_layout=True)\n",
    "axes = axes.flatten(order=\"F\")\n",
    "dataset = \"ERA5\"\n",
    "datadir = f\"{DATADIR}/{dataset}/processed\" # daint\n",
    "autocorrs = xr.open_dataset(f\"{datadir}/Zoo_autocorrs.nc\")\n",
    "howmany = len(autocorrs.coords[\"lag\"])\n",
    "newlist = []\n",
    "for key in list(Zoo.data_vars.keys())[:11]:\n",
    "    for suffix in [\"_anomaly\"]:\n",
    "        newlist.append(f\"{key}{suffix}\")\n",
    "telatex = r\"$T^e_{\\rho}$\"\n",
    "tdlatex = r\"$T^d_{\\rho}$\"\n",
    "tclatex = r\"$T^c_{\\rho}$\"\n",
    "lw = 2\n",
    "for i, varname in enumerate(newlist):\n",
    "    te = np.argmax(autocorrs[varname].values <= 1 / np.exp(1))\n",
    "    td = 1 + 2 * np.sum(autocorrs[varname])\n",
    "    tc = 1 + np.sum(autocorrs[varname] * (1 - np.arange(1, howmany + 1) / (howmany + 1)))\n",
    "    axes[i].plot(np.arange(howmany), autocorrs[varname], color=COLORS5[0], lw=lw)\n",
    "    axes[i].plot([te, te], [0, 1], label=telatex, color=COLORS5[2], lw=lw)\n",
    "    axes[i].plot([tc, tc], [0, 1], label=tclatex, color=COLORS5[3], lw=lw)\n",
    "    axes[i].plot([td, td], [0, 1], label=tdlatex, color=COLORS5[4], lw=lw)\n",
    "    axes[i].grid()\n",
    "    axes[i].legend()\n",
    "    axes[i].set_title(f\"{varname}, {telatex}={te}, {tdlatex}={td:.3f}, {tclatex}={tc:.3f}\")\n",
    "    axes[i].set_ylabel(\"Autocorrelation\")\n",
    "    axes[i].set_xlabel(\"Lag time [days]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e649b4ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###  Zoo Hurst exponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366e7c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "subdivs = [2**n for n in range(11)]\n",
    "lengths = [len(Zoo.time) // n for n in subdivs]\n",
    "all_lengths = np.repeat(lengths, subdivs)\n",
    "N_chunks = np.sum(subdivs)\n",
    "Hurst = {}\n",
    "for i, varname in enumerate(Zoo.data_vars):\n",
    "    adjusted_ranges = []\n",
    "    for n_chunks, n in zip(subdivs, lengths):\n",
    "        start = 0\n",
    "        aranges = []\n",
    "        for k in range(n_chunks):\n",
    "            end = start + n\n",
    "            series = Zoo[varname].isel(time=np.arange(start, end)).values\n",
    "            mean = np.mean(series)\n",
    "            std = np.std(series)\n",
    "            series -= mean\n",
    "            series = np.cumsum(series)\n",
    "            raw_range = series.max() - series.min()\n",
    "            aranges.append(raw_range / std)\n",
    "        adjusted_ranges.append(np.mean(aranges))\n",
    "    print(varname)\n",
    "    ax.loglog(lengths, adjusted_ranges, color=COLORS10[i % 10])\n",
    "    coeffs = np.polyfit(np.log(lengths), np.log(adjusted_ranges), deg=1)\n",
    "    Hurst[varname] = [coeffs[0], np.exp(coeffs[1])]\n",
    "    ax.loglog(lengths, np.exp(coeffs[1]) * lengths ** coeffs[0], color=COLORS10[i % 10])\n",
    "with open(f\"{datadir}/Hurst_ERA5.pkl\", \"wb\") as handle:\n",
    "    pkl.dump(Hurst, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b7a5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{DATADIR}/ERA5/processed/Hurst.pkl\", \"rb\") as handle:\n",
    "    Hurst = pkl.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501a2709",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hurst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba74dfe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f748df38",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Z500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f4a5a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(\n",
    "    f\"{DATADIR}/ERA5/Geopotential/north_atlantic/full.nc\"\n",
    ").rename(\n",
    "    {\"longitude\": \"lon\", \"latitude\": \"lat\"}\n",
    ").isel(lon=np.arange(241), lat=np.arange(30, 131))\n",
    "ds[\"z\"] /= co.g\n",
    "da = ds[\"z\"].chunk({\"time\": -1, \"lon\": 121})\n",
    "anomaly = xr.map_blocks(compute_anomaly, da, template=da)\n",
    "detrended = xr.map_blocks(xrft.detrend, anomaly, args=(\"time\", \"linear\"), template=da)\n",
    "anomaly.to_netcdf(f\"{DATADIR}/ERA5/Geopotential/north_atlantic/anomaly.nc\")\n",
    "detrended.to_netcdf(f\"{DATADIR}/ERA5/Geopotential/north_atlantic/detrended.nc\")\n",
    "# ds = xr.open_dataset(\n",
    "#     f\"{DATADIR}/NCEP/packaged/Z500NA.nc\"\n",
    "# ).rename({\"hgt\": \"z\"}).reset_index(\"level\", drop=True).squeeze().isel(lat=np.arange(4, 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716f7b7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_clu = 4\n",
    "thisda = xr.open_dataarray(f\"{DATADIR}/ERA5/Geopotential/north_atlantic/detrended.nc\")\n",
    "tbt = (thisda * degcos(thisda.lat)).values.reshape(len(thisda.time), -1)\n",
    "results = KMeans(n_clu, n_init=\"auto\").fit(tbt)\n",
    "with open(\"kmeans_ERA5_detrended.pkl\", \"wb\") as handle:\n",
    "    pkl.dump(results, handle)\n",
    "# distmatrix = euclidean_distances(tbt)\n",
    "# results = kmedoids.fasterpam(distmatrix, n_clu)\n",
    "# centers = thisda.isel(time=results.medoids).rename({\"time\": \"cluster\"}).assign_coords({\"cluster\": np.arange(n_clu)}).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01212676",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"kmeans_ERA5_detrended.pkl\", \"rb\") as handle:\n",
    "    results = pkl.load(handle)\n",
    "thisda = xr.open_dataarray(f\"{DATADIR}/ERA5/Geopotential/north_atlantic/detrended.nc\")\n",
    "centers = xr.DataArray(\n",
    "    results.cluster_centers_.reshape(n_clu, *thisda.shape[1:]), \n",
    "    coords={\"cluster\": np.arange(n_clu), \"lat\": ds.lat.values, \"lon\": ds.lon.values},\n",
    ") / degcos(thisda.lat)\n",
    "projection = ccrs.LambertConformal(\n",
    "    central_longitude=np.mean(da.lon.values),\n",
    ")\n",
    "lon = ds[\"lon\"].values\n",
    "lat = ds[\"lat\"].values\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 7.5), subplot_kw={\"projection\": projection}, constrained_layout=True)\n",
    "extent = [np.amin(lon), np.amax(lon), np.amin(lat), np.amax(lat)]\n",
    "boundary = make_boundary_path(*extent)\n",
    "levels = 11\n",
    "cmap = \"seismic\"\n",
    "lower, upper = -150, 150\n",
    "levels = np.delete(np.append(np.linspace(lower, 0, levels), np.linspace(0, upper, levels)), [levels - 1, levels])\n",
    "cmap = cm.get_cmap(cmap)\n",
    "norm = mpl.colors.BoundaryNorm(levels, cmap.N, extend='both')\n",
    "im = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "axes = axes.flatten()\n",
    "for i in range(n_clu):\n",
    "    axes[i].contourf(\n",
    "        lon,\n",
    "        lat,\n",
    "        centers.isel(cluster=i), \n",
    "        transform=ccrs.PlateCarree(),\n",
    "        levels=levels, cmap=cmap, norm=norm,\n",
    "        extend=\"both\",\n",
    "    )\n",
    "    axes[i].contour(\n",
    "        lon,\n",
    "        lat,\n",
    "        centers.isel(cluster=i), \n",
    "        transform=ccrs.PlateCarree(),\n",
    "        levels=levels, colors=\"k\",\n",
    "    )\n",
    "    axes[i].set_boundary(boundary, transform=ccrs.PlateCarree())\n",
    "    axes[i].add_feature(COASTLINE)\n",
    "    axes[i].add_feature(BORDERS)\n",
    "    axes[i].set_title(f\"Regime {i + 1}, {np.sum(results.labels_ == i) / len(results.labels_) * 100:.2f}%\")\n",
    "cbar = fig.colorbar(im, ax=axes.ravel().tolist(), spacing=\"proportional\")\n",
    "cbar.ax.set_ylabel(\"Z500 [m]\")\n",
    "_ = cbar.ax.set_yticks(np.concatenate([np.arange(-150, 20, 30), np.arange(30, 151, 30)]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d8a4d4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Predict hot spells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bed2f67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"kmeans_ERA5_detrended.pkl\", \"rb\") as handle:\n",
    "    results = pkl.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f593c0c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_of_dates = np.loadtxt(\"hotspells.csv\", delimiter=\",\", dtype=np.datetime64)\n",
    "hotspells_clusters = {}\n",
    "keys = [\"South\", \"West\", \"Balkans\", \"Scandinavia\", \"Russia\", \"Arctic\"]\n",
    "detrended = detrended.compute()\n",
    "minus = 21\n",
    "plus = 5\n",
    "for j, key in enumerate(keys):\n",
    "    dates = np.sort(list_of_dates[:, j])\n",
    "    dates = dates[~(np.isnat(dates) | (np.datetime_as_string(dates, unit=\"Y\") == \"2022\"))]\n",
    "    hotspells_clusters[key] = []\n",
    "    for i, date in enumerate(dates):\n",
    "        tsta = date - np.timedelta64(minus, \"D\") + np.timedelta64(9, \"h\")\n",
    "        tend = date + np.timedelta64(plus, \"D\") + np.timedelta64(9, \"h\")\n",
    "        to_predict = detrended.sel(time=pd.date_range(tsta, tend, freq=\"1D\"))\n",
    "        to_predict = to_predict.values.reshape(len(to_predict.time), -1)\n",
    "        hotspells_clusters[key].append(results.predict(to_predict))\n",
    "    hotspells_clusters[key] = xr.DataArray(\n",
    "        np.stack(hotspells_clusters[key]).transpose(), \n",
    "        coords={\"time\": np.arange(-minus, plus + 1), \"hotspell\": dates}\n",
    "    )\n",
    "with open(\"hotspells_clusters.pkl\", \"wb\") as handle:\n",
    "    pkl.dump(hotspells_clusters, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dc126d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "with open(\"hotspells_clusters.pkl\", \"rb\") as handle:\n",
    "    hotspells_clusters = pkl.load(handle)\n",
    "hot_time = hotspells_clusters[\"South\"].time.values\n",
    "to_plot = np.zeros((n_clu // 2, len(keys), 2 * len(hot_time)))\n",
    "cmaps = [\"Blues\", \"Greens\", \"Reds\", \"Purples\"]\n",
    "for i in range(4):\n",
    "    for j, (key, value) in enumerate(hotspells_clusters.items()):\n",
    "        to_plot[int(i <= 1), j, (i % 2)::2] = i + (value == i).mean(dim=\"hotspell\").values\n",
    "class HandlerColormap(HandlerBase): # https://stackoverflow.com/questions/55501860/how-to-put-multiple-colormap-patches-in-a-matplotlib-legend\n",
    "    def __init__(self, cmap, num_stripes=8, **kw):\n",
    "        HandlerBase.__init__(self, **kw)\n",
    "        self.cmap = cmap\n",
    "        self.num_stripes = num_stripes\n",
    "    def create_artists(self, legend, orig_handle, xdescent, ydescent, width, height, fontsize, trans):\n",
    "        stripes = []\n",
    "        for i in range(self.num_stripes):\n",
    "            s = Rectangle(\n",
    "                [xdescent + i * width / self.num_stripes, ydescent], \n",
    "                width / self.num_stripes, \n",
    "                height, \n",
    "                fc=self.cmap((2 * i + 1) / (2 * self.num_stripes)), \n",
    "                transform=trans,\n",
    "            )\n",
    "            stripes.append(s)\n",
    "        return stripes\n",
    "\n",
    "cmaps = [mpl.colormaps[cmap].resampled(256) for cmap in [\"Blues\", \"Greens\", \"Reds\", \"Purples\"]]\n",
    "cmap = ListedColormap(np.concatenate([cmap(np.linspace(0, 1, 256)) for cmap in cmaps]))\n",
    "\n",
    "fig, axes = plt.subplots(len(keys), 1, figsize=(15, 5))\n",
    "norm = Normalize(0, len(cmaps))\n",
    "for j, ax in enumerate(axes):\n",
    "    ax.set_yticks([1])\n",
    "    ax.set_yticklabels([list(hotspells_clusters.keys())[j]])\n",
    "    if j==len(axes) - 1:\n",
    "        ax.set_xticks(np.arange(hot_time[0] + 0.5, hot_time[-1] + 1.5))\n",
    "        ax.set_xticklabels(np.arange(hot_time[0], hot_time[-1] + 1))\n",
    "        ax.set_xlabel(\"Days around center\")\n",
    "    else:\n",
    "        ax.set_xticks([])\n",
    "    ax.set_frame_on(False)\n",
    "    for i in range(2):\n",
    "        ax.pcolormesh(\n",
    "            np.arange(hot_time[0], hot_time[-1] + 1.1, .5), \n",
    "            np.arange(3), to_plot[:, j].reshape(2, len(hot_time) * 2), \n",
    "            cmap=cmap, norm=norm\n",
    "        )\n",
    "    ax.vlines(np.arange(hot_time[0] + 1, hot_time[-1] + 1), 0, 2, color=\"white\", lw=4)\n",
    "cmap_handles = [Rectangle((0, 0), 3, 1) for _ in cmaps]\n",
    "handler_map = dict(zip(cmap_handles, [HandlerColormap(cm, num_stripes=20) for cm in cmaps]))\n",
    "cmap_labels = [f\"regime {k + 1}\" for k in range(4)]\n",
    "axes[-1].legend(\n",
    "    handles=cmap_handles, \n",
    "    labels=cmap_labels, \n",
    "    handler_map=handler_map, \n",
    "    fontsize=12,\n",
    "    bbox_to_anchor=(1.13, 4.2),\n",
    "    loc=\"upper right\",\n",
    ")\n",
    "plt.subplots_adjust(hspace=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e819982",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "with open(\"hotspells_clusters.pkl\", \"rb\") as handle:\n",
    "    hotspells_clusters = pkl.load(handle)\n",
    "with open(\"kmeans_ERA5_detrended.pkl\", \"rb\") as handle:\n",
    "    results = pkl.load(handle)\n",
    "n_clu = 4\n",
    "abs_freq = [np.mean(results.labels_ == i) for i in range(n_clu)]\n",
    "hot_time = hotspells_clusters[\"South\"].time.values\n",
    "to_plot = np.zeros((n_clu, len(hotspells_clusters), len(hot_time)))\n",
    "cmaps = [\"Blues\", \"Greens\", \"Reds\", \"Purples\"]\n",
    "for i in range(n_clu):\n",
    "    for j, (key, value) in enumerate(hotspells_clusters.items()):\n",
    "        to_plot[i, j] = abs_freq[i] - (value == i).mean(dim=\"hotspell\").values\n",
    "\n",
    "fig, axes = plt.subplots(len(hotspells_clusters), 1, figsize=(15, 8), sharex=True)\n",
    "fig.subplots_adjust(hspace=0, wspace=0, left=0.06)\n",
    "for j, key in enumerate(hotspells_clusters):\n",
    "    ax = axes[j]\n",
    "#     ax.spines[[\"left\", \"right\"]].set_visible(False)\n",
    "    for i in range(n_clu):\n",
    "        ax.plot(hot_time, to_plot[i, j], lw=2, color=COLORS5[i], label=f\"Regime {i + 1}\")\n",
    "    ax.set_ylim([-0.5, 0.5])\n",
    "    ax.set_yticks([-0.25, 0, 0.25])\n",
    "    ax.set_yticklabels([-25, 0, 25])\n",
    "    ax.text(-20.8, 0.21, key, fontweight=\"bold\")\n",
    "    ax.grid()\n",
    "fig.supylabel('Regime relative occurence [%]')\n",
    "ax.set_xlabel(\"Time around center [Days]\")\n",
    "ax.set_xlim([-21, 5])\n",
    "ax.legend(bbox_to_anchor=(1.12, 3.53),)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a053d1dc",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97521897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA as pca\n",
    "n_components = 20\n",
    "thisda = xr.open_dataarray(f\"{DATADIR}/ERA5/Geopotential/north_atlantic/detrended.nc\")\n",
    "tbt = (thisda * np.sqrt(degcos(thisda.lat))).values.reshape(len(thisda.time), -1)\n",
    "pca_results = pca(n_components=n_components, whiten=True).fit(tbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289471f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = xr.DataArray(\n",
    "    pca_results.components_.reshape(n_components, *thisda.shape[1:]), \n",
    "    coords={\"component\": np.arange(n_components), \"lat\": ds.lat.values, \"lon\": ds.lon.values},\n",
    ") / np.sqrt(degcos(thisda.lat))\n",
    "projection = ccrs.LambertConformal(\n",
    "    central_longitude=np.mean(da.lon.values),\n",
    ")\n",
    "lon = ds[\"lon\"].values\n",
    "lat = ds[\"lat\"].values\n",
    "fig, axes = plt.subplots(5, 4, figsize=(20, 25), subplot_kw={\"projection\": projection}, constrained_layout=True)\n",
    "extent = [np.amin(lon), np.amax(lon), np.amin(lat), np.amax(lat)]\n",
    "boundary = make_boundary_path(*extent)\n",
    "levels = 10\n",
    "cmap = \"seismic\"\n",
    "lower, upper = -0.045, 0.045\n",
    "levels = np.delete(np.append(np.linspace(lower, 0, levels), np.linspace(0, upper, levels)), [levels - 1, levels])\n",
    "cmap = cm.get_cmap(cmap)\n",
    "norm = mpl.colors.BoundaryNorm(levels, cmap.N, extend='both')\n",
    "im = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "axes = axes.flatten()\n",
    "for i in range(n_components):\n",
    "    axes[i].contourf(\n",
    "        lon,\n",
    "        lat,\n",
    "        centers.isel(component=i), \n",
    "        transform=ccrs.PlateCarree(),\n",
    "        levels=levels, \n",
    "        cmap=cmap, \n",
    "        norm=norm,\n",
    "        extend=\"both\",\n",
    "    )\n",
    "    axes[i].contour(\n",
    "        lon,\n",
    "        lat,\n",
    "        centers.isel(component=i), \n",
    "        transform=ccrs.PlateCarree(),\n",
    "        levels=levels, \n",
    "        colors=\"k\",\n",
    "    )\n",
    "    axes[i].set_boundary(boundary, transform=ccrs.PlateCarree())\n",
    "    axes[i].add_feature(COASTLINE)\n",
    "    axes[i].add_feature(BORDERS)\n",
    "    axes[i].set_title(f\"{pca_results.explained_variance_ratio_[i] * 100:.2f} %\")\n",
    "cbar = fig.colorbar(im, ax=axes.ravel().tolist(), spacing=\"proportional\")\n",
    "cbar.ax.set_ylabel(\"Z500 [m]\")\n",
    "# _ = cbar.ax.set_yticks(np.concatenate([np.arange(-0.045, -0.004, 0.005), np.arange(0.005, 0.046, 0.005)]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1196389",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pca_ERA5_detrended.pkl\", \"wb\") as handle:\n",
    "    pkl.dump(pca_results, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0056dc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced = pca_results.transform(tbt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e63918f",
   "metadata": {},
   "source": [
    "# K means on EOFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d88a2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_kmeans_results = KMeans(n_clu, n_init=\"auto\").fit(reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bab357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"kmeans_ERA5_detrended.pkl\", \"rb\") as handle:\n",
    "#     results = pkl.load(handle)\n",
    "thisda = xr.open_dataarray(f\"{DATADIR}/ERA5/Geopotential/north_atlantic/detrended.nc\")\n",
    "centers = xr.DataArray(\n",
    "    pca_results.inverse_transform(pca_kmeans_results.cluster_centers_).reshape(n_clu, *thisda.shape[1:]), \n",
    "    coords={\"cluster\": np.arange(n_clu), \"lat\": ds.lat.values, \"lon\": ds.lon.values},\n",
    ") / np.sqrt(degcos(thisda.lat))\n",
    "projection = ccrs.LambertConformal(\n",
    "    central_longitude=np.mean(da.lon.values),\n",
    ")\n",
    "lon = ds[\"lon\"].values\n",
    "lat = ds[\"lat\"].values\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 7.5), subplot_kw={\"projection\": projection}, constrained_layout=True)\n",
    "extent = [np.amin(lon), np.amax(lon), np.amin(lat), np.amax(lat)]\n",
    "boundary = make_boundary_path(*extent)\n",
    "levels = 11\n",
    "cmap = \"seismic\"\n",
    "lower, upper = -150, 150\n",
    "levels = np.delete(np.append(np.linspace(lower, 0, levels), np.linspace(0, upper, levels)), [levels - 1, levels])\n",
    "cmap = cm.get_cmap(cmap)\n",
    "norm = mpl.colors.BoundaryNorm(levels, cmap.N, extend='both')\n",
    "im = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "axes = axes.flatten()\n",
    "for i in range(n_clu):\n",
    "    axes[i].contourf(\n",
    "        lon,\n",
    "        lat,\n",
    "        centers.isel(cluster=i), \n",
    "        transform=ccrs.PlateCarree(),\n",
    "        levels=levels, cmap=cmap, norm=norm,\n",
    "        extend=\"both\",\n",
    "    )\n",
    "    axes[i].contour(\n",
    "        lon,\n",
    "        lat,\n",
    "        centers.isel(cluster=i), \n",
    "        transform=ccrs.PlateCarree(),\n",
    "        levels=levels, colors=\"k\",\n",
    "    )\n",
    "    axes[i].set_boundary(boundary, transform=ccrs.PlateCarree())\n",
    "    axes[i].add_feature(COASTLINE)\n",
    "    axes[i].add_feature(BORDERS)\n",
    "    axes[i].set_title(f\"Regime {i + 1}, {np.sum(results.labels_ == i) / len(results.labels_) * 100:.2f}%\")\n",
    "cbar = fig.colorbar(im, ax=axes.ravel().tolist(), spacing=\"proportional\")\n",
    "cbar.ax.set_ylabel(\"Z500 [m]\")\n",
    "_ = cbar.ax.set_yticks(np.concatenate([np.arange(-150, 20, 30), np.arange(30, 151, 30)]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e12fe38",
   "metadata": {},
   "source": [
    "# OPP from EOFs (optimal $T_1$, TODO $T_2$) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ef5f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_reduced = xr.DataArray(reduced, coords={\"time\": thisda.time.values, \"projection\": np.arange(n_components)})\n",
    "\n",
    "lag_max = 15 # days\n",
    "autocorrs = []\n",
    "for j in range(lag_max):\n",
    "    autocorrs.append(np.cov(da_reduced.values[j:], da_reduced.shift(time=j).values[j:], rowvar=False)[n_components:, :n_components])\n",
    "\n",
    "autocorrs = np.asarray(autocorrs)\n",
    "M = autocorrs[0] + np.sum([autocorrs[i] + autocorrs[i].transpose() for i in range(1, lag_max)], axis=0)\n",
    "\n",
    "eigenvals, eigenvecs = np.linalg.eig(1 / autocorrs[0] @ M)\n",
    "\n",
    "OPPs_realspace = np.tensordot(eigenvecs, pca_results.components_, axes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c65cb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = xr.DataArray(\n",
    "    OPPs_realspace.reshape(n_components, *thisda.shape[1:]), \n",
    "    coords={\"component\": np.arange(n_components), \"lat\": ds.lat.values, \"lon\": ds.lon.values},\n",
    ")\n",
    "projection = ccrs.LambertConformal(\n",
    "    central_longitude=np.mean(da.lon.values),\n",
    ")\n",
    "lon = ds[\"lon\"].values\n",
    "lat = ds[\"lat\"].values\n",
    "fig, axes = plt.subplots(5, 4, figsize=(20, 25), subplot_kw={\"projection\": projection}, constrained_layout=True)\n",
    "extent = [np.amin(lon), np.amax(lon), np.amin(lat), np.amax(lat)]\n",
    "boundary = make_boundary_path(*extent)\n",
    "levels = 10\n",
    "cmap = \"seismic\"\n",
    "lower, upper = -0.045, 0.045\n",
    "# levels = np.delete(np.append(np.linspace(lower, 0, levels), np.linspace(0, upper, levels)), [levels - 1, levels])\n",
    "# cmap = cm.get_cmap(cmap)\n",
    "# norm = mpl.colors.BoundaryNorm(levels, cmap.N, extend='both')\n",
    "# im = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "axes = axes.flatten()\n",
    "for i in range(n_components):\n",
    "    im = axes[i].contourf(\n",
    "        lon,\n",
    "        lat,\n",
    "        centers.isel(component=i), \n",
    "        transform=ccrs.PlateCarree(),\n",
    "#         levels=levels, \n",
    "        cmap=cmap, \n",
    "#         norm=norm,\n",
    "        extend=\"both\",\n",
    "    )\n",
    "    axes[i].contour(\n",
    "        lon,\n",
    "        lat,\n",
    "        centers.isel(component=i), \n",
    "        transform=ccrs.PlateCarree(),\n",
    "#         levels=levels, \n",
    "        colors=\"k\",\n",
    "    )\n",
    "    axes[i].set_boundary(boundary, transform=ccrs.PlateCarree())\n",
    "    axes[i].add_feature(COASTLINE)\n",
    "    axes[i].add_feature(BORDERS)\n",
    "    axes[i].set_title(eigenvals[i])\n",
    "cbar = fig.colorbar(im, ax=axes.ravel().tolist(), spacing=\"proportional\")\n",
    "cbar.ax.set_ylabel(\"Z500 [m]\")\n",
    "# _ = cbar.ax.set_yticks(np.concatenate([np.arange(-0.045, -0.004, 0.005), np.arange(0.005, 0.046, 0.005)]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ec4f15",
   "metadata": {},
   "source": [
    "## Hidden Markov Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca7fcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Zoo = xr.open_dataset(f\"{DATADIR}/ERA5/processed/BarriopedroZooDetrended.nc\")\n",
    "Y = Zoo[\"Lat_anomaly\"].isel(time=Zoo.time.dt.season==\"DJF\").values[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07038739",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 3\n",
    "ghmm = GaussianHMM(n_components=n_components).fit(Y)\n",
    "# im = plt.imshow(ghmm.transmat_)\n",
    "# plt.colorbar(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab88eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_components):\n",
    "    thisnorm = norm(loc=ghmm.means_[i][0], scale=np.sqrt(ghmm.covars_[i][0][0]))\n",
    "    X = np.linspace(thisnorm.ppf(0.005), thisnorm.ppf(0.995), 100)\n",
    "    plt.plot(X, thisnorm.pdf(X) / n_components)\n",
    "plt.plot(*kde(Y, season=None, bins=np.arange(-30, 30.1, 0.25), scaled=False, return_x=True, bw_method=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184616a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "thisda = xr.open_dataarray(f\"{DATADIR}/ERA5/Geopotential/north_atlantic/detrended.nc\").isel(time=Zoo.time.dt.season==\"DJF\")\n",
    "states = ghmm.predict(Y)\n",
    "projection = ccrs.LambertConformal(\n",
    "    central_longitude=np.mean(thisda.lon.values),\n",
    ")\n",
    "fig, axes = plt.subplots(1, n_components, figsize=(20, 6), subplot_kw={\"projection\": projection}, constrained_layout=True)\n",
    "to_plot = [thisda.isel(time=states==i).mean(dim=\"time\") for i in range(n_components)]\n",
    "lon = thisda[\"lon\"].values\n",
    "lat = thisda[\"lat\"].values\n",
    "extent = [np.amin(lon), np.amax(lon), np.amin(lat), np.amax(lat)]\n",
    "boundary = make_boundary_path(*extent)\n",
    "levels = 11\n",
    "cmap = \"seismic\"\n",
    "lower, upper = -150, 150\n",
    "levels = np.delete(np.append(np.linspace(lower, 0, levels), np.linspace(0, upper, levels)), [levels - 1, levels])\n",
    "cmap = cm.get_cmap(cmap)\n",
    "norm = mpl.colors.BoundaryNorm(levels, cmap.N, extend='both')\n",
    "im = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "axes = axes.flatten()\n",
    "for i in range(n_components):\n",
    "    axes[i].contourf(\n",
    "        lon,\n",
    "        lat,\n",
    "        to_plot[i], \n",
    "        transform=ccrs.PlateCarree(),\n",
    "        levels=levels, cmap=cmap, norm=norm,\n",
    "        extend=\"both\",\n",
    "    )\n",
    "    axes[i].contour(\n",
    "        lon,\n",
    "        lat,\n",
    "        to_plot[i], \n",
    "        transform=ccrs.PlateCarree(),\n",
    "        levels=levels, colors=\"k\",\n",
    "    )\n",
    "    axes[i].set_boundary(boundary, transform=ccrs.PlateCarree())\n",
    "    axes[i].add_feature(COASTLINE)\n",
    "    axes[i].add_feature(BORDERS)\n",
    "cbar = fig.colorbar(im, ax=axes.ravel().tolist(), spacing=\"proportional\")\n",
    "cbar.ax.set_ylabel(\"Z500 [m]\")\n",
    "_ = cbar.ax.set_yticks(np.concatenate([np.arange(-150, 20, 30), np.arange(30, 151, 30)]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102631e6",
   "metadata": {},
   "source": [
    "### Length of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e16d068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runs_of_ones_array(bits): # https://stackoverflow.com/questions/1066758/find-length-of-sequences-of-identical-values-in-a-numpy-array-run-length-encodi\n",
    "    # make sure all runs of ones are well-bounded\n",
    "    bounded = np.hstack(([0], bits, [0]))\n",
    "    # get 1 at run starts and -1 at run ends\n",
    "    difs = np.diff(bounded)\n",
    "    run_starts, = np.where(difs > 0)\n",
    "    run_ends, = np.where(difs < 0)\n",
    "    return run_starts, run_ends - run_starts\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(ghmm.n_components):\n",
    "    thisseq = states==i\n",
    "    _, durations = runs_of_ones_array(thisseq)\n",
    "    dur, occu = np.unique(durations, return_counts=True)\n",
    "    durind = np.argsort(dur)\n",
    "    dur = dur[durind]\n",
    "    occu = occu[durind] / np.amax(occu)\n",
    "    ax.plot(dur, occu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482e7a61",
   "metadata": {},
   "source": [
    "## Quasi stationary states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ec26e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dynamical systems theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed16d787",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import CDSK as ck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2600d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_indices = []\n",
    "for year in YEARSPL:\n",
    "    print(year)\n",
    "    da = xr.open_dataset(f\"{DATADIR}/ERA5/Wind/300/north_atlantic/{year}.nc\")[\"u\"]\n",
    "    local_indices.append(ck.dynamical_local_indexes(da.values.reshape(len(da.time), -1, 1)))\n",
    "# todo : classify wind using Lachmy & Harnik 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1b7592-9c87-4de1-915e-57a43b54bb8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ld = xr.DataArray(np.concatenate([li[0].flatten() for li in local_indices]), coords={\"time\": DATERANGEPL}, name=\"ld\")\n",
    "theta = xr.DataArray(np.concatenate([li[1].flatten() for li in local_indices]), coords={\"time\": DATERANGEPL}, name=\"theta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f5f127",
   "metadata": {},
   "outputs": [],
   "source": [
    "ld.to_netcdf(\"ld_u300_NA.nc\")\n",
    "theta.to_netcdf(\"theta_u300_NA.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617fbb64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(10, 12), sharex=True)\n",
    "fig.subplots_adjust(hspace=0)\n",
    "ld.plot(ax=axes[0])\n",
    "persistence = 1 / theta\n",
    "q95 = np.quantile(persistence, 0.95)\n",
    "theta.plot(ax=axes[1])\n",
    "theta.isel(time=persistence>=q95).plot(color=\"red\", ls=\"\", marker=\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764d1a06",
   "metadata": {},
   "source": [
    "## HMM on theta, because it looks ladder-y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67931694",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_theta = theta.values.reshape((-1, 1))\n",
    "n_components = 4\n",
    "ghmm_theta = GaussianHMM(n_components=n_components).fit(Y_theta)\n",
    "belongs = ghmm_theta.predict(Y_theta)\n",
    "theta.plot()\n",
    "for i in range(n_components):\n",
    "    theta.isel(time=belongs==i).plot(marker=\"x\", color=COLORS10[i%10], ls=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b925c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for i, s in enumerate([\"DJF\", \"MAM\", \"JJA\", \"SON\"]):\n",
    "    ax.scatter(ld.isel(time=ld.time.dt.season==s), theta.isel(time=ld.time.dt.season==s), s=2, label=s, c=COLORS5[i])\n",
    "# plt.scatter(ld[20000:], theta[5:])\n",
    "ax.set_xlim([0, 25])\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2cf84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EKE = xr.open_dataarray(f\"{DATADIR}/NCEP/processed/EDG/EKE.nc\").sel(time=DATERANGEPL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8ef3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EKE.lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbbf5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_EKE_NA = EKE.isel(lat=(EKE.lat >= 20)&(EKE.lat <= 80), lon=(EKE.lon >= -30) & (EKE.lon <= 90)).mean(dim=[\"lon\", \"lat\"])\n",
    "mean_EKE_NA.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c8c201",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "idx = np.argsort(mean_EKE_NA).values\n",
    "hi = ax.scatter(ld[idx], theta[idx], c=mean_EKE_NA[idx], s=mean_EKE_NA[idx], cmap=\"cool\", vmin=0, vmax=140)\n",
    "fig.colorbar(hi)\n",
    "ax.set_xlim([-1, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d933cf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "da.differentiate(coord=\"time\").mean(dim=[\"longitude\", \"latitude\"]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be39065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e605aba",
   "metadata": {},
   "source": [
    "### Residence times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591d4ab7",
   "metadata": {},
   "source": [
    "# Recurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a42a3c",
   "metadata": {},
   "source": [
    "## Window counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20518ef1",
   "metadata": {},
   "source": [
    "## Dispersion metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a0cb29",
   "metadata": {},
   "source": [
    "## Ripley K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adf0d3c",
   "metadata": {},
   "source": [
    "## Recurrence plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baff1d4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Duncan's hotspells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a045e37",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Create hotspells file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6951755b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_of_dates = np.loadtxt(\"hotspells.csv\", delimiter=\",\", dtype=np.datetime64)\n",
    "hotspells = {}\n",
    "dataset = \"ERA5\"\n",
    "keys = [\"South\", \"West\", \"Balkans\", \"Scandinavia\", \"Russia\", \"Arctic\"]\n",
    "minus = 21\n",
    "plus = 5\n",
    "for j, key in enumerate(keys):\n",
    "    hotspells[key] = []\n",
    "    dates = np.sort(list_of_dates[:, j])\n",
    "    dates = dates[~(np.isnat(dates) | (np.datetime_as_string(dates, unit=\"Y\") == \"2022\"))]\n",
    "    print(key, len(dates))\n",
    "    for i, date in enumerate(dates):\n",
    "        tsta = date - np.timedelta64(minus, \"D\") + np.timedelta64(9, \"h\")\n",
    "        tend = date + np.timedelta64(plus, \"D\") + np.timedelta64(9, \"h\")\n",
    "        thisds = xr.open_dataset(f\"{DATADIR}/{dataset}/Wind/300/dailymean/{np.datetime_as_string(date, unit='Y')}.nc\")\n",
    "        thisds = thisds.sel(time=pd.date_range(tsta, tend, freq=\"1D\"))\n",
    "        thisds = thisds.assign_coords({\"time\": np.arange(-minus, plus + 1)})\n",
    "        hotspells[key].append(thisds)\n",
    "    hotspells[key] = xr.concat(hotspells[key], dim=\"hotspell\").assign_coords({\"hotspell\": dates})\n",
    "with open(f\"{DATADIR}/{dataset}/processed/hotspells_uv300.pkl\", \"wb\") as handle:\n",
    "    pkl.dump(hotspells, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c0001c",
   "metadata": {},
   "source": [
    "- Replace time coords with range(21), keep track of the center date (or all time as index but not coord) : done\n",
    "- create aggregates : mean, std, correlation with T, look at individuals for all clusters\n",
    "- compare with duncan's plot, also get boundaries of the clusters\n",
    "- cluster wind myself and compare backwards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f961a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Study hotspells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc062dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = \"ERA5\"\n",
    "with open(f\"{DATADIR}/{dataset}/processed/hotspells_uv300.pkl\", \"rb\") as handle:\n",
    "    hotspells = pkl.load(handle)\n",
    "\n",
    "meanwind_hotspell = xr.concat([hotspell.mean(dim=\"hotspell\") for hotspell in hotspells.values()], dim=\"region\").assign_coords({\"region\": list(hotspells.keys())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a0e419",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "meanwind_hotspell_NA = meanwind_hotspell.sel(latitude=np.arange(30, 90, 0.5)).load()\n",
    "lon_NA = meanwind_hotspell_NA.longitude.values\n",
    "lat_NA = meanwind_hotspell_NA.latitude.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be28da25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hvplot.extension('bokeh')\n",
    "\n",
    "ticker_region = pnw.Select(name=\"Region\", options=meanwind_hotspell.region.values.tolist())\n",
    "ticker_variable = pnw.Select(name=\"Component\", options=list(meanwind_hotspell.data_vars.keys()))\n",
    "ticker_kind= pnw.Select(name=\"Kind\", options=[\"contour\", \"contourf\", \"quadmesh\"])\n",
    "\n",
    "tsta, tend = int(np.amin(meanwind_hotspell.time.values)), int(np.amax(meanwind_hotspell.time.values))\n",
    "slider = pnw.IntSlider(name=\"Day around center\", start=tsta, end=tend)\n",
    "\n",
    "extent = [np.amin(lon_NA), np.amax(lon_NA), np.amin(lat_NA), np.amax(lat_NA)]\n",
    "\n",
    "meanwind_hotspell_NA.interactive.sel(region=ticker_region, time=slider).hvplot(\n",
    "    kind=ticker_kind, x=\"longitude\", y=\"latitude\", z=ticker_variable, \n",
    "    title=\"Wind at 300hPa\", cmap=\"seismic\", symmetric=True, line_width=1.5, projection=\"NorthPolarStereo\",\n",
    "    geo=True, coastline='110m', levels=11, xlim=extent[:2], ylim=extent[2:4],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd8c193",
   "metadata": {},
   "source": [
    "## Zoo during hotspells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fd5b78",
   "metadata": {},
   "source": [
    "### Create hotspells_zoo file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408e4263",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_of_dates = np.loadtxt(\"hotspells.csv\", delimiter=\",\", dtype=np.datetime64)\n",
    "hotspells_Zoo = {}\n",
    "dataset = \"ERA5\"\n",
    "keys = [\"South\", \"West\", \"Balkans\", \"Scandinavia\", \"Russia\", \"Arctic\"]\n",
    "dataset = \"ERA5\"\n",
    "datadir = f\"{DATADIR}/{dataset}/processed\"\n",
    "Zoo = xr.open_dataset(f\"{datadir}/BarriopedroZooDetrended.nc\")\n",
    "Zookeys = list(Zoo.data_vars.keys()) # copy and not view !\n",
    "minus = 21\n",
    "plus = 5\n",
    "for varname in Zookeys:\n",
    "    if varname[-11:].split('_')[-1] == \"climatology\":\n",
    "        del Zoo[varname]\n",
    "for j, key in enumerate(keys):\n",
    "    hotspells_Zoo[key] = []\n",
    "    dates = np.sort(list_of_dates[:, j])\n",
    "    dates = dates[~(np.isnat(dates) | (np.datetime_as_string(dates, unit=\"Y\") == \"2022\"))]\n",
    "    for i, date in enumerate(dates):\n",
    "        tsta = date - np.timedelta64(minus, \"D\") + np.timedelta64(9, \"h\")\n",
    "        tend = date + np.timedelta64(plus, \"D\") + np.timedelta64(9, \"h\")\n",
    "        thisds = Zoo.sel(time=pd.date_range(tsta, tend, freq=\"1D\"))\n",
    "        thisds.attrs[\"center_date\"] = date\n",
    "        thisds = thisds.assign_coords({\"time\": np.arange(-minus, plus + 1)}).reset_index(\"dayofyear\", drop=True)\n",
    "        hotspells_Zoo[key].append(thisds)\n",
    "    hotspells_Zoo[key] = xr.concat(hotspells_Zoo[key], dim=\"hotspell\").assign_coords({\"hotspell\": dates})\n",
    "with open(f\"{DATADIR}/{dataset}/processed/hotspells_Zoo.pkl\", \"wb\") as handle:\n",
    "    pkl.dump(hotspells_Zoo, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b84f820",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d43ba4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = \"ERA5\"\n",
    "datadir = f\"{DATADIR}/{dataset}/processed\"\n",
    "Zoo = xr.open_dataset(f\"{datadir}/BarriopedroZooDetrended.nc\")\n",
    "with open(f\"{DATADIR}/{dataset}/processed/hotspells_Zoo.pkl\", \"rb\") as handle:\n",
    "    hotspells_Zoo = pkl.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06726fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 3, figsize=(20, 25), tight_layout=True)\n",
    "axes = axes.flatten()\n",
    "for k, key in enumerate([f\"{key}_anomaly\" for key in ZOO]):\n",
    "    ax = axes[k]\n",
    "    for i, regionkey in enumerate(hotspells_Zoo):\n",
    "        to_plot = hotspells_Zoo[regionkey][key].mean(dim=\"hotspell\")\n",
    "        (to_plot / np.amax(np.abs(to_plot))).plot(ax=ax, label=regionkey, color=COLORS10[(2 * i) % 9], lw=2)\n",
    "        # ax.fill_between(\n",
    "        #     to_plot.time, \n",
    "        #     *(np.quantile(hotspells_Zoo[regionkey][key], [0.05, 0.95], axis=0) / np.amax(np.abs(to_plot)).values), \n",
    "        #     color=COLORS10[(2 * i) % 9],\n",
    "        #     alpha=0.1,\n",
    "        # )\n",
    "    ax.set_title(key)\n",
    "    ax.set_xlabel(\"Time around center\")\n",
    "    ax.set_ylabel(\"Normalized anomaly\")\n",
    "    if k==9:\n",
    "        ax.legend(ncol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950060d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 9), tight_layout=True)\n",
    "axes = axes.flatten()\n",
    "tsta, tend = hotspells_Zoo[\"South\"].time.values[[0, -1]]\n",
    "for k, regionkey in enumerate(hotspells_Zoo):\n",
    "    ax = axes[k]\n",
    "    for i, key in enumerate([f\"{key}_anomaly\" for key in [\"Lat\", \"Int\", \"Tilt\", \"Lon\", \"Mea\"]]):\n",
    "        to_plot = hotspells_Zoo[regionkey][key].mean(dim=\"hotspell\")\n",
    "        (to_plot / np.amax(np.abs(Zoo[key]))).plot(ax=ax, label=key.split(\"_\")[0], color=COLORS5[i % 5], lw=2)\n",
    "    ax.set_title(regionkey)\n",
    "    ax.set_xlabel(\"Days around center\")\n",
    "    ax.set_ylabel(\"Normalized anomaly\")\n",
    "    ax.set_xticks(np.arange(tsta, tend + 1, 2))\n",
    "    if k==5:\n",
    "        ax.legend(ncol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac946b5",
   "metadata": {},
   "source": [
    "## Dynamical indices during hotspells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e023f31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dates = np.loadtxt(\"hotspells.csv\", delimiter=\",\", dtype=np.datetime64)\n",
    "hotspells_dynind = {}\n",
    "dataset = \"ERA5\"\n",
    "keys = [\"South\", \"West\", \"Balkans\", \"Scandinavia\", \"Russia\", \"Arctic\"]\n",
    "dataset = \"ERA5\"\n",
    "datadir = f\"{DATADIR}/{dataset}/processed\"\n",
    "ld = xr.open_dataarray(f\"{datadir}/ld.nc\")\n",
    "theta = xr.open_dataarray(f\"{datadir}/theta.nc\")\n",
    "indices = xr.Dataset({\"ld\": ld, \"theta\": theta})\n",
    "indikeys = list(indices.data_vars.keys()) # copy and not view !\n",
    "minus = 21\n",
    "plus = 5\n",
    "for j, key in enumerate(keys):\n",
    "    hotspells_dynind[key] = []\n",
    "    dates = np.sort(list_of_dates[:, j])\n",
    "    dates = dates[~(np.isnat(dates) | (np.datetime_as_string(dates, unit=\"Y\") == \"2022\"))]\n",
    "    for i, date in enumerate(dates):\n",
    "        tsta = date - np.timedelta64(minus, \"D\")\n",
    "        tend = date + np.timedelta64(plus, \"D\")\n",
    "        thisds = indices.sel(time=pd.date_range(tsta, tend, freq=\"1D\"))\n",
    "        thisds.attrs[\"center_date\"] = date\n",
    "        thisds = thisds.assign_coords({\"time\": np.arange(-minus, plus + 1)})\n",
    "        hotspells_dynind[key].append(thisds)\n",
    "    hotspells_dynind[key] = xr.concat(hotspells_dynind[key], dim=\"hotspell\").assign_coords({\"hotspell\": dates})\n",
    "with open(f\"{DATADIR}/{dataset}/processed/hotspells_dynind.pkl\", \"wb\") as handle:\n",
    "    pkl.dump(hotspells_dynind, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1a8579",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 9), tight_layout=True)\n",
    "axes = axes.flatten()\n",
    "tsta, tend = hotspells_dynind[\"South\"].time.values[[0, -1]]\n",
    "for k, regionkey in enumerate(hotspells_dynind):\n",
    "    ax = axes[k]\n",
    "    for i, key in enumerate(hotspells_dynind[\"South\"].data_vars):\n",
    "        to_plot = hotspells_dynind[regionkey][key].mean(dim=\"hotspell\")\n",
    "        (to_plot / np.amax(to_plot)).plot(ax=ax, label=key.split(\"_\")[0], color=COLORS5[i % 5], lw=2)\n",
    "    ax.set_title(regionkey)\n",
    "    ax.set_xlabel(\"Day around center\")\n",
    "    ax.set_ylabel(\"Normalized index\")\n",
    "    ax.set_xticks(np.arange(tsta, tend + 1, 2))\n",
    "    if k==5:\n",
    "        ax.legend(ncol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7616a2cc",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c275439",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Create_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0356ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plot(to_plot, titles, levels, twolevel=False, startindex=-1):\n",
    "    # Figure\n",
    "    transform = ccrs.PlateCarree()\n",
    "    projection = transform\n",
    "    if twolevel:\n",
    "        fig, axes = plt.subplots(\n",
    "            2,\n",
    "            int(len(to_plot) / 2),\n",
    "            subplot_kw={\"projection\": projection}, constrained_layout=True #, figsize=(6 * len(to_plot) // 2, 13)\n",
    "        )\n",
    "    else:\n",
    "        fig, axes = plt.subplots(\n",
    "            1, len(to_plot), subplot_kw={\"projection\": projection}, constrained_layout=True, figsize=(3.5 * len(to_plot), 6)\n",
    "        )\n",
    "    axes = np.atleast_1d(axes)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Add coastline and boarders\n",
    "    coastline = feat.NaturalEarthFeature(\n",
    "        \"physical\", \"coastline\", \"10m\", edgecolor=\"black\", facecolor=\"none\"\n",
    "    )\n",
    "    borders = feat.NaturalEarthFeature(\n",
    "        \"cultural\",\n",
    "        \"admin_0_boundary_lines_land\",\n",
    "        \"10m\",\n",
    "        edgecolor=\"grey\",\n",
    "        facecolor=\"none\",\n",
    "    )\n",
    "    plt_rej = []\n",
    "    cbar = [None] * len(to_plot)\n",
    "    for j in range(len(to_plot)):\n",
    "        ax = axes[j]\n",
    "        plt_rej.append(\n",
    "            ax.contourf(\n",
    "                to_plot[j][\"lon\"].values[:, None] * np.ones(len(to_plot[j][\"lat\"])),\n",
    "                to_plot[j][\"lat\"].values[None, :] * np.ones(len(to_plot[j][\"lon\"]))[:, None],\n",
    "                to_plot[j].isel(time=startindex).transpose(),\n",
    "                levels=levels[j],\n",
    "                transform=transform,\n",
    "                transform_first=True,\n",
    "                # cmap=cmap,\n",
    "                zorder=0,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        ax.add_feature(coastline)\n",
    "        ax.add_feature(borders)\n",
    "        ax.set_xmargin(0)\n",
    "        ax.set_ymargin(0)\n",
    "        ax.set_title(f\"Day {startindex}, {titles[j]}, g.a : {np.mean(to_plot[j][startindex]):.2f}\")\n",
    "\n",
    "        cbar[j] = fig.colorbar(plt_rej[j], ax=ax,fraction=0.046, pad=0.04)\n",
    "\n",
    "    def animate_all(i):\n",
    "        global plt_rej\n",
    "        for j in range(len(to_plot)):\n",
    "            ax = axes[j]\n",
    "            for c in plt_rej[j].collections:\n",
    "                c.remove()\n",
    "            plt_rej[j] = ax.contourf(\n",
    "                to_plot[j][\"lon\"].values[:, None] * np.ones(len(to_plot[j][\"lat\"])),\n",
    "                to_plot[j][\"lat\"].values[None, :] * np.ones(len(to_plot[j][\"lon\"]))[:, None],\n",
    "                to_plot[j].isel(time=i).transpose(),\n",
    "                levels=levels[j],\n",
    "                transform=transform,\n",
    "                transform_first=True,\n",
    "                # cmap=cmap,\n",
    "                zorder=0,\n",
    "            )\n",
    "            ax.set_title(f\"Day {i + 1}, {titles[j]}, g.a : {np.mean(to_plot[j][i]):.2f}\")\n",
    "            cbar[j] = fig.colorbar(plt_rej[j], cax=fig.axes[len(axes) + j])\n",
    "        return plt_rej\n",
    "\n",
    "    return fig, axes, plt_rej, animate_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae42819",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716d6aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "longname = {\n",
    "    \"u\": \"U-component of wind\",\n",
    "    \"v\": \"V-component of wind\",\n",
    "    \"w\": \"W-component of wind\",\n",
    "    \"z\": \"Geopotential\",\n",
    "    \"t\": \"Temperature\",\n",
    "    \"vo\": \"Relative vorticity\",\n",
    "    \"q\": \"Specific humidity\",\n",
    "    \"r\": \"relative humidity\"\n",
    "    \n",
    "}\n",
    "variablemap = {\n",
    "    f\"{var}{lev}\": [var, \"PL\", lev, f\"{longname[var]} at {lev} hPa\"] \n",
    "    for var in [\"u\", \"v\", \"vo\"] \n",
    "    for lev in range(700, 901, 50)\n",
    "}\n",
    "var = \"z\"\n",
    "vm2 = {\n",
    "    f\"{var}{lev}\": [var, \"PL\", lev, f\"{longname[var]} at {lev} hPa\"] \n",
    "    for lev in [300, 500]\n",
    "}\n",
    "\n",
    "variablemap.update(vm2)\n",
    "variablemap[\"t850\"] = [\"t\", \"PL\", 850, f\"{longname['t']} at 850 hPa\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8452f1fe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### PV calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767dcf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_mfdataset(fn(DATERANGEML[0], which=\"ML\")[:1], combine=\"nested\", concat_dim=\"time\")\n",
    "ds[\"P\"] = (ds[\"hybm\"] * ds[\"PS\"] + ds[\"hyam\"]).isel(lev_2=0).drop(\"lev_2\").rename({\"nhym\": \"lev\"})\n",
    "ds[\"P\"].attrs[\"units\"] = \"Pa\"\n",
    "ds[\"T\"].attrs[\"units\"] = \"celsius\"\n",
    "ds = ds.isel(lat=range(1, len(ds.lat) -1)).metpy.quantify()\n",
    "ds[\"THETA\"] = mcalc.potential_temperature(ds[\"P\"], ds[\"T\"])\n",
    "ds[\"PV\"] = mcalc.potential_vorticity_baroclinic(ds[\"THETA\"], ds[\"P\"], ds[\"U\"], ds[\"V\"], x_dim=3, y_dim=2, vertical_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baad1777",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 4, figsize=[20, 20])\n",
    "axes = axes.flatten()\n",
    "for l, k in enumerate(range(0, 137, int(137/20) + 1)):\n",
    "    ds[\"PV\"].isel(lev=k, time=0).plot(ax=axes[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9ba649",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = [\"time\", \"lat\", \"lon\"]\n",
    "ds[\"U_2PVU\"] = xr.DataArray(np.empty([len(ds[\"T\"].coords[dim]) for dim in dims]), dims=dims, coords={dim: ds[\"T\"].coords[dim] for dim in dims})\n",
    "ds[\"V_2PVU\"] = ds[\"U_2PVU\"].copy()\n",
    "for ti, t in enumerate(ds.time):\n",
    "    this_ds = ds.isel(time=ti)\n",
    "    for w in [\"U\", \"V\"]:\n",
    "        ds[f\"{w}_2PVU\"][ti, :, :] = minterpolate.interpolate_to_isosurface(this_ds[\"PV\"].values, this_ds[w].values, 2e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea5bfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"U_2PVU\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93445d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"V_2PVU\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e005c1e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Matplotlib widgets example (use panel + hvplot it's much faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b531afdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = ccrs.PlateCarree()\n",
    "projection = ccrs.LambertConformal(central_longitude=np.mean(lon_NA))\n",
    "fig, ax = plt.subplots(figsize=(15, 7), subplot_kw={\"projection\": projection})\n",
    "REGION = \"South\"\n",
    "DAY = 0\n",
    "VARIABLE = \"u\"\n",
    "mesh = ax.pcolormesh(\n",
    "    lon_NA, lat_NA, meanwind_hotspell_NA[VARIABLE].sel(region=REGION, time=DAY), \n",
    "    shading=\"nearest\", cmap=\"bwr\", transform=transform, norm=CenteredNorm())\n",
    "fig.subplots_adjust(left=0.3)\n",
    "cbar = fig.colorbar(mesh)\n",
    "ax.add_feature(COASTLINE)\n",
    "ax.add_feature(BORDERS)\n",
    "extent = [np.amin(lon_NA), np.amax(lon_NA), np.amin(lat_NA), np.amax(lat_NA)]\n",
    "boundary = make_boundary_path(*extent)\n",
    "ax.set_boundary(boundary, transform=ccrs.PlateCarree())\n",
    "\n",
    "def update_mesh():\n",
    "    global mesh, cbar\n",
    "    mesh.set_array(meanwind_hotspell_NA[VARIABLE].sel(region=REGION, time=DAY).values.flatten())\n",
    "    mesh.autoscale()\n",
    "    cbar.update_normal(mesh)\n",
    "    plt.draw()\n",
    "\n",
    "def change_region(region):\n",
    "    global REGION\n",
    "    REGION=region\n",
    "    update_mesh()\n",
    "\n",
    "def change_day(day):\n",
    "    global DAY\n",
    "    DAY=day\n",
    "    update_mesh()\n",
    "\n",
    "def change_variable(variable):\n",
    "    global VARIABLE\n",
    "    VARIABLE=variable\n",
    "    update_mesh()\n",
    "\n",
    "rax = fig.add_axes([0.05, 0.4, 0.2, 0.4])\n",
    "radio_region = RadioButtons(rax, meanwind_hotspell[VARIABLE].region.values)\n",
    "radio_region.on_clicked(change_region)\n",
    "\n",
    "rax = fig.add_axes([0.05, 0.25, 0.2, 0.17])\n",
    "radio_variable = RadioButtons(rax, list(meanwind_hotspell.data_vars.keys()))\n",
    "radio_variable.on_clicked(change_variable)\n",
    "\n",
    "sax = fig.add_axes([0.32, 0.05, 0.4, 0.03])\n",
    "slider_day = Slider(\n",
    "    ax=sax,\n",
    "    label='Day',\n",
    "    valmin=-10,\n",
    "    valstep=1,\n",
    "    valmax=10,\n",
    "    valinit=0,\n",
    ")\n",
    "slider_day.on_changed(change_day)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "9223f93c41a71907969096cc96eb822b996dbdaa12a79296458cf8973dfe72e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
