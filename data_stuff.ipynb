{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetstream_hugo.definitions import *\n",
    "from jetstream_hugo.plots import *\n",
    "from jetstream_hugo.data import *\n",
    "from jetstream_hugo.anyspell import *\n",
    "from jetstream_hugo.jet_finding import *\n",
    "from jetstream_hugo.clustering import *\n",
    "import intake\n",
    "\n",
    "import colormaps\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# from dask.distributed import Client, progress\n",
    "# client = Client(**COMPUTE_KWARGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# arco-era5 tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar\n",
    "def _compute(obj, progress: bool = False, **kwargs):\n",
    "    kwargs = COMPUTE_KWARGS | kwargs\n",
    "    try:\n",
    "        if progress:\n",
    "            with ProgressBar():\n",
    "                return obj.compute(**kwargs)\n",
    "        else:\n",
    "            return obj.compute(**kwargs)\n",
    "    except AttributeError:\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "ds = xr.open_zarr(\n",
    "    'gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3',\n",
    "    chunks=None,\n",
    "    storage_options=dict(token='anon'),\n",
    ")\n",
    "ar_full_37_1h = ds.sel(time=slice(ds.attrs['valid_time_start'], ds.attrs['valid_time_stop']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_ds = ar_full_37_1h[[\"u_component_of_wind\", \"v_component_of_wind\"]].sel(time=ar_full_37_1h.time.dt.hour % 6 == 0, latitude=ar_full_37_1h.latitude >= 0, level=[175,  200,  225,  250,  300,  350]).isel(longitude=slice(None, None, 2), latitude=slice(None, None, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(f\"/storage/workspaces/giub_meteo_impacts/ci01/ERA5/plev/flat_wind/dailymean\")\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def downloader(base_ds, base_path, month, year):\n",
    "    month_str = str(month).zfill(2)\n",
    "    opath = base_path.joinpath(f\"{year}{month_str}.nc\")\n",
    "    if opath.is_file():\n",
    "        return f\"Already had {year}{month}\"\n",
    "    ds = _compute(base_ds.sel(time=(base_ds.time.dt.year==year) & (base_ds.time.dt.month==month)), progress=True)\n",
    "    ds = standardize(ds)\n",
    "    ds[\"s\"] = np.sqrt(ds[\"u\"] ** 2 + ds[\"v\"] ** 2)\n",
    "    ds = flatten_by(ds, \"s\")\n",
    "    ds.to_netcdf(opath)\n",
    "    return f\"Completed {year}{month}\"\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=6) as executor:\n",
    "    futures = [\n",
    "        executor.submit(downloader, base_ds.copy(), base_path, month, year) for year in YEARS for month in range(1, 13)\n",
    "    ]\n",
    "    for f in as_completed(futures):\n",
    "        try:\n",
    "            print(f.result())\n",
    "        except:\n",
    "            print(\"could not retrieve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/workspaces/giub_meteo_impacts/ci01/ERA5/surf/t2m/dailymean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:29<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/workspaces/giub_meteo_impacts/ci01/ERA5/surf/mslp/dailymean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:29<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/workspaces/giub_meteo_impacts/ci01/ERA5/surf/sst/dailymean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:34<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/workspaces/giub_meteo_impacts/ci01/ERA5/surf/tp/dailymean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:59<00:00,  1.08it/s]\n"
     ]
    }
   ],
   "source": [
    "varnames = {\n",
    "    \"t2m\": \"2m_temperature\",\n",
    "    \"mslp\": \"mean_sea_level_pressure\",\n",
    "    \"sst\": \"sea_surface_temperature\",\n",
    "    \"tp\": \"total_precipitation\",\n",
    "}\n",
    "for key, val in varnames.items():\n",
    "    base_da = ar_full_37_1h[val].sel(time=ar_full_37_1h.time.dt.hour % 6 == 0, latitude=ar_full_37_1h.latitude >= 0)[:, ::2, ::2]\n",
    "    base_path = Path(f\"/storage/workspaces/giub_meteo_impacts/ci01/ERA5/surf/{key}/dailymean\")\n",
    "    print(base_path)\n",
    "    for year in tqdm(YEARS):\n",
    "        opath = base_path.joinpath(f\"{year}.nc\")\n",
    "        if opath.is_file():\n",
    "            da = xr.open_dataarray(opath)\n",
    "            da = standardize(da)\n",
    "        else:\n",
    "            da = _compute(base_da.sel(time=base_da.time.dt.year==year), progress=True).resample(time=\"1D\").mean()\n",
    "        da.to_netcdf(opath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climatologies, datahandlers of new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100.00% Completed | 30.55 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:25<00:00,  2.52it/s]\n"
     ]
    }
   ],
   "source": [
    "compute_all_smoothed_anomalies(\"ERA5\", \"surf\", \"t2m\", \"dailymean\", 'dayofyear', {'dayofyear': ('win', 15)}, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100.00% Completed | 38.06 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:57<00:00,  1.11it/s]\n"
     ]
    }
   ],
   "source": [
    "compute_all_smoothed_anomalies(\"ERA5\", \"surf\", \"tp\", \"dailymean\", 'dayofyear', {'dayofyear': ('win', 15)}, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100.00% Completed | 30.18 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:22<00:00,  2.82it/s]\n"
     ]
    }
   ],
   "source": [
    "compute_all_smoothed_anomalies(\"ERA5\", \"surf\", \"sst\", \"dailymean\", 'dayofyear', {'dayofyear': ('win', 15)}, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100.00% Completed | 31.71 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:22<00:00,  2.81it/s]\n"
     ]
    }
   ],
   "source": [
    "compute_all_smoothed_anomalies(\"ERA5\", \"surf\", \"mslp\", \"dailymean\", 'dayofyear', {'dayofyear': ('win', 15)}, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new pvs das"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR 1: PROJ: proj_create_from_database: Open of /storage/homefs/hb22g102/miniforge3/envs/env11_2/share/proj failed\n",
      "WARNING: Skipping field com: unsupported OGR type: 3\n"
     ]
    }
   ],
   "source": [
    "from jetstream_hugo.definitions import TIMERANGE\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "from tqdm import trange\n",
    "from wavebreaking import to_xarray\n",
    "da = xr.open_dataarray(f\"/storage/workspaces/giub_meteo_impacts/ci01/ERA5/surf/t2m/dailymean/1999.nc\")\n",
    "coords = {\n",
    "    \"time\": TIMERANGE,\n",
    "    \"lat\": da.lat.values,\n",
    "    \"lon\": da.lon.values,\n",
    "}\n",
    "shape = [len(co) for co in coords.values()]\n",
    "dummy_da = xr.DataArray(np.zeros(shape), coords=coords)\n",
    "das_ones = []\n",
    "das_int = []\n",
    "events = gpd.read_file(f\"/storage/workspaces/giub_meteo_impacts/ci01/ERA5/RWB_index/era5_pv_streamers_310K_1959-2022.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def one_level_events(level: int):\n",
    "    \n",
    "    \n",
    "    return da_ones, da_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]WARNING: Skipping field com: unsupported OGR type: 3\n",
      "  0%|          | 0/11 [00:10<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from jetstream_hugo.definitions import TIMERANGE\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "from tqdm import trange\n",
    "from wavebreaking import to_xarray\n",
    "da = xr.open_dataarray(f\"/storage/workspaces/giub_meteo_impacts/ci01/ERA5/surf/t2m/dailymean/1999.nc\")\n",
    "coords = {\n",
    "    \"time\": TIMERANGE,\n",
    "    \"lat\": da.lat.values,\n",
    "    \"lon\": da.lon.values,\n",
    "}\n",
    "shape = [len(co) for co in coords.values()]\n",
    "dummy_da = xr.DataArray(np.zeros(shape), coords=coords)\n",
    "das_ones = []\n",
    "das_int = []\n",
    "\n",
    "da = xr.open_dataarray(f\"/storage/workspaces/giub_meteo_impacts/ci01/ERA5/surf/t2m/dailymean/1999.nc\")\n",
    "coords = {\n",
    "    \"time\": TIMERANGE,\n",
    "    \"lat\": da.lat.values,\n",
    "    \"lon\": da.lon.values,\n",
    "}\n",
    "shape = [len(co) for co in coords.values()]\n",
    "dummy_da = xr.DataArray(np.zeros(shape), coords=coords)\n",
    "\n",
    "for level in trange(310, 365, 5):\n",
    "    events = gpd.read_file(f\"/storage/workspaces/giub_meteo_impacts/ci01/ERA5/RWB_index/era5_pv_streamers_{level}K_1959-2022.parquet\")\n",
    "    \n",
    "    tropospheric = events[events.mean_var < events.level]\n",
    "\n",
    "    anticyclonic = tropospheric[tropospheric.intensity >= 0].reset_index(drop=True)\n",
    "    cyclonic = tropospheric[tropospheric.intensity < 0].reset_index(drop=True)\n",
    "    \n",
    "    # da_anti = to_xarray(dummy_da, anticyclonic)\n",
    "    # da_cycl = to_xarray(dummy_da, cyclonic)\n",
    "    \n",
    "    # da_ones = xr.concat([da_anti, da_cycl], dim=\"type\").assign_coords({\"type\": [\"anti\", \"cycl\"]})\n",
    "    \n",
    "    # da_anti = to_xarray(dummy_da, anticyclonic, \"intensity\", \"intensity\")\n",
    "    # da_cycl = to_xarray(dummy_da, cyclonic, \"intensity\", \"intensity\")\n",
    "    \n",
    "    # da_int = xr.concat([da_anti, da_cycl], dim=\"type\").assign_coords({\"type\": [\"anti\", \"cycl\"]})\n",
    "    break\n",
    "\n",
    "# res = map_maybe_parallel(levels, one_level_events, len(levels), processes=1)\n",
    "# da_ones, da_int = list(zip(*res))\n",
    "# da_ones = xr.concat(da_ones, dim=\"level\").assign_coords(level=levels)\n",
    "# da_int = xr.concat(da_int, dim=\"level\").assign_coords(level=levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>level</th>\n",
       "      <th>mean_var</th>\n",
       "      <th>event_area</th>\n",
       "      <th>intensity</th>\n",
       "      <th>__index_level_0__</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1959-01-01 00:00:00</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.40</td>\n",
       "      <td>387642.21</td>\n",
       "      <td>-56.03</td>\n",
       "      <td>0</td>\n",
       "      <td>MULTIPOLYGON (((35 -48, 36 -48, 37 -48, 38 -48...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1959-01-01 00:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.69</td>\n",
       "      <td>1507990.88</td>\n",
       "      <td>-98.13</td>\n",
       "      <td>4</td>\n",
       "      <td>MULTIPOLYGON (((179 70, 179 66, 178 66, 177 66...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1959-01-01 06:00:00</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.55</td>\n",
       "      <td>622487.10</td>\n",
       "      <td>-98.13</td>\n",
       "      <td>5</td>\n",
       "      <td>MULTIPOLYGON (((39 -48, 40 -48, 41 -48, 42 -48...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1959-01-01 06:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.88</td>\n",
       "      <td>901350.71</td>\n",
       "      <td>-116.81</td>\n",
       "      <td>9</td>\n",
       "      <td>MULTIPOLYGON (((179 71, 179 68, 178 68, 177 68...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1959-01-01 12:00:00</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.43</td>\n",
       "      <td>1030645.39</td>\n",
       "      <td>-59.38</td>\n",
       "      <td>10</td>\n",
       "      <td>MULTIPOLYGON (((40 -50, 41 -50, 42 -50, 43 -50...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191464</th>\n",
       "      <td>2022-12-31 18:00:00</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.63</td>\n",
       "      <td>462634.04</td>\n",
       "      <td>-121.72</td>\n",
       "      <td>663852</td>\n",
       "      <td>MULTIPOLYGON (((106 -59, 106 -58, 107 -57, 108...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191465</th>\n",
       "      <td>2022-12-31 18:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.59</td>\n",
       "      <td>439534.20</td>\n",
       "      <td>-19.05</td>\n",
       "      <td>663854</td>\n",
       "      <td>MULTIPOLYGON (((-105 52, -106 53, -106 54, -10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191466</th>\n",
       "      <td>2022-12-31 18:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.42</td>\n",
       "      <td>1222794.74</td>\n",
       "      <td>-25.11</td>\n",
       "      <td>663856</td>\n",
       "      <td>MULTIPOLYGON (((-89 52, -88 53, -87 54, -86 55...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191467</th>\n",
       "      <td>2022-12-31 18:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1049064.84</td>\n",
       "      <td>-64.59</td>\n",
       "      <td>663860</td>\n",
       "      <td>MULTIPOLYGON (((32 62, 33 63, 33 64, 33 65, 32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191468</th>\n",
       "      <td>2022-12-31 18:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.63</td>\n",
       "      <td>470875.96</td>\n",
       "      <td>-22.23</td>\n",
       "      <td>663862</td>\n",
       "      <td>MULTIPOLYGON (((52 52, 52 53, 52 54, 53 55, 54...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191469 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      date  level  mean_var  event_area  intensity  \\\n",
       "0      1959-01-01 00:00:00   -2.0     -2.40   387642.21     -56.03   \n",
       "1      1959-01-01 00:00:00    2.0      1.69  1507990.88     -98.13   \n",
       "2      1959-01-01 06:00:00   -2.0     -2.55   622487.10     -98.13   \n",
       "3      1959-01-01 06:00:00    2.0      1.88   901350.71    -116.81   \n",
       "4      1959-01-01 12:00:00   -2.0     -2.43  1030645.39     -59.38   \n",
       "...                    ...    ...       ...         ...        ...   \n",
       "191464 2022-12-31 18:00:00   -2.0     -2.63   462634.04    -121.72   \n",
       "191465 2022-12-31 18:00:00    2.0      1.59   439534.20     -19.05   \n",
       "191466 2022-12-31 18:00:00    2.0      1.42  1222794.74     -25.11   \n",
       "191467 2022-12-31 18:00:00    2.0      1.48  1049064.84     -64.59   \n",
       "191468 2022-12-31 18:00:00    2.0      1.63   470875.96     -22.23   \n",
       "\n",
       "        __index_level_0__                                           geometry  \n",
       "0                       0  MULTIPOLYGON (((35 -48, 36 -48, 37 -48, 38 -48...  \n",
       "1                       4  MULTIPOLYGON (((179 70, 179 66, 178 66, 177 66...  \n",
       "2                       5  MULTIPOLYGON (((39 -48, 40 -48, 41 -48, 42 -48...  \n",
       "3                       9  MULTIPOLYGON (((179 71, 179 68, 178 68, 177 68...  \n",
       "4                      10  MULTIPOLYGON (((40 -50, 41 -50, 42 -50, 43 -50...  \n",
       "...                   ...                                                ...  \n",
       "191464             663852  MULTIPOLYGON (((106 -59, 106 -58, 107 -57, 108...  \n",
       "191465             663854  MULTIPOLYGON (((-105 52, -106 53, -106 54, -10...  \n",
       "191466             663856  MULTIPOLYGON (((-89 52, -88 53, -87 54, -86 55...  \n",
       "191467             663860  MULTIPOLYGON (((32 62, 33 63, 33 64, 33 65, 32...  \n",
       "191468             663862  MULTIPOLYGON (((52 52, 52 53, 52 54, 53 55, 54...  \n",
       "\n",
       "[191469 rows x 7 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cyclonic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoms = cyclonic[\"geometry\"].get_coordinates(index_parts=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoms[\"time\"] = cyclonic.loc[geoms[\"level_0\"].values, \"date\"].values\n",
    "geoms[\"intensity\"] = cyclonic.loc[geoms[\"level_0\"].values, \"intensity\"].values\n",
    "geoms = geoms.drop([\"level_0\", \"level_1\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>time</th>\n",
       "      <th>intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35.0</td>\n",
       "      <td>-48.0</td>\n",
       "      <td>1959-01-01 00:00:00</td>\n",
       "      <td>-56.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36.0</td>\n",
       "      <td>-48.0</td>\n",
       "      <td>1959-01-01 00:00:00</td>\n",
       "      <td>-56.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37.0</td>\n",
       "      <td>-48.0</td>\n",
       "      <td>1959-01-01 00:00:00</td>\n",
       "      <td>-56.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38.0</td>\n",
       "      <td>-48.0</td>\n",
       "      <td>1959-01-01 00:00:00</td>\n",
       "      <td>-56.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39.0</td>\n",
       "      <td>-48.0</td>\n",
       "      <td>1959-01-01 00:00:00</td>\n",
       "      <td>-56.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8167195</th>\n",
       "      <td>62.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2022-12-31 18:00:00</td>\n",
       "      <td>-22.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8167196</th>\n",
       "      <td>62.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2022-12-31 18:00:00</td>\n",
       "      <td>-22.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8167197</th>\n",
       "      <td>63.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2022-12-31 18:00:00</td>\n",
       "      <td>-22.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8167198</th>\n",
       "      <td>63.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2022-12-31 18:00:00</td>\n",
       "      <td>-22.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8167199</th>\n",
       "      <td>52.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2022-12-31 18:00:00</td>\n",
       "      <td>-22.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8167200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            x     y                time  intensity\n",
       "0        35.0 -48.0 1959-01-01 00:00:00     -56.03\n",
       "1        36.0 -48.0 1959-01-01 00:00:00     -56.03\n",
       "2        37.0 -48.0 1959-01-01 00:00:00     -56.03\n",
       "3        38.0 -48.0 1959-01-01 00:00:00     -56.03\n",
       "4        39.0 -48.0 1959-01-01 00:00:00     -56.03\n",
       "...       ...   ...                 ...        ...\n",
       "8167195  62.0  58.0 2022-12-31 18:00:00     -22.23\n",
       "8167196  62.0  57.0 2022-12-31 18:00:00     -22.23\n",
       "8167197  63.0  56.0 2022-12-31 18:00:00     -22.23\n",
       "8167198  63.0  55.0 2022-12-31 18:00:00     -22.23\n",
       "8167199  52.0  52.0 2022-12-31 18:00:00     -22.23\n",
       "\n",
       "[8167200 rows x 4 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_int = da_int.astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.74176512"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "93504 * 181 * 720 * 2 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.74176512"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da_int.nbytes / 10 ** 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CESM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "basepath = Path(\"/storage/workspaces/giub_meteo_impacts/ci01/CESM2/flat_wind\")\n",
    "paths = list(basepath.iterdir())\n",
    "paths = [path for path in paths if path.suffix == \".nc\" and path.name != \"ds.nc\"]\n",
    "parts = [path.name.split(\".\")[0].split(\"-\") for path in paths]\n",
    "parts = np.asarray(parts)\n",
    "sorted_order = np.argsort([memb.replace(\"r10\", \"r0\") for memb in parts[:, 0]])\n",
    "parts = parts[sorted_order]\n",
    "paths = [paths[i] for i in sorted_order]\n",
    "all_members = np.unique(parts[:, 0])\n",
    "all_years = np.unique(parts[:, 1])\n",
    "\n",
    "not_here = []\n",
    "here = []\n",
    "for year in all_years:\n",
    "    for member in all_members:\n",
    "        potential_path = basepath.joinpath(f\"{member}-{year}.nc\")\n",
    "        if potential_path.is_file():\n",
    "            here.append(potential_path)\n",
    "        else:\n",
    "            not_here.append(potential_path)\n",
    "len(here)\n",
    "\n",
    "from itertools import groupby\n",
    "paths_to_load = []\n",
    "valid_ensembles = []\n",
    "for key, indices in groupby(range(len(parts)), lambda i: parts[i][0]):\n",
    "    indices = list(indices)\n",
    "    group = parts[indices]\n",
    "    these_paths = [paths[i] for i in indices]\n",
    "    years = np.asarray(list([g[1] for g in group]))\n",
    "    if len(years) == 60:\n",
    "        paths_to_load.append(these_paths)\n",
    "        valid_ensembles.append(key)\n",
    "    else:\n",
    "        print(key, len(years))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "ds = []\n",
    "for ptl in tqdm(paths_to_load):\n",
    "    ds_ = []\n",
    "    for p in ptl:\n",
    "        this = xr.open_dataset(p)\n",
    "        this = this.reset_coords(\"time_bnds\", drop=True).drop_dims(\"nbnd\")\n",
    "        ds_.append(this)\n",
    "    ds.append(xr.concat(ds_, dim=\"time\"))\n",
    "ds = xr.concat(ds, dim=\"member\")\n",
    "# ds = xr.concat([xr.concat([xr.open_dataset[ptl_] for ptl_ in ptl], dim=\"time\") for ptl in paths_to_load], dim=\"member\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import progress, Client\n",
    "from jetstream_hugo.definitions import COMPUTE_KWARGS\n",
    "client = Client(**COMPUTE_KWARGS)\n",
    "dask.persist(ds)\n",
    "progress(ds, notebook=False)\n",
    "ds = dask.compute(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds[0]\n",
    "to_comp = ds.to_zarr(f\"/storage/workspaces/giub_meteo_impacts/ci01/CESM2/flat_wind/ds.zarr\", compute=False, encoding={var: {\"chunks\": (-1, 100, -1, -1)} for var in ds.data_vars}, mode=\"w\")\n",
    "dask.persist(to_comp)\n",
    "progress(to_comp, notebook=False)\n",
    "dask.compute(to_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extreme cesm clim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from jetstream_hugo.data import *\n",
    "quantiles = ds[\"s\"].quantile(np.arange(0.6, 1, 0.05), [\"member\", \"lon\", \"lat\"]).compute()\n",
    "quantiles = smooth(quantiles, {\"time\": (\"win\", 15)}).load()\n",
    "quantiles.to_netcdf(f\"{DATADIR}/CESM2/flat_wind/results/s_q.nc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.dates import DateFormatter, MonthLocator\n",
    "from jetstream_hugo.definitions import *\n",
    "from scipy.stats import linregress\n",
    "\n",
    "def get_trend(da):\n",
    "    years = np.unique(da.time.dt.year)\n",
    "    if \"jet\" not in da.dims:\n",
    "        result = linregress(years, da.values)\n",
    "        return xr.Dataset({\"slope\": result.slope, \"p\": result.pvalue})\n",
    "    jets = da.jet.values\n",
    "    slopes = xr.DataArray(np.zeros(len(jets)), coords={\"jet\": jets})\n",
    "    pvalues = slopes.copy()\n",
    "    for j, jet in enumerate(jets):\n",
    "        result = linregress(years, da.isel(jet=j).values)\n",
    "        slopes[j] = result.slope\n",
    "        pvalues[j] = result.pvalue\n",
    "    return xr.Dataset({\"slope\": slopes, \"p\": pvalues})\n",
    "\n",
    "winsize = 15\n",
    "halfwinsize = int(np.ceil(winsize / 2))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for q, qval in zip(quantiles[::2], np.arange(0.6, 1, 0.05 * 2)):\n",
    "    gb = q.groupby(\"time.dayofyear\")\n",
    "    x = list(gb.groups)\n",
    "    x = DATERANGE[x]\n",
    "    ys = gb.map(get_trend) \n",
    "    ps = ys[\"p\"]\n",
    "    ys = ys[\"slope\"]\n",
    "    ys = ys.pad({\"dayofyear\": halfwinsize}, mode=\"wrap\")\n",
    "    ys = ys.rolling(dayofyear=winsize, center=True).mean()\n",
    "    ys = ys.isel({\"dayofyear\": slice(halfwinsize, -halfwinsize)})\n",
    "    ax.plot(x, ys, label=f\"$q={qval:.1f}$\", lw=2)\n",
    "ax.grid(True)\n",
    "ax.xaxis.set_major_locator(MonthLocator(range(0, 13, 3)))\n",
    "ax.xaxis.set_major_formatter(DateFormatter(\"1 %b\"))\n",
    "ax.set_xlim(min(x), max(x))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_clim = compute_clim(quantiles, \"dayofyear\")\n",
    "q_clim = smooth(q_clim, {\"dayofyear\": (\"win\", 61)}).load()\n",
    "q_clim.to_netcdf(f\"{DATADIR}/CESM2/flat_wind/results/s_q_clim.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.reset_coords(\"time_bnds\", drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extreme experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_tp = ExtremeExperiment(\n",
    "    DataHandler(\"ERA5\", \"surf\", \"tp\", \"6H\", \"all\", \"JJA\", -30, 40, 30, 75, 250, 'hourofyear', {'hourofyear': ('win', 4 * 15)}, None),\n",
    "    q = 0.95,\n",
    ")\n",
    "da_tp = exp_tp.da.load()\n",
    "\n",
    "data_handlers = {}\n",
    "for varname in [\"u\", \"v\", \"s\"]:\n",
    "    dh = DataHandler(\"ERA5\", \"plev\", varname, \"6H\", \"all\", None, -80, 40, 15, 80, [175, 200, 225, 250, 300, 350], reduce_da=False)\n",
    "    data_handlers[varname] = dh\n",
    "exp = MultiVarExperiment(data_handlers)\n",
    "\n",
    "all_jets_one_df, where_are_jets, all_jets_one_array, all_jets_over_time, flags = exp.track_jets()\n",
    "props_as_ds = exp.props_as_ds(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepdiff import DeepHash\n",
    "DeepHash(load_pickle(\"/storage/workspaces/giub_meteo_impacts/ci01/ERA5/surf/tp/6H/hourofyear_hourofyearwin60/results/1/predictions/1/metadata.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = [\"mean_lon\", \"mean_lat\", \"mean_lev\", \"spe_star\", \"width\", \"wavinessR16\", \"persistence\", \"com_speed\", \"int\"]\n",
    "predictors = prepare_predictors(\n",
    "    props_as_ds,\n",
    "    subset=subset,\n",
    "    anomalize=True,\n",
    "    normalize=True,\n",
    "    detrend=True,\n",
    "    nan_method=\"nearest\",\n",
    "    season=\"JJA\",\n",
    ")\n",
    "time_before = pd.Timedelta(0, \"D\")\n",
    "n_clu = 22\n",
    "clusters_da = exp_tp.spatial_clusters_as_da(n_clu)\n",
    "targets, length_targets, all_spells_ts, all_spells = exp_tp.create_targets(n_clu, 0.95, minlen=np.timedelta64(1, \"D\"))\n",
    "binary_targets = length_targets > 0\n",
    "masked_predictors = mask_from_spells_multi_region(predictors, targets, all_spells_ts, all_spells, time_before=time_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clu = Clusterplot(1, 1, exp_tp.region)\n",
    "cmap = colormaps.BlAqGrYeOrReVi200\n",
    "ax = clu.axes[0]\n",
    "unique_clusters = np.arange(n_clu)\n",
    "norm = BoundaryNorm(np.concatenate([[-1], unique_clusters]) + 0.5, cmap.N)\n",
    "clusters_da.unstack().plot(\n",
    "    ax=ax,\n",
    "    cmap=cmap,\n",
    "    norm=norm,\n",
    "    add_colorbar=False,\n",
    "    add_labels=False\n",
    ")\n",
    "for j in range(n_clu):\n",
    "    lo = clusters_da.lon.where(clusters_da==j).mean().item()\n",
    "    la = clusters_da.lat.where(clusters_da==j).mean().item()\n",
    "    ax.text(lo, la, f\"${j}$\", ha=\"center\", va=\"center\", fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_all_smoothed_anomalies(\"ERA5\", \"plev\", \"s\", \"6H\", 'hourofyear', {'hourofyear': ('win', 4 * 15)}, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_all_smoothed_anomalies(\"ERA5\", \"surf\", \"tp\", \"6H\", 'hourofyear', {'hourofyear': ('win', 4 * 15)}, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = Path(f\"{DATADIR}/ERA5/surf\")\n",
    "varnames = [\"u10\", \"v10\", \"s10\"]\n",
    "for year, month in tqdm(product(YEARS, range(1, 13)), total=len(YEARS) * 12):\n",
    "    month_str = str(month).zfill(2)\n",
    "    ofiles = {varname: basepath.joinpath(f\"{varname}/6H/{year}{month_str}.nc\") for varname in varnames}\n",
    "    if all([ofile.is_file() for ofile in ofiles.values()]):\n",
    "        continue\n",
    "    ds = xr.open_dataset(basepath.joinpath(f\"raw/{year}{month_str}.nc\"))\n",
    "    ds = ds.rename(longitude=\"lon\", latitude=\"lat\")\n",
    "    ds = ds.assign_coords(lon=(((ds.lon + 180) % 360) - 180))\n",
    "    ds = ds.sortby(\"lon\")\n",
    "    ds = ds.sortby(\"lat\")\n",
    "    ds[\"s10\"] = np.sqrt(ds[\"u10\"] ** 2 + ds[\"v10\"] ** 2)\n",
    "    for varname in varnames:\n",
    "        da = ds[varname]\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=xr.SerializationWarning)\n",
    "            da.to_netcdf(ofiles[varname])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env112",
   "language": "python",
   "name": "env112"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
