{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetstream_hugo.definitions import *\n",
    "from jetstream_hugo.plots import *\n",
    "from jetstream_hugo.data import *\n",
    "from jetstream_hugo.anyspell import *\n",
    "from jetstream_hugo.jet_finding import *\n",
    "from jetstream_hugo.clustering import *\n",
    "from scipy import signal\n",
    "\n",
    "import colormaps\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# full globe jet finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = xr.open_dataset(\"/storage/workspaces/giub_meteo_impacts/ci01/ERA5/plev/high_wind/6H/1959.nc\")\n",
    "# dh = DataHandler(ds, \"/storage/workspaces/giub_meteo_impacts/ci01/ERA5/plev/high_wind/6H/results\")\n",
    "# exp = JetFindingExperiment(dh)\n",
    "all_jets = pl.read_parquet(\"/storage/workspaces/giub_meteo_impacts/ci01/ERA5/plev/high_wind/6H/results/3/all_jets_one_df2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = extract_features(all_jets.filter(pl.col(\"lat\") > 0, pl.col(\"lon\") > -80, pl.col(\"lon\") < 100), [\"lon\", \"lat\", \"lev\", \"s\", \"theta\", \"s_low\", \"ratio\", \"EKE\"], season=\"JJA\")\n",
    "Xp = X.with_columns(pl.col(\"EKE\").clip(0, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = extract_features(all_jets.filter(pl.col(\"lat\") > 15, pl.col(\"lat\") < 80), [\"lon\", \"lat\", \"lev\", \"s\", \"theta\", \"s_low\", \"ratio\"], season=\"JJA\")\n",
    "X = X.with_columns(pl.col(\"ratio\").clip(0, 0.75))\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8), tight_layout=True)\n",
    "axes = axes.ravel()\n",
    "cmap = colormaps.matter\n",
    "kwargs = {\"cmap\": cmap, \"gridsize\": 31}\n",
    "pairs = [[\"lon\", \"lat\"], [\"lat\", \"lev\"], [\"s\", \"theta\"], [\"s\", \"lev\"], [\"ratio\", \"theta\"], [\"ratio\", \"lat\"]]\n",
    "for ax, pair in zip(axes, pairs):\n",
    "    ax.hexbin(X[pair[0]], X[pair[1]], **kwargs)\n",
    "    ax.set_xlabel(pair[0])\n",
    "    ax.set_ylabel(pair[1])\n",
    "    if pair[0] in [\"lev\", \"theta\"]:\n",
    "        ax.invert_xaxis()\n",
    "    elif pair[1] in [\"lev\", \"theta\"]:\n",
    "        ax.invert_yaxis() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = extract_features(all_jets.filter(pl.col(\"lat\") > 15, pl.col(\"lat\") < 80), [\"lon\", \"lat\", \"lev\", \"s\", \"theta\", \"s_low\", \"ratio\"], season=\"JJA\")\n",
    "is_polar = one_gmix(X[[\"lat\", \"theta\", \"ratio\"]])\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8), tight_layout=True)\n",
    "axes = axes.ravel()\n",
    "cmap = colormaps.matter\n",
    "kwargs = {\"cmap\": colormaps.balance, \"gridsize\": 31, \"C\": is_polar}\n",
    "pairs = [[\"lon\", \"lat\"], [\"lat\", \"lev\"], [\"s\", \"theta\"], [\"s\", \"lev\"], [\"ratio\", \"theta\"], [\"ratio\", \"lat\"]]\n",
    "for ax, pair in zip(axes, pairs):\n",
    "    im = ax.hexbin(X[pair[0]], X[pair[1]], **kwargs)\n",
    "    ax.set_xlabel(pair[0])\n",
    "    ax.set_ylabel(pair[1])\n",
    "    if pair[0] in [\"lev\", \"theta\"]:\n",
    "        ax.invert_xaxis()\n",
    "    elif pair[1] in [\"lev\", \"theta\"]:\n",
    "        ax.invert_yaxis() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = extract_features(all_jets.filter(pl.col(\"lat\") > 15, pl.col(\"lat\") < 80), [\"lon\", \"lat\", \"lev\", \"s\", \"theta\", \"s_low\", \"ratio\"], season=\"SON\")\n",
    "X = X.with_columns(pl.col(\"ratio\").clip(0, 0.75))\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8), tight_layout=True)\n",
    "axes = axes.ravel()\n",
    "cmap = colormaps.matter\n",
    "kwargs = {\"cmap\": cmap, \"gridsize\": 31}\n",
    "pairs = [[\"lon\", \"lat\"], [\"lat\", \"lev\"], [\"s\", \"theta\"], [\"s\", \"lev\"], [\"ratio\", \"theta\"], [\"ratio\", \"lat\"]]\n",
    "for ax, pair in zip(axes, pairs):\n",
    "    ax.hexbin(X[pair[0]], X[pair[1]], **kwargs)\n",
    "    ax.set_xlabel(pair[0])\n",
    "    ax.set_ylabel(pair[1])\n",
    "    if pair[0] in [\"lev\", \"theta\"]:\n",
    "        ax.invert_xaxis()\n",
    "    elif pair[1] in [\"lev\", \"theta\"]:\n",
    "        ax.invert_yaxis() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = extract_features(all_jets.filter(pl.col(\"lat\") > 15, pl.col(\"lat\") < 80), [\"lon\", \"lat\", \"lev\", \"s\", \"theta\", \"s_low\", \"ratio\"], season=\"DJF\")\n",
    "X = X.with_columns(pl.col(\"ratio\").clip(0, 0.75))\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8), tight_layout=True)\n",
    "axes = axes.ravel()\n",
    "cmap = colormaps.matter\n",
    "kwargs = {\"cmap\": cmap, \"gridsize\": 31}\n",
    "pairs = [[\"lon\", \"lat\"], [\"lat\", \"lev\"], [\"s\", \"theta\"], [\"s\", \"lev\"], [\"ratio\", \"theta\"], [\"ratio\", \"lat\"]]\n",
    "for ax, pair in zip(axes, pairs):\n",
    "    ax.hexbin(X[pair[0]], X[pair[1]], **kwargs)\n",
    "    ax.set_xlabel(pair[0])\n",
    "    ax.set_ylabel(pair[1])\n",
    "    if pair[0] in [\"lev\", \"theta\"]:\n",
    "        ax.invert_xaxis()\n",
    "    elif pair[1] in [\"lev\", \"theta\"]:\n",
    "        ax.invert_yaxis() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = extract_features(all_jets.filter(pl.col(\"lat\") > 15, pl.col(\"lat\") < 80), [\"lon\", \"lat\", \"lev\", \"s\", \"theta\", \"s_low\", \"ratio\"], season=\"MAM\")\n",
    "X = X.with_columns(pl.col(\"ratio\").clip(0, 0.75))\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8), tight_layout=True)\n",
    "axes = axes.ravel()\n",
    "cmap = colormaps.matter\n",
    "kwargs = {\"cmap\": cmap, \"gridsize\": 31}\n",
    "pairs = [[\"lon\", \"lat\"], [\"lat\", \"lev\"], [\"s\", \"theta\"], [\"s\", \"lev\"], [\"ratio\", \"theta\"], [\"ratio\", \"lat\"]]\n",
    "for ax, pair in zip(axes, pairs):\n",
    "    ax.hexbin(X[pair[0]], X[pair[1]], **kwargs)\n",
    "    ax.set_xlabel(pair[0])\n",
    "    ax.set_ylabel(pair[1])\n",
    "    if pair[0] in [\"lev\", \"theta\"]:\n",
    "        ax.invert_xaxis()\n",
    "    elif pair[1] in [\"lev\", \"theta\"]:\n",
    "        ax.invert_yaxis() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12), tight_layout=True)\n",
    "axes = axes.ravel()\n",
    "cmap = colormaps.matter\n",
    "kwargs = {\"cmap\": cmap, \"gridsize\": 25}\n",
    "pair = [\"ratio\", \"theta\"]\n",
    "for ax, season in zip(axes, SEASONS):\n",
    "    Xp= extract_features(all_jets.filter(pl.col(\"lat\") > 0, pl.col(\"lon\") > -80, pl.col(\"lon\") < 100), [\"lon\", \"lat\", \"lev\", \"s\", \"theta\", \"s_low\", \"ratio\", \"EKE\"], season=season)\n",
    "    ax.hexbin(Xp[pair[0]], Xp[pair[1]], **kwargs)\n",
    "    ax.set_xlabel(pair[0])\n",
    "    ax.set_ylabel(pair[1])\n",
    "    if pair[0] in [\"lev\", \"theta\"]:\n",
    "        ax.invert_xaxis()\n",
    "    elif pair[1] in [\"lev\", \"theta\"]:\n",
    "        ax.invert_yaxis() \n",
    "    ax.set_title(season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clu = Clusterplot(1, 1, get_region(ds))\n",
    "it = np.random.choice(len(ds.time))\n",
    "clu.add_contourf([ds[\"s\"][it]], cmap=colormaps.greys, levels=7)\n",
    "jets = all_jets.filter(pl.col(\"time\") == ds[\"time\"][it]) \n",
    "norm = Normalize(all_jets[\"theta\"].quantile(0.05), all_jets[\"theta\"].quantile(0.95))\n",
    "for indexer, jet in jets.group_by(\"jet ID\"):\n",
    "    lon, lat, theta = jet[[\"lon\", \"lat\", \"theta\"]].to_numpy().T\n",
    "    plt.scatter(lon, lat, c=theta, lw=1, s=10, norm=norm, cmap=colormaps.thermal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# arco-era5 tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_zarr(\n",
    "    \"gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3\",\n",
    "    chunks=None,\n",
    "    storage_options=dict(token=\"anon\"),\n",
    ")\n",
    "ar_full_37_1h = ds.sel(\n",
    "    time=slice(ds.attrs[\"valid_time_start\"], ds.attrs[\"valid_time_stop\"])\n",
    ")\n",
    "\n",
    "base_ds = (\n",
    "    ar_full_37_1h[[\"u_component_of_wind\", \"v_component_of_wind\"]]\n",
    "    .sel(\n",
    "        time=ar_full_37_1h.time.dt.hour % 6 == 0,\n",
    "        latitude=ar_full_37_1h.latitude >= 0,\n",
    "        level=[175, 200, 225, 250, 300, 350],\n",
    "    )\n",
    "    .isel(longitude=slice(None, None, 2), latitude=slice(None, None, 2))\n",
    ")\n",
    "\n",
    "base_path = Path(f\"{DATADIR}/ERA5/plev/flat_wind/dailymean\")\n",
    "for year in YEARS:\n",
    "    for month in range(1, 13):\n",
    "        month_str = str(month).zfill(2)\n",
    "        opath = base_path.joinpath(f\"{year}{month_str}.nc\")\n",
    "        if opath.is_file():\n",
    "            continue\n",
    "        ds = compute(\n",
    "            base_ds.sel(\n",
    "                time=(base_ds.time.dt.year == year) & (base_ds.time.dt.month == month)\n",
    "            ),\n",
    "            progress=True,\n",
    "        )\n",
    "        ds = standardize(ds)\n",
    "        ds[\"s\"] = np.sqrt(ds[\"u\"] ** 2 + ds[\"v\"] ** 2)\n",
    "        ds = flatten_by(ds, \"s\")\n",
    "        ds.to_netcdf(opath)\n",
    "        print(f\"Completed {year}{month}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_zarr(\n",
    "    \"gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3\",\n",
    "    chunks=None,\n",
    "    storage_options=dict(token=\"anon\"),\n",
    ")\n",
    "ar_full_37_1h = ds.sel(\n",
    "    time=slice(ds.attrs[\"valid_time_start\"], ds.attrs[\"valid_time_stop\"])\n",
    ")\n",
    "\n",
    "temp_full = (\n",
    "    ar_full_37_1h[\"temperature\"]\n",
    "    .sel(\n",
    "        time=ar_full_37_1h.time.dt.hour % 6 == 0,\n",
    "        latitude=ar_full_37_1h.latitude >= 0,\n",
    "        level=[175, 200, 225, 250, 300, 350],\n",
    "    )\n",
    "    .isel(longitude=slice(None, None, 2), latitude=slice(None, None, 2))\n",
    ")\n",
    "\n",
    "temp_full = standardize(temp_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_zarr(\n",
    "    \"gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3\",\n",
    "    chunks=None,\n",
    "    storage_options=dict(token=\"anon\"),\n",
    ")\n",
    "ar_full_37_1h = ds.sel(\n",
    "    time=slice(ds.attrs[\"valid_time_start\"], ds.attrs[\"valid_time_stop\"])\n",
    ")\n",
    "\n",
    "temp_full = (\n",
    "    ar_full_37_1h[\"temperature\"]\n",
    "    .sel(\n",
    "        time=ar_full_37_1h.time.dt.hour % 6 == 0,\n",
    "        latitude=ar_full_37_1h.latitude >= 0,\n",
    "        level=[175, 200, 225, 250, 300, 350],\n",
    "    )\n",
    "    .isel(longitude=slice(None, None, 2), latitude=slice(None, None, 2))\n",
    ")\n",
    "\n",
    "temp_full = standardize(temp_full)\n",
    "\n",
    "orig_path = Path(f\"{DATADIR}/ERA5/plev/flat_wind/dailymean\")\n",
    "base_path = Path(f\"{DATADIR}/ERA5/plev/flat_wind/dailymean_2\")\n",
    "for year in tqdm(YEARS):\n",
    "    for month in trange(1, 13, leave=False):\n",
    "        month_str = str(month).zfill(2)\n",
    "        opath = base_path.joinpath(f\"{year}{month_str}.nc\")\n",
    "        if opath.is_file():\n",
    "            continue\n",
    "        ipath = orig_path.joinpath(f\"{year}{month_str}.nc\")\n",
    "        ds = xr.open_dataset(ipath)\n",
    "        this_temp = temp_full.sel(time=ds.time.values, lev=ds[\"lev\"])\n",
    "        this_temp = this_temp * (1000 / this_temp.lev) ** KAPPA\n",
    "        this_temp = this_temp.reset_coords(\"lev\", drop=True)\n",
    "        ds[\"theta\"] = compute(this_temp, progress_flag=True)\n",
    "        ds.to_netcdf(opath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climatologies, datahandlers of new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_all_smoothed_anomalies(\"ERA5\", \"surf\", \"t2m\", \"dailymean\", 'dayofyear', {'dayofyear': ('win', 15)}, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_all_smoothed_anomalies(\"ERA5\", \"surf\", \"tp\", \"dailymean\", 'dayofyear', {'dayofyear': ('win', 15)}, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_all_smoothed_anomalies(\"ERA5\", \"surf\", \"sst\", \"dailymean\", 'dayofyear', {'dayofyear': ('win', 15)}, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_all_smoothed_anomalies(\"ERA5\", \"surf\", \"mslp\", \"dailymean\", 'dayofyear', {'dayofyear': ('win', 15)}, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_all_smoothed_anomalies(\"ERA5\", \"thetalev\", \"apvs\", \"dailymean\", 'dayofyear', {'dayofyear': ('win', 15)}, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_all_smoothed_anomalies(\"ERA5\", \"thetalev\", \"cpvs\", \"dailymean\", 'dayofyear', {'dayofyear': ('win', 15)}, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in tqdm(YEARS):\n",
    "    da = open_da(\"ERA5\", \"thetalev\", \"apvs\", \"6H\", period=[year])\n",
    "    da = compute(da.chunk({\"lev\": 1}).resample(time=\"1d\").mean(), progress_flag=True)\n",
    "    da.to_netcdf(data_path(\"ERA5\", \"thetalev\", \"apvs\", \"dailymean\").joinpath(f\"{year}.nc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in tqdm(YEARS):\n",
    "    da = open_da(\"ERA5\", \"plev\", \"z\", \"6H\", period=[year])\n",
    "    da = compute(da.chunk({\"lev\": 1}).resample(time=\"1d\").mean(), progress_flag=True)\n",
    "    da.to_netcdf(data_path(\"ERA5\", \"plev\", \"z\", \"dailymean\").joinpath(f\"{year}.nc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_all_smoothed_anomalies(\"ERA5\", \"plev\", \"z\", \"dailymean\", 'dayofyear', {'dayofyear': ('win', 15)}, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in tqdm(YEARS):\n",
    "    da = open_da(\"ERA5\", \"thetalev\", \"cpvs\", \"6H\", period=[year])\n",
    "    da = compute(da.chunk({\"lev\": 1}).resample(time=\"1d\").mean(), progress_flag=False)\n",
    "    da.to_netcdf(data_path(\"ERA5\", \"thetalev\", \"cpvs\", \"dailymean\").joinpath(f\"{year}.nc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_ = compute(da.sel(lev=350), progress_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_ = pl.from_pandas(da_.to_dataframe().reset_index()).cast({\"lat\": pl.Float32, \"lon\": pl.Float32, \"lev\": pl.UInt16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_.group_by(pl.col(\"time\").dt.ordinal_day(), pl.col(\"lat\"), pl.col(\"lon\")).agg(pl.col(\"apvs\").mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_all_smoothed_anomalies(\"ERA5\", \"thetalev\", \"apvs\", \"6H\", 'hourofyear', {'hourofyear': ('win', 60)}, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_all_smoothed_anomalies(\"ERA5\", \"thetalev\", \"cpvs\", \"6H\", 'hourofyear', {'hourofyear': ('win', 60)}, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new pvs das"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars_st as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_xarray(events: st.GeoDataFrame, dummy_da: xr.DataArray, varname: str):\n",
    "    dummy_da = dummy_da.rename(\"dummy\")\n",
    "    da_df = pl.from_pandas(dummy_da.to_dataframe().reset_index())\n",
    "    orig_times = da_df[\"time\"]\n",
    "    timedtype = orig_times.dtype\n",
    "    da_df = da_df.drop(\"time\", \"dummy\")\n",
    "    da_df = da_df.unique([\"lat\", \"lon\"]).sort([\"lat\", \"lon\"]).with_columns(geometry=st.from_xy(\"lon\", \"lat\"))\n",
    "    da_df = st.GeoDataFrame(da_df)\n",
    "    events = events.with_columns(pl.col(\"geometry\").st.buffer(0.25))\n",
    "    events = (\n",
    "        events.select([\"date\", \"geometry\", varname])\n",
    "        .cast({varname: pl.Float32})\n",
    "        .rename({\"date\": \"time\"})\n",
    "    )\n",
    "    events = events.cast({\"time\": timedtype})\n",
    "    events = events.filter(pl.col(\"time\").is_in(orig_times))\n",
    "    dummy_da = xr.zeros_like(dummy_da, dtype=np.float32)\n",
    "    events = events.st.sjoin(da_df, on=\"geometry\", how=\"inner\", predicate=\"contains\")\n",
    "    events = events.unique([\"time\", \"lon\", \"lat\"])\n",
    "    events_da = xr.DataArray.from_series(\n",
    "        events[[\"time\", varname, \"lat\", \"lon\"]]\n",
    "        .to_pandas()\n",
    "        .set_index([\"time\", \"lat\", \"lon\"])[varname]\n",
    "    ).fillna(0)\n",
    "    dummy_da.loc[\n",
    "        {\n",
    "            \"time\": events_da.time.values,\n",
    "            \"lat\": events_da.lat.values,\n",
    "            \"lon\": events_da.lon.values,\n",
    "        }\n",
    "    ] = events_da\n",
    "    return dummy_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events = {}\n",
    "for level in trange(310, 365, 5):\n",
    "    events = st.from_geopandas(gpd.read_parquet(f\"/storage/workspaces/giub_meteo_impacts/ci01/ERA5/RWB_index/era5_pv_streamers_{level}K_1959-2022.parquet\"))\n",
    "\n",
    "    tropospheric = events.filter(pl.col(\"mean_var\") < pl.col(\"level\"))\n",
    "    anticyclonic = tropospheric.filter(pl.col(\"intensity\") >= pl.col(\"level\"))\n",
    "    cyclonic = tropospheric.filter(pl.col(\"intensity\") < pl.col(\"level\"))\n",
    "    \n",
    "    all_events[level] = {\"anti\": anticyclonic, \"cycl\": cyclonic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in YEARS:\n",
    "    # for month in range(1, 13):\n",
    "    ofile_anti = Path(f\"{DATADIR}/ERA5/thetalev/apvs/6H/{year}.nc\")\n",
    "    ofile_cycl = Path(f\"{DATADIR}/ERA5/thetalev/cpvs/6H/{year}.nc\")\n",
    "    if ofile_cycl.is_file() and ofile_anti.is_file():\n",
    "        continue\n",
    "    time_mask = (TIMERANGE.year == year)# & (TIMERANGE.month == month)\n",
    "    coords = {\n",
    "        \"time\": TIMERANGE[time_mask],\n",
    "        \"lat\": np.arange(0, 90.5, .5),\n",
    "        \"lon\": np.arange(-180, 180, .5),\n",
    "    }\n",
    "    shape = [len(co) for co in coords.values()]\n",
    "    dummy_da = xr.DataArray(np.zeros(shape), coords=coords)\n",
    "    anti_all_levs = {}\n",
    "    cycl_all_levs = {}\n",
    "    for lev, events in tqdm(all_events.items(), total=11):\n",
    "        anti_all_levs[lev] = to_xarray(events[\"anti\"], dummy_da, \"intensity\")\n",
    "        cycl_all_levs[lev] = to_xarray(events[\"cycl\"], dummy_da, \"intensity\")\n",
    "    anti_all_levs = xr.concat(anti_all_levs.values(), dim=\"lev\").assign_coords(lev=list(anti_all_levs))\n",
    "    cycl_all_levs = xr.concat(cycl_all_levs.values(), dim=\"lev\").assign_coords(lev=list(cycl_all_levs))\n",
    "    anti_all_levs.to_netcdf(ofile_anti)\n",
    "    cycl_all_levs.to_netcdf(ofile_cycl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CESM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "basepath = Path(\"/storage/workspaces/giub_meteo_impacts/ci01/CESM2/flat_wind\")\n",
    "paths = list(basepath.iterdir())\n",
    "paths = [path for path in paths if path.suffix == \".nc\" and path.name != \"ds.nc\"]\n",
    "parts = [path.name.split(\".\")[0].split(\"-\") for path in paths]\n",
    "parts = np.asarray(parts)\n",
    "sorted_order = np.argsort([memb.replace(\"r10\", \"r0\") for memb in parts[:, 0]])\n",
    "parts = parts[sorted_order]\n",
    "paths = [paths[i] for i in sorted_order]\n",
    "all_members = np.unique(parts[:, 0])\n",
    "all_years = np.unique(parts[:, 1])\n",
    "\n",
    "not_here = []\n",
    "here = []\n",
    "for year in all_years:\n",
    "    for member in all_members:\n",
    "        potential_path = basepath.joinpath(f\"{member}-{year}.nc\")\n",
    "        if potential_path.is_file():\n",
    "            here.append(potential_path)\n",
    "        else:\n",
    "            not_here.append(potential_path)\n",
    "len(here)\n",
    "\n",
    "from itertools import groupby\n",
    "paths_to_load = []\n",
    "valid_ensembles = []\n",
    "for key, indices in groupby(range(len(parts)), lambda i: parts[i][0]):\n",
    "    indices = list(indices)\n",
    "    group = parts[indices]\n",
    "    these_paths = [paths[i] for i in indices]\n",
    "    years = np.asarray(list([g[1] for g in group]))\n",
    "    if len(years) == 60:\n",
    "        paths_to_load.append(these_paths)\n",
    "        valid_ensembles.append(key)\n",
    "    else:\n",
    "        print(key, len(years))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "ds = []\n",
    "for ptl in tqdm(paths_to_load):\n",
    "    ds_ = []\n",
    "    for p in ptl:\n",
    "        this = xr.open_dataset(p)\n",
    "        this = this.reset_coords(\"time_bnds\", drop=True).drop_dims(\"nbnd\")\n",
    "        ds_.append(this)\n",
    "    ds.append(xr.concat(ds_, dim=\"time\"))\n",
    "ds = xr.concat(ds, dim=\"member\")\n",
    "# ds = xr.concat([xr.concat([xr.open_dataset[ptl_] for ptl_ in ptl], dim=\"time\") for ptl in paths_to_load], dim=\"member\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import progress, Client\n",
    "from jetstream_hugo.definitions import COMPUTE_KWARGS\n",
    "client = Client(**COMPUTE_KWARGS)\n",
    "dask.persist(ds)\n",
    "progress(ds, notebook=False)\n",
    "ds = dask.compute(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds[0]\n",
    "to_comp = ds.to_zarr(f\"/storage/workspaces/giub_meteo_impacts/ci01/CESM2/flat_wind/ds.zarr\", compute=False, encoding={var: {\"chunks\": (-1, 100, -1, -1)} for var in ds.data_vars}, mode=\"w\")\n",
    "dask.persist(to_comp)\n",
    "progress(to_comp, notebook=False)\n",
    "dask.compute(to_comp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env112",
   "language": "python",
   "name": "env112"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
