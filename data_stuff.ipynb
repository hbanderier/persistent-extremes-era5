{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetstream_hugo.definitions import *\n",
    "from jetstream_hugo.plots import *\n",
    "from jetstream_hugo.data import *\n",
    "# from jetstream_hugo.anyspell import *\n",
    "from jetstream_hugo.jet_finding import *\n",
    "from jetstream_hugo.clustering import *\n",
    "\n",
    "import colormaps\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# arco-era5 tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_zarr(\n",
    "    \"gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3\",\n",
    "    chunks=None,\n",
    "    storage_options=dict(token=\"anon\"),\n",
    ")\n",
    "ar_full_37_1h = ds.sel(\n",
    "    time=slice(ds.attrs[\"valid_time_start\"], ds.attrs[\"valid_time_stop\"])\n",
    ")\n",
    "\n",
    "base_ds = (\n",
    "    ar_full_37_1h[[\"u_component_of_wind\", \"v_component_of_wind\"]]\n",
    "    .sel(\n",
    "        time=ar_full_37_1h.time.dt.hour % 6 == 0,\n",
    "        latitude=ar_full_37_1h.latitude >= 0,\n",
    "        level=[175, 200, 225, 250, 300, 350],\n",
    "    )\n",
    "    .isel(longitude=slice(None, None, 2), latitude=slice(None, None, 2))\n",
    ")\n",
    "\n",
    "base_path = Path(f\"{DATADIR}/ERA5/plev/flat_wind/dailymean\")\n",
    "for year in YEARS:\n",
    "    for month in range(1, 13):\n",
    "        month_str = str(month).zfill(2)\n",
    "        opath = base_path.joinpath(f\"{year}{month_str}.nc\")\n",
    "        if opath.is_file():\n",
    "            continue\n",
    "        ds = compute(\n",
    "            base_ds.sel(\n",
    "                time=(base_ds.time.dt.year == year) & (base_ds.time.dt.month == month)\n",
    "            ),\n",
    "            progress=True,\n",
    "        )\n",
    "        ds = standardize(ds)\n",
    "        ds[\"s\"] = np.sqrt(ds[\"u\"] ** 2 + ds[\"v\"] ** 2)\n",
    "        ds = flatten_by(ds, \"s\")\n",
    "        ds.to_netcdf(opath)\n",
    "        print(f\"Completed {year}{month}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_zarr(\n",
    "    \"gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3\",\n",
    "    chunks=None,\n",
    "    storage_options=dict(token=\"anon\"),\n",
    ")\n",
    "ar_full_37_1h = ds.sel(\n",
    "    time=slice(ds.attrs[\"valid_time_start\"], ds.attrs[\"valid_time_stop\"])\n",
    ")\n",
    "\n",
    "temp_full = (\n",
    "    ar_full_37_1h[\"temperature\"]\n",
    "    .sel(\n",
    "        time=ar_full_37_1h.time.dt.hour % 6 == 0,\n",
    "        latitude=ar_full_37_1h.latitude >= 0,\n",
    "        level=[175, 200, 225, 250, 300, 350],\n",
    "    )\n",
    "    .isel(longitude=slice(None, None, 2), latitude=slice(None, None, 2))\n",
    ")\n",
    "\n",
    "temp_full = standardize(temp_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_zarr(\n",
    "    \"gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3\",\n",
    "    chunks=None,\n",
    "    storage_options=dict(token=\"anon\"),\n",
    ")\n",
    "ar_full_37_1h = ds.sel(\n",
    "    time=slice(ds.attrs[\"valid_time_start\"], ds.attrs[\"valid_time_stop\"])\n",
    ")\n",
    "\n",
    "temp_full = (\n",
    "    ar_full_37_1h[\"temperature\"]\n",
    "    .sel(\n",
    "        time=ar_full_37_1h.time.dt.hour % 6 == 0,\n",
    "        latitude=ar_full_37_1h.latitude >= 0,\n",
    "        level=[175, 200, 225, 250, 300, 350],\n",
    "    )\n",
    "    .isel(longitude=slice(None, None, 2), latitude=slice(None, None, 2))\n",
    ")\n",
    "\n",
    "temp_full = standardize(temp_full)\n",
    "\n",
    "orig_path = Path(f\"{DATADIR}/ERA5/plev/flat_wind/dailymean\")\n",
    "base_path = Path(f\"{DATADIR}/ERA5/plev/flat_wind/dailymean_2\")\n",
    "for year in tqdm(YEARS):\n",
    "    for month in trange(1, 13, leave=False):\n",
    "        month_str = str(month).zfill(2)\n",
    "        opath = base_path.joinpath(f\"{year}{month_str}.nc\")\n",
    "        if opath.is_file():\n",
    "            continue\n",
    "        ipath = orig_path.joinpath(f\"{year}{month_str}.nc\")\n",
    "        ds = xr.open_dataset(ipath)\n",
    "        this_temp = temp_full.sel(time=ds.time.values, lev=ds[\"lev\"])\n",
    "        this_temp = this_temp * (1000 / this_temp.lev) ** KAPPA\n",
    "        this_temp = this_temp.reset_coords(\"lev\", drop=True)\n",
    "        ds[\"theta\"] = compute(this_temp, progress_flag=True)\n",
    "        ds.to_netcdf(opath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new pvs das"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars_st as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_xarray(events: st.GeoDataFrame, dummy_da: xr.DataArray, varname: str):\n",
    "    dummy_da = dummy_da.rename(\"dummy\")\n",
    "    da_df = pl.from_pandas(dummy_da.to_dataframe().reset_index())\n",
    "    orig_times = da_df[\"time\"]\n",
    "    timedtype = orig_times.dtype\n",
    "    da_df = da_df.drop(\"time\", \"dummy\")\n",
    "    da_df = da_df.unique([\"lat\", \"lon\"]).sort([\"lat\", \"lon\"]).with_columns(geometry=st.from_xy(\"lon\", \"lat\"))\n",
    "    da_df = st.GeoDataFrame(da_df)\n",
    "    events = events.with_columns(pl.col(\"geometry\").st.buffer(0.25))\n",
    "    events = (\n",
    "        events.select([\"date\", \"geometry\", varname])\n",
    "        .cast({varname: pl.Float32})\n",
    "        .rename({\"date\": \"time\"})\n",
    "    )\n",
    "    events = events.cast({\"time\": timedtype})\n",
    "    events = events.filter(pl.col(\"time\").is_in(orig_times))\n",
    "    dummy_da = xr.zeros_like(dummy_da, dtype=np.float32)\n",
    "    events = events.st.sjoin(da_df, on=\"geometry\", how=\"inner\", predicate=\"contains\")\n",
    "    events = events.unique([\"time\", \"lon\", \"lat\"])\n",
    "    events_da = xr.DataArray.from_series(\n",
    "        events[[\"time\", varname, \"lat\", \"lon\"]]\n",
    "        .to_pandas()\n",
    "        .set_index([\"time\", \"lat\", \"lon\"])[varname]\n",
    "    ).fillna(0)\n",
    "    dummy_da.loc[\n",
    "        {\n",
    "            \"time\": events_da.time.values,\n",
    "            \"lat\": events_da.lat.values,\n",
    "            \"lon\": events_da.lon.values,\n",
    "        }\n",
    "    ] = events_da\n",
    "    return dummy_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events = {}\n",
    "for level in trange(310, 365, 5):\n",
    "    events = st.from_geopandas(gpd.read_parquet(f\"/storage/workspaces/giub_meteo_impacts/ci01/ERA5/RWB_index/era5_pv_streamers_{level}K_1959-2022.parquet\"))\n",
    "\n",
    "    tropospheric = events.filter(pl.col(\"mean_var\") < pl.col(\"level\"))\n",
    "    anticyclonic = tropospheric.filter(pl.col(\"intensity\") >= pl.col(\"level\"))\n",
    "    cyclonic = tropospheric.filter(pl.col(\"intensity\") < pl.col(\"level\"))\n",
    "    \n",
    "    all_events[level] = {\"anti\": anticyclonic, \"cycl\": cyclonic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in YEARS:\n",
    "    # for month in range(1, 13):\n",
    "    ofile_anti = Path(f\"{DATADIR}/ERA5/thetalev/apvs/6H/{year}.nc\")\n",
    "    ofile_cycl = Path(f\"{DATADIR}/ERA5/thetalev/cpvs/6H/{year}.nc\")\n",
    "    if ofile_cycl.is_file() and ofile_anti.is_file():\n",
    "        continue\n",
    "    time_mask = (TIMERANGE.year == year)# & (TIMERANGE.month == month)\n",
    "    coords = {\n",
    "        \"time\": TIMERANGE[time_mask],\n",
    "        \"lat\": np.arange(0, 90.5, .5),\n",
    "        \"lon\": np.arange(-180, 180, .5),\n",
    "    }\n",
    "    shape = [len(co) for co in coords.values()]\n",
    "    dummy_da = xr.DataArray(np.zeros(shape), coords=coords)\n",
    "    anti_all_levs = {}\n",
    "    cycl_all_levs = {}\n",
    "    for lev, events in tqdm(all_events.items(), total=11):\n",
    "        anti_all_levs[lev] = to_xarray(events[\"anti\"], dummy_da, \"intensity\")\n",
    "        cycl_all_levs[lev] = to_xarray(events[\"cycl\"], dummy_da, \"intensity\")\n",
    "    anti_all_levs = xr.concat(anti_all_levs.values(), dim=\"lev\").assign_coords(lev=list(anti_all_levs))\n",
    "    cycl_all_levs = xr.concat(cycl_all_levs.values(), dim=\"lev\").assign_coords(lev=list(cycl_all_levs))\n",
    "    anti_all_levs.to_netcdf(ofile_anti)\n",
    "    cycl_all_levs.to_netcdf(ofile_cycl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CESM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'component.experiment.frequency.forcing_variant'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:09&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/homefs/hb22g102/jetstream_hugo/src/jetstream_hugo/data.py:137: RuntimeWarning: Converting a CFTimeIndex with dates from a non-standard calendar, 'noleap', to a pandas.DatetimeIndex, which uses dates from the standard calendar.  This may lead to subtle errors in operations that depend on the length of time between dates.\n",
      "  da[\"time\"] = da.indexes[\"time\"].to_datetimeindex()\n"
     ]
    }
   ],
   "source": [
    "from jetstream_hugo.definitions import *\n",
    "from jetstream_hugo.data import *\n",
    "import intake\n",
    "url = 'https://raw.githubusercontent.com/NCAR/cesm2-le-aws/main/intake-catalogs/aws-cesm2-le.json'\n",
    "varname = \"TS\"\n",
    "varname_to_search = varname\n",
    "component = \"atm\"\n",
    "# experiment = \"historical\"\n",
    "experiment = \"ssp370\"\n",
    "frequency = \"daily\"\n",
    "forcing_variant = \"cmip6\"\n",
    "# period = (1980, 2009)\n",
    "period = (2070, 2099)\n",
    "season = None\n",
    "minlon = -180\n",
    "maxlon = 180\n",
    "minlat = 0\n",
    "maxlat = 90\n",
    "levels = list(range(13, 19))\n",
    "members = \"all\"\n",
    "reduce_da = True\n",
    "\n",
    "indexers = [component, experiment, frequency, forcing_variant]\n",
    "indexers = [np.atleast_1d(indexer) for indexer in indexers]\n",
    "indexers = [[str(idx_) for idx_ in indexer] for indexer in indexers]\n",
    "indexers = list(product(*indexers))\n",
    "ensemble_keys = [\".\".join(indexer) for indexer in indexers]\n",
    "\n",
    "basepath = Path(f\"{DATADIR}/CESM2/{varname}\")\n",
    "catalog = intake.open_esm_datastore(url)\n",
    "catalog_subset = catalog.search(\n",
    "    variable=varname_to_search,\n",
    "    component=component,\n",
    "    experiment=experiment,\n",
    "    frequency=frequency,\n",
    "    forcing_variant=forcing_variant,\n",
    ")\n",
    "dsets = catalog_subset.to_dataset_dict(\n",
    "    xarray_open_kwargs={\"consolidated\": False},\n",
    "    storage_options={\"anon\": True}\n",
    ")\n",
    "ds = dsets[ensemble_keys[0]]\n",
    "\n",
    "ds = standardize(ds)\n",
    "\n",
    "da = extract(\n",
    "    ds,\n",
    "    period=period,\n",
    "    season=season,\n",
    "    minlon=minlon,\n",
    "    maxlon=maxlon,\n",
    "    minlat=minlat,\n",
    "    maxlat=maxlat,\n",
    "    # levels=levels,\n",
    "    # members=members,\n",
    ")[varname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 12m 3ss\n"
     ]
    }
   ],
   "source": [
    "da = compute(da, progress_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x7f243135eac0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da.to_zarr(\"/storage/workspaces/giub_meteo_impacts/ci01/CESM2/TS/past.zarr\", mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "basepath = Path(\"/storage/workspaces/giub_meteo_impacts/ci01/CESM2/flat_wind\")\n",
    "paths = list(basepath.iterdir())\n",
    "paths = [path for path in paths if path.suffix == \".nc\" and path.name != \"ds.nc\"]\n",
    "parts = [path.name.split(\".\")[0].split(\"-\") for path in paths]\n",
    "parts = np.asarray(parts)\n",
    "sorted_order = np.argsort([memb.replace(\"r10\", \"r0\") for memb in parts[:, 0]])\n",
    "parts = parts[sorted_order]\n",
    "paths = [paths[i] for i in sorted_order]\n",
    "all_members = np.unique(parts[:, 0])\n",
    "all_years = np.unique(parts[:, 1])\n",
    "\n",
    "not_here = []\n",
    "here = []\n",
    "for year in all_years:\n",
    "    for member in all_members:\n",
    "        potential_path = basepath.joinpath(f\"{member}-{year}.nc\")\n",
    "        if potential_path.is_file():\n",
    "            here.append(potential_path)\n",
    "        else:\n",
    "            not_here.append(potential_path)\n",
    "len(here)\n",
    "\n",
    "from itertools import groupby\n",
    "paths_to_load = []\n",
    "valid_ensembles = []\n",
    "for key, indices in groupby(range(len(parts)), lambda i: parts[i][0]):\n",
    "    indices = list(indices)\n",
    "    group = parts[indices]\n",
    "    these_paths = [paths[i] for i in indices]\n",
    "    years = np.asarray(list([g[1] for g in group]))\n",
    "    if len(years) == 60:\n",
    "        paths_to_load.append(these_paths)\n",
    "        valid_ensembles.append(key)\n",
    "    else:\n",
    "        print(key, len(years))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "ds = []\n",
    "for ptl in tqdm(paths_to_load):\n",
    "    ds_ = []\n",
    "    for p in ptl:\n",
    "        this = xr.open_dataset(p)\n",
    "        this = this.reset_coords(\"time_bnds\", drop=True).drop_dims(\"nbnd\")\n",
    "        ds_.append(this)\n",
    "    ds.append(xr.concat(ds_, dim=\"time\"))\n",
    "ds = xr.concat(ds, dim=\"member\")\n",
    "# ds = xr.concat([xr.concat([xr.open_dataset[ptl_] for ptl_ in ptl], dim=\"time\") for ptl in paths_to_load], dim=\"member\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import progress, Client\n",
    "from jetstream_hugo.definitions import COMPUTE_KWARGS\n",
    "client = Client(**COMPUTE_KWARGS)\n",
    "dask.persist(ds)\n",
    "progress(ds, notebook=False)\n",
    "ds = dask.compute(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds[0]\n",
    "to_comp = ds.to_zarr(f\"/storage/workspaces/giub_meteo_impacts/ci01/CESM2/flat_wind/ds.zarr\", compute=False, encoding={var: {\"chunks\": (-1, 100, -1, -1)} for var in ds.data_vars}, mode=\"w\")\n",
    "dask.persist(to_comp)\n",
    "progress(to_comp, notebook=False)\n",
    "dask.compute(to_comp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env112",
   "language": "python",
   "name": "env112"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
