{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetstream_hugo.definitions import *\n",
    "from jetstream_hugo.plots import *\n",
    "from jetstream_hugo.data import *\n",
    "# from jetstream_hugo.anyspell import *\n",
    "from jetstream_hugo.jet_finding import *\n",
    "from jetstream_hugo.clustering import *\n",
    "\n",
    "import colormaps\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# arco-era5 tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_zarr(\n",
    "    \"gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3\",\n",
    "    chunks=None,\n",
    "    storage_options=dict(token=\"anon\"),\n",
    ")\n",
    "ar_full_37_1h = ds.sel(\n",
    "    time=slice(ds.attrs[\"valid_time_start\"], ds.attrs[\"valid_time_stop\"])\n",
    ")\n",
    "\n",
    "base_ds = (\n",
    "    ar_full_37_1h[[\"u_component_of_wind\", \"v_component_of_wind\"]]\n",
    "    .sel(\n",
    "        time=ar_full_37_1h.time.dt.hour % 6 == 0,\n",
    "        latitude=ar_full_37_1h.latitude >= 0,\n",
    "        level=[175, 200, 225, 250, 300, 350],\n",
    "    )\n",
    "    .isel(longitude=slice(None, None, 2), latitude=slice(None, None, 2))\n",
    ")\n",
    "\n",
    "base_path = Path(f\"{DATADIR}/ERA5/plev/flat_wind/dailymean\")\n",
    "for year in YEARS:\n",
    "    for month in range(1, 13):\n",
    "        month_str = str(month).zfill(2)\n",
    "        opath = base_path.joinpath(f\"{year}{month_str}.nc\")\n",
    "        if opath.is_file():\n",
    "            continue\n",
    "        ds = compute(\n",
    "            base_ds.sel(\n",
    "                time=(base_ds.time.dt.year == year) & (base_ds.time.dt.month == month)\n",
    "            ),\n",
    "            progress=True,\n",
    "        )\n",
    "        ds = standardize(ds)\n",
    "        ds[\"s\"] = np.sqrt(ds[\"u\"] ** 2 + ds[\"v\"] ** 2)\n",
    "        ds = flatten_by(ds, \"s\")\n",
    "        ds.to_netcdf(opath)\n",
    "        print(f\"Completed {year}{month}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_zarr(\n",
    "    \"gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3\",\n",
    "    chunks=None,\n",
    "    storage_options=dict(token=\"anon\"),\n",
    ")\n",
    "ar_full_37_1h = ds.sel(\n",
    "    time=slice(ds.attrs[\"valid_time_start\"], ds.attrs[\"valid_time_stop\"])\n",
    ")\n",
    "\n",
    "temp_full = (\n",
    "    ar_full_37_1h[\"temperature\"]\n",
    "    .sel(\n",
    "        time=ar_full_37_1h.time.dt.hour % 6 == 0,\n",
    "        latitude=ar_full_37_1h.latitude >= 0,\n",
    "        level=[175, 200, 225, 250, 300, 350],\n",
    "    )\n",
    "    .isel(longitude=slice(None, None, 2), latitude=slice(None, None, 2))\n",
    ")\n",
    "\n",
    "temp_full = standardize(temp_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_zarr(\n",
    "    \"gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3\",\n",
    "    chunks=None,\n",
    "    storage_options=dict(token=\"anon\"),\n",
    ")\n",
    "ar_full_37_1h = ds.sel(\n",
    "    time=slice(ds.attrs[\"valid_time_start\"], ds.attrs[\"valid_time_stop\"])\n",
    ")\n",
    "\n",
    "temp_full = (\n",
    "    ar_full_37_1h[\"temperature\"]\n",
    "    .sel(\n",
    "        time=ar_full_37_1h.time.dt.hour % 6 == 0,\n",
    "        latitude=ar_full_37_1h.latitude >= 0,\n",
    "        level=[175, 200, 225, 250, 300, 350],\n",
    "    )\n",
    "    .isel(longitude=slice(None, None, 2), latitude=slice(None, None, 2))\n",
    ")\n",
    "\n",
    "temp_full = standardize(temp_full)\n",
    "\n",
    "orig_path = Path(f\"{DATADIR}/ERA5/plev/flat_wind/dailymean\")\n",
    "base_path = Path(f\"{DATADIR}/ERA5/plev/flat_wind/dailymean_2\")\n",
    "for year in tqdm(YEARS):\n",
    "    for month in trange(1, 13, leave=False):\n",
    "        month_str = str(month).zfill(2)\n",
    "        opath = base_path.joinpath(f\"{year}{month_str}.nc\")\n",
    "        if opath.is_file():\n",
    "            continue\n",
    "        ipath = orig_path.joinpath(f\"{year}{month_str}.nc\")\n",
    "        ds = xr.open_dataset(ipath)\n",
    "        this_temp = temp_full.sel(time=ds.time.values, lev=ds[\"lev\"])\n",
    "        this_temp = this_temp * (1000 / this_temp.lev) ** KAPPA\n",
    "        this_temp = this_temp.reset_coords(\"lev\", drop=True)\n",
    "        ds[\"theta\"] = compute(this_temp, progress_flag=True)\n",
    "        ds.to_netcdf(opath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new pvs das"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars_st as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_xarray(events: st.GeoDataFrame, dummy_da: xr.DataArray, varname: str):\n",
    "    dummy_da = dummy_da.rename(\"dummy\")\n",
    "    da_df = pl.from_pandas(dummy_da.to_dataframe().reset_index())\n",
    "    orig_times = da_df[\"time\"]\n",
    "    timedtype = orig_times.dtype\n",
    "    da_df = da_df.drop(\"time\", \"dummy\")\n",
    "    da_df = da_df.unique([\"lat\", \"lon\"]).sort([\"lat\", \"lon\"]).with_columns(geometry=st.from_xy(\"lon\", \"lat\"))\n",
    "    da_df = st.GeoDataFrame(da_df)\n",
    "    events = events.with_columns(pl.col(\"geometry\").st.buffer(0.25))\n",
    "    events = (\n",
    "        events.select([\"date\", \"geometry\", varname])\n",
    "        .cast({varname: pl.Float32})\n",
    "        .rename({\"date\": \"time\"})\n",
    "    )\n",
    "    events = events.cast({\"time\": timedtype})\n",
    "    events = events.filter(pl.col(\"time\").is_in(orig_times))\n",
    "    dummy_da = xr.zeros_like(dummy_da, dtype=np.float32)\n",
    "    events = events.st.sjoin(da_df, on=\"geometry\", how=\"inner\", predicate=\"contains\")\n",
    "    events = events.unique([\"time\", \"lon\", \"lat\"])\n",
    "    events_da = xr.DataArray.from_series(\n",
    "        events[[\"time\", varname, \"lat\", \"lon\"]]\n",
    "        .to_pandas()\n",
    "        .set_index([\"time\", \"lat\", \"lon\"])[varname]\n",
    "    ).fillna(0)\n",
    "    dummy_da.loc[\n",
    "        {\n",
    "            \"time\": events_da.time.values,\n",
    "            \"lat\": events_da.lat.values,\n",
    "            \"lon\": events_da.lon.values,\n",
    "        }\n",
    "    ] = events_da\n",
    "    return dummy_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events = {}\n",
    "for level in trange(310, 365, 5):\n",
    "    events = st.from_geopandas(gpd.read_parquet(f\"/storage/workspaces/giub_meteo_impacts/ci01/ERA5/RWB_index/era5_pv_streamers_{level}K_1959-2022.parquet\"))\n",
    "\n",
    "    tropospheric = events.filter(pl.col(\"mean_var\") < pl.col(\"level\"))\n",
    "    anticyclonic = tropospheric.filter(pl.col(\"intensity\") >= pl.col(\"level\"))\n",
    "    cyclonic = tropospheric.filter(pl.col(\"intensity\") < pl.col(\"level\"))\n",
    "    \n",
    "    all_events[level] = {\"anti\": anticyclonic, \"cycl\": cyclonic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in YEARS:\n",
    "    # for month in range(1, 13):\n",
    "    ofile_anti = Path(f\"{DATADIR}/ERA5/thetalev/apvs/6H/{year}.nc\")\n",
    "    ofile_cycl = Path(f\"{DATADIR}/ERA5/thetalev/cpvs/6H/{year}.nc\")\n",
    "    if ofile_cycl.is_file() and ofile_anti.is_file():\n",
    "        continue\n",
    "    time_mask = (TIMERANGE.year == year)# & (TIMERANGE.month == month)\n",
    "    coords = {\n",
    "        \"time\": TIMERANGE[time_mask],\n",
    "        \"lat\": np.arange(0, 90.5, .5),\n",
    "        \"lon\": np.arange(-180, 180, .5),\n",
    "    }\n",
    "    shape = [len(co) for co in coords.values()]\n",
    "    dummy_da = xr.DataArray(np.zeros(shape), coords=coords)\n",
    "    anti_all_levs = {}\n",
    "    cycl_all_levs = {}\n",
    "    for lev, events in tqdm(all_events.items(), total=11):\n",
    "        anti_all_levs[lev] = to_xarray(events[\"anti\"], dummy_da, \"intensity\")\n",
    "        cycl_all_levs[lev] = to_xarray(events[\"cycl\"], dummy_da, \"intensity\")\n",
    "    anti_all_levs = xr.concat(anti_all_levs.values(), dim=\"lev\").assign_coords(lev=list(anti_all_levs))\n",
    "    cycl_all_levs = xr.concat(cycl_all_levs.values(), dim=\"lev\").assign_coords(lev=list(cycl_all_levs))\n",
    "    anti_all_levs.to_netcdf(ofile_anti)\n",
    "    cycl_all_levs.to_netcdf(ofile_cycl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CESM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetstream_hugo.definitions import *\n",
    "from jetstream_hugo.data import *\n",
    "import intake\n",
    "url = 'https://raw.githubusercontent.com/NCAR/cesm2-le-aws/main/intake-catalogs/aws-cesm2-le.json'\n",
    "catalog = intake.open_esm_datastore(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable        long_name                                                            \n",
       "DIC             dissolved inorganic carbon                                               4\n",
       "DOC             dissolved organic carbon                                                 3\n",
       "FLNS            net longwave flux at surface                                             8\n",
       "FLNSC           clearsky net longwave flux at surface                                    8\n",
       "FLUT            upwelling longwave flux at top of model                                  8\n",
       "FSNO            fraction of ground covered by snow                                       8\n",
       "FSNS            net solar flux at surface                                                8\n",
       "FSNSC           clearsky net solar flux at surface                                       8\n",
       "FSNTOA          net solar flux at top of atmosphere                                      4\n",
       "H2OSNO          snow depth (liquid water)                                                8\n",
       "ICEFRAC         fraction of sfc area covered by sea-ice                                  4\n",
       "LHFLX           surface latent heat flux                                                 8\n",
       "NPP             net primary production                                                   4\n",
       "O2              dissolved oxygen                                                         4\n",
       "PD              potential density ref to surface                                         2\n",
       "PRECC           convective precipitation rate (liq + ice)                                8\n",
       "PRECL           large-scale (stable) precipitation rate (liq + ice)                      8\n",
       "PRECSC          convective snow rate (water equivalent)                                  8\n",
       "PRECSL          large-scale (stable) snow rate (water equivalent)                        8\n",
       "PS              surface pressure                                                         8\n",
       "PSL             sea level pressure                                                       8\n",
       "Q               specific humidity                                                        7\n",
       "QRUNOFF         total liquid runoff not including correction for land use change         8\n",
       "RAIN            atmospheric rain, after rain/snow repartitioning based on temperature    8\n",
       "SALT            salinity                                                                 4\n",
       "SHFLX           surface sensible heat flux                                               8\n",
       "SNOW            atmospheric snow, after rain/snow repartitioning based on temperature    8\n",
       "SOILLIQ         soil liquid water (vegetated landunits only)                             4\n",
       "SOILWATER_10CM  soil liquid water + ice in top 10cm of soil (veg landunits only)         8\n",
       "T               temperature                                                              6\n",
       "TEMP            potential temperature                                                    3\n",
       "TMQ             total (vertically integrated) precipitable water                         8\n",
       "TREFHT          reference height temperature                                             8\n",
       "TREFHTMN        minimum reference height temperature over output period                  8\n",
       "TREFHTMX        maximum reference height temperature over output period                  8\n",
       "TREFMXAV        daily maximum of average 2-m temperature                                 6\n",
       "TS              surface temperature (radiative)                                          8\n",
       "U               zonal wind                                                               7\n",
       "UES             salt flux in grid-x direction                                            4\n",
       "UET             flux of heat in grid-x direction                                         3\n",
       "UVEL            velocity in grid-x direction                                             4\n",
       "V               meridional wind                                                          3\n",
       "VNS             salt flux in grid-y direction                                            4\n",
       "VNT             flux of heat in grid-y direction                                         4\n",
       "VVEL            velocity in grid-y direction                                             2\n",
       "WTS             salt flux across top face                                                2\n",
       "WTT             heat flux across top face                                                4\n",
       "WVEL            vertical velocity                                                        3\n",
       "Z3              geopotential height (above sea level)                                    7\n",
       "aice            ice area  (aggregate)                                                    4\n",
       "aice_d          ice area  (aggregate)                                                    4\n",
       "hi              grid cell mean ice thickness                                             4\n",
       "hi_d            grid cell mean ice thickness                                             4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalog.df[[\"variable\", \"long_name\"]].groupby([\"variable\", \"long_name\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'component.experiment.frequency.forcing_variant'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:27&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/homefs/hb22g102/jetstream_hugo/src/jetstream_hugo/data.py:137: RuntimeWarning: Converting a CFTimeIndex with dates from a non-standard calendar, 'noleap', to a pandas.DatetimeIndex, which uses dates from the standard calendar.  This may lead to subtle errors in operations that depend on the length of time between dates.\n",
      "  da[\"time\"] = da.indexes[\"time\"].to_datetimeindex()\n"
     ]
    }
   ],
   "source": [
    "from jetstream_hugo.definitions import *\n",
    "from jetstream_hugo.data import *\n",
    "import intake\n",
    "url = 'https://raw.githubusercontent.com/NCAR/cesm2-le-aws/main/intake-catalogs/aws-cesm2-le.json'\n",
    "varname = \"TS\"\n",
    "varname_to_search = varname\n",
    "component = \"atm\"\n",
    "experiment = \"historical\"\n",
    "# experiment = \"ssp370\"\n",
    "frequency = \"daily\"\n",
    "forcing_variant = \"cmip6\"\n",
    "period = (1980, 2009)\n",
    "# period = (2070, 2099)\n",
    "season = None\n",
    "minlon = -180\n",
    "maxlon = 180\n",
    "minlat = 0\n",
    "maxlat = 90\n",
    "levels = list(range(13, 19))\n",
    "members = \"all\"\n",
    "reduce_da = True\n",
    "\n",
    "indexers = [component, experiment, frequency, forcing_variant]\n",
    "indexers = [np.atleast_1d(indexer) for indexer in indexers]\n",
    "indexers = [[str(idx_) for idx_ in indexer] for indexer in indexers]\n",
    "indexers = list(product(*indexers))\n",
    "ensemble_keys = [\".\".join(indexer) for indexer in indexers]\n",
    "\n",
    "basepath = Path(f\"{DATADIR}/CESM2/{varname}\")\n",
    "catalog = intake.open_esm_datastore(url)\n",
    "catalog_subset = catalog.search(\n",
    "    variable=varname_to_search,\n",
    "    component=component,\n",
    "    experiment=experiment,\n",
    "    frequency=frequency,\n",
    "    forcing_variant=forcing_variant,\n",
    ")\n",
    "dsets = catalog_subset.to_dataset_dict(\n",
    "    xarray_open_kwargs={\"consolidated\": False},\n",
    "    storage_options={\"anon\": True}\n",
    ")\n",
    "ds = dsets[ensemble_keys[0]]\n",
    "\n",
    "ds = standardize(ds)\n",
    "\n",
    "da = extract(\n",
    "    ds,\n",
    "    period=period,\n",
    "    season=season,\n",
    "    minlon=minlon,\n",
    "    maxlon=maxlon,\n",
    "    minlat=minlat,\n",
    "    maxlat=maxlat,\n",
    "    # levels=levels,\n",
    "    # members=members,\n",
    ")[varname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 11m 31s\n"
     ]
    }
   ],
   "source": [
    "da = compute(da, progress_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x7f1353fb0540>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da.to_zarr(\"/storage/workspaces/giub_meteo_impacts/ci01/CESM2/TS/past.zarr\", mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "basepath = Path(\"/storage/workspaces/giub_meteo_impacts/ci01/CESM2/flat_wind\")\n",
    "paths = list(basepath.iterdir())\n",
    "paths = [path for path in paths if path.suffix == \".nc\" and path.name != \"ds.nc\"]\n",
    "parts = [path.name.split(\".\")[0].split(\"-\") for path in paths]\n",
    "parts = np.asarray(parts)\n",
    "sorted_order = np.argsort([memb.replace(\"r10\", \"r0\") for memb in parts[:, 0]])\n",
    "parts = parts[sorted_order]\n",
    "paths = [paths[i] for i in sorted_order]\n",
    "all_members = np.unique(parts[:, 0])\n",
    "all_years = np.unique(parts[:, 1])\n",
    "\n",
    "not_here = []\n",
    "here = []\n",
    "for year in all_years:\n",
    "    for member in all_members:\n",
    "        potential_path = basepath.joinpath(f\"{member}-{year}.nc\")\n",
    "        if potential_path.is_file():\n",
    "            here.append(potential_path)\n",
    "        else:\n",
    "            not_here.append(potential_path)\n",
    "len(here)\n",
    "\n",
    "from itertools import groupby\n",
    "paths_to_load = []\n",
    "valid_ensembles = []\n",
    "for key, indices in groupby(range(len(parts)), lambda i: parts[i][0]):\n",
    "    indices = list(indices)\n",
    "    group = parts[indices]\n",
    "    these_paths = [paths[i] for i in indices]\n",
    "    years = np.asarray(list([g[1] for g in group]))\n",
    "    if len(years) == 60:\n",
    "        paths_to_load.append(these_paths)\n",
    "        valid_ensembles.append(key)\n",
    "    else:\n",
    "        print(key, len(years))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "ds = []\n",
    "for ptl in tqdm(paths_to_load):\n",
    "    ds_ = []\n",
    "    for p in ptl:\n",
    "        this = xr.open_dataset(p)\n",
    "        this = this.reset_coords(\"time_bnds\", drop=True).drop_dims(\"nbnd\")\n",
    "        ds_.append(this)\n",
    "    ds.append(xr.concat(ds_, dim=\"time\"))\n",
    "ds = xr.concat(ds, dim=\"member\")\n",
    "# ds = xr.concat([xr.concat([xr.open_dataset[ptl_] for ptl_ in ptl], dim=\"time\") for ptl in paths_to_load], dim=\"member\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import progress, Client\n",
    "from jetstream_hugo.definitions import COMPUTE_KWARGS\n",
    "client = Client(**COMPUTE_KWARGS)\n",
    "dask.persist(ds)\n",
    "progress(ds, notebook=False)\n",
    "ds = dask.compute(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds[0]\n",
    "to_comp = ds.to_zarr(f\"/storage/workspaces/giub_meteo_impacts/ci01/CESM2/flat_wind/ds.zarr\", compute=False, encoding={var: {\"chunks\": (-1, 100, -1, -1)} for var in ds.data_vars}, mode=\"w\")\n",
    "dask.persist(to_comp)\n",
    "progress(to_comp, notebook=False)\n",
    "dask.compute(to_comp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env112",
   "language": "python",
   "name": "env112"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
