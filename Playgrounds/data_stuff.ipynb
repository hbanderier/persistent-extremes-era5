{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetstream_hugo.definitions import *\n",
    "from jetstream_hugo.plots import *\n",
    "from jetstream_hugo.data import *\n",
    "# from jetstream_hugo.anyspell import *\n",
    "from jetstream_hugo.jet_finding import *\n",
    "from jetstream_hugo.clustering import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create jet relative climatologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = JetFindingExperiment(DataHandler(f\"{DATADIR}/ERA5/plev/high_wind/6H/results/1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_jet_relative_clim(exp, da):\n",
    "    all_jets_one_df = exp.find_jets()\n",
    "    jets = all_jets_one_df.with_columns(pl.col(\"time\").dt.round(\"1d\"))\n",
    "    jets = jets.with_columns(jets.group_by(\"time\", maintain_order=True).agg(pl.col(\"jet ID\").rle_id())[\"jet ID\"].explode())\n",
    "    indexer = iterate_over_year_maybe_member(jets, da)\n",
    "    to_average = []\n",
    "    for idx1, idx2 in tqdm(indexer, total=len(YEARS)):\n",
    "        jets_ = jets.filter(*idx1)\n",
    "        da_ = da.sel(**idx2)\n",
    "        try:\n",
    "            jets_with_interp = gather_normal_da_jets(jets_, da_, half_length=20)\n",
    "        except (KeyError, ValueError):\n",
    "            break\n",
    "        varname = da_.name + \"_interp\"\n",
    "        jets_with_interp = interp_jets_to_zero_one(jets_with_interp, [varname, \"is_polar\"])\n",
    "        jets_with_interp = jets_with_interp.group_by(\"time\", pl.col(\"is_polar\") > 0.5, \"norm_index\", \"n\", maintain_order=True).agg(pl.col(varname).mean() )\n",
    "        to_average.append(jets_with_interp)\n",
    "    to_average = pl.concat(to_average)\n",
    "    clim = to_average.group_by(pl.col(\"time\").dt.ordinal_day().alias(\"dayofyear\"), \"is_polar\", \"norm_index\", \"n\").agg(pl.col(varname).mean()).sort(\"dayofyear\", \"is_polar\", \"norm_index\", \"n\")\n",
    "    clim_ds = polars_to_xarray(clim, [\"dayofyear\", \"is_polar\", \"n\", \"norm_index\"])\n",
    "    clim_ds.to_netcdf(exp.path.joinpath(f\"{da.name}_relative_clim.nc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_all_smoothed_anomalies(\"ERA5\", \"surf\", \"t2m\", \"dailymean\", 'dayofyear', {'dayofyear': ('win', 15)}, None)\n",
    "compute_all_smoothed_anomalies(\"ERA5\", \"surf\", \"t2m\", \"6H\", 'dayofyear', {'dayofyear': ('win', 15)}, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [16:37<00:00, 15.59s/it]\n"
     ]
    }
   ],
   "source": [
    "da_t2m = open_da(\"ERA5\", \"surf\", \"t2m\", \"dailymean\", \"all\")\n",
    "create_jet_relative_clim(exp, da_t2m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [15:14<00:00, 14.28s/it]\n"
     ]
    }
   ],
   "source": [
    "da_tp = open_da(\"ERA5\", \"surf\", \"tp\", \"dailysum\", \"all\")\n",
    "create_jet_relative_clim(exp, da_tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [21:16<00:00, 19.94s/it]\n"
     ]
    }
   ],
   "source": [
    "da_apvs = open_da(\"ERA5\", \"thetalev\", \"apvs\", \"dailymean\", \"all\", levels=350)\n",
    "create_jet_relative_clim(exp, da_apvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [19:48<00:00, 18.56s/it]\n"
     ]
    }
   ],
   "source": [
    "da_cpvs = open_da(\"ERA5\", \"thetalev\", \"cpvs\", \"dailymean\", \"all\", levels=350)\n",
    "create_jet_relative_clim(exp, da_cpvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# arco-era5 tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_zarr(\n",
    "    \"gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3\",\n",
    "    chunks=None,\n",
    "    storage_options=dict(token=\"anon\"),\n",
    ")\n",
    "ar_full_37_1h = ds.sel(\n",
    "    time=slice(ds.attrs[\"valid_time_start\"], ds.attrs[\"valid_time_stop\"])\n",
    ")\n",
    "\n",
    "base_ds = standardize(ar_full_37_1h[\"total_precipitation\"].chunk(\"auto\"))\n",
    "base_ds = (\n",
    "    base_ds\n",
    "    .sel(\n",
    "        lat=base_ds.lat >= 0,\n",
    "        time=np.isin(base_ds.time.dt.year, YEARS)\n",
    "    )\n",
    "    .isel(lon=slice(None, None, 2), lat=slice(None, None, 2))\n",
    ")\n",
    "\n",
    "six_hourly = base_ds.resample(time=\"6h\").sum()\n",
    "daily = six_hourly.resample(time=\"1d\").sum()\n",
    "six_hourly = six_hourly * 4\n",
    "\n",
    "base_path_1 = Path(f\"{DATADIR}/ERA5/surf/tp/6H\")\n",
    "base_path_1.mkdir(exist_ok=True, parents=True)\n",
    "base_path_2 = Path(f\"{DATADIR}/ERA5/surf/tp/dailysum\")\n",
    "base_path_2.mkdir(exist_ok=True, parents=True)\n",
    "for year in YEARS:\n",
    "    opath_1 = base_path_1.joinpath(f\"{year}.nc\")\n",
    "    opath_2 = base_path_2.joinpath(f\"{year}.nc\")\n",
    "    if not opath_1.is_file():\n",
    "        six_hourly_ = compute(six_hourly.sel(time=six_hourly.time.dt.year == year), progress_flag=True)\n",
    "        six_hourly_.to_netcdf(opath_1)\n",
    "    if not opath_2.is_file():\n",
    "        daily_ = compute(daily.sel(time=daily.time.dt.year == year), progress_flag=True)\n",
    "        daily_.to_netcdf(opath_2)\n",
    "    print(f\"Completed {year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "six_hourly = base_ds.resample(time=\"6h\").sum()\n",
    "daily = six_hourly.resample(time=\"1d\").sum()\n",
    "six_hourly = six_hourly * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_zarr(\n",
    "    \"gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3\",\n",
    "    chunks=None,\n",
    "    storage_options=dict(token=\"anon\"),\n",
    ")\n",
    "ar_full_37_1h = ds.sel(\n",
    "    time=slice(ds.attrs[\"valid_time_start\"], ds.attrs[\"valid_time_stop\"])\n",
    ")\n",
    "\n",
    "temp_full = (\n",
    "    ar_full_37_1h[\"temperature\"]\n",
    "    .sel(\n",
    "        time=ar_full_37_1h.time.dt.hour % 6 == 0,\n",
    "        latitude=ar_full_37_1h.latitude >= 0,\n",
    "        level=[175, 200, 225, 250, 300, 350],\n",
    "    )\n",
    "    .isel(longitude=slice(None, None, 2), latitude=slice(None, None, 2))\n",
    ")\n",
    "\n",
    "temp_full = standardize(temp_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_zarr(\n",
    "    \"gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3\",\n",
    "    chunks=None,\n",
    "    storage_options=dict(token=\"anon\"),\n",
    ")\n",
    "ar_full_37_1h = ds.sel(\n",
    "    time=slice(ds.attrs[\"valid_time_start\"], ds.attrs[\"valid_time_stop\"])\n",
    ")\n",
    "\n",
    "temp_full = (\n",
    "    ar_full_37_1h[\"temperature\"]\n",
    "    .sel(\n",
    "        time=ar_full_37_1h.time.dt.hour % 6 == 0,\n",
    "        latitude=ar_full_37_1h.latitude >= 0,\n",
    "        level=[175, 200, 225, 250, 300, 350],\n",
    "    )\n",
    "    .isel(longitude=slice(None, None, 2), latitude=slice(None, None, 2))\n",
    ")\n",
    "\n",
    "temp_full = standardize(temp_full)\n",
    "\n",
    "orig_path = Path(f\"{DATADIR}/ERA5/plev/flat_wind/dailymean\")\n",
    "base_path = Path(f\"{DATADIR}/ERA5/plev/flat_wind/dailymean_2\")\n",
    "for year in tqdm(YEARS):\n",
    "    for month in trange(1, 13, leave=False):\n",
    "        month_str = str(month).zfill(2)\n",
    "        opath = base_path.joinpath(f\"{year}{month_str}.nc\")\n",
    "        if opath.is_file():\n",
    "            continue\n",
    "        ipath = orig_path.joinpath(f\"{year}{month_str}.nc\")\n",
    "        ds = xr.open_dataset(ipath)\n",
    "        this_temp = temp_full.sel(time=ds.time.values, lev=ds[\"lev\"])\n",
    "        this_temp = this_temp * (1000 / this_temp.lev) ** KAPPA\n",
    "        this_temp = this_temp.reset_coords(\"lev\", drop=True)\n",
    "        ds[\"theta\"] = compute(this_temp, progress_flag=True)\n",
    "        ds.to_netcdf(opath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new pvs das"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CESM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### new download with urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from itertools import pairwise\n",
    "from pathlib import Path\n",
    "from jetstream_hugo.definitions import DATADIR, compute\n",
    "from jetstream_hugo.data import standardize\n",
    "import numpy as np \n",
    "import xarray as xr\n",
    "\n",
    "experiment_dict = {\n",
    "    \"past\": \"BHISTcmip6\",\n",
    "    \"future\": \"BSSP370cmip6\",\n",
    "}\n",
    "yearbounds = {\n",
    "    \"past\": np.arange(1960, 2021, 10),\n",
    "    \"future\": np.arange(2045, 2106, 10),\n",
    "}\n",
    "yearbounds[\"past\"][-1] = yearbounds[\"past\"][-1] - 5\n",
    "yearbounds[\"future\"][-1] = yearbounds[\"future\"][-1] - 4\n",
    "timebounds = {key: [f\"{year1}0101-{year2 - 1}1231\" for year1, year2 in pairwise(val)] for key, val in yearbounds.items()}\n",
    "\n",
    "members = [f\"{year}.{str(number).zfill(3)}\" for year, number in zip(range(1001, 1201, 20), range(1, 11))]\n",
    "for startyear in [1231, 1251, 1281, 1301]:\n",
    "    members.extend(f\"{startyear}.{str(number).zfill(3)}\" for number in range(1, 11))\n",
    "    \n",
    "members2 = [f\"r{number}i{year}p1f1\" for year, number in zip(range(1001, 1201, 20), range(1, 11))]\n",
    "for startyear in [1231, 1251, 1281, 1301]:\n",
    "    members2.extend(f\"r{number}i{startyear}p1f1\" for number in range(1, 11))\n",
    "    \n",
    "season = None\n",
    "minlon = -180\n",
    "maxlon = 180\n",
    "minlat = 0\n",
    "maxlat = 90\n",
    "    \n",
    "    \n",
    "def get_url(varname: str, period: str, member: str, timebounds: str):\n",
    "    experiment = experiment_dict[period]\n",
    "    h = 6 if varname in [\"U\", \"V\", \"T\"] else 1\n",
    "\n",
    "    return fr\"https://tds.ucar.edu/thredds/fileServer/datazone/campaign/cgd/cesm/CESM2-LE/atm/proc/tseries/day_1/{varname}/b.e21.{experiment}.f09_g17.LE2-{member}.cam.h{h}.{varname}.{timebounds}.nc?api-token=ayhBFVYTOtGi2LM2cHDn6DjFCoKeCAqt69z8Ezt4#mode=bytes\"\n",
    "\n",
    "basepath = Path(f\"{DATADIR}/CESM2/high_wind/ssp370\")\n",
    "var = \"T\"\n",
    "period = \"future\"\n",
    "for member1, member2 in zip(members, members2):\n",
    "    opath = basepath.joinpath(f\"{member2}.nc\")\n",
    "    if opath.is_file():\n",
    "        print(member1, \"already there\")\n",
    "        continue\n",
    "    da = []\n",
    "    for tb in timebounds[\"future\"]:\n",
    "        da.append(\n",
    "            standardize(xr.open_dataset(\n",
    "                get_url(var, period, member1,tb),            \n",
    "                engine=\"h5netcdf\"\n",
    "            )[var])\n",
    "        )\n",
    "    da = xr.concat(da, \"time\")\n",
    "    ds = xr.open_mfdataset(basepath.glob(f\"{member2}-????.nc\"))\n",
    "\n",
    "    for coord in [\"lon\", \"lat\", \"lev\"]:\n",
    "        da[coord] = da[coord].astype(np.float32)\n",
    "\n",
    "    da[\"time\"] = da.indexes[\"time\"].to_datetimeindex(time_unit=\"us\") + datetime.timedelta(hours=12)\n",
    "    ds[\"time\"] = ds.indexes[\"time\"].to_datetimeindex(time_unit=\"us\")\n",
    "    da_ = da.sel(time=ds.time.values, lon=ds.lon.values, lat=ds.lat.values)\n",
    "    da_ = compute(da_.sel(lev=ds[\"lev\"]), progress_flag=True)\n",
    "    da_.to_netcdf(opath)\n",
    "    print(member1, \"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## newnew merger script: download then postprocess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/local/17548418'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"TMPDIR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "from itertools import pairwise\n",
    "from pathlib import Path\n",
    "from jetstream_hugo.definitions import DATADIR, compute\n",
    "from jetstream_hugo.data import standardize\n",
    "import numpy as np \n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "experiment_dict = {\n",
    "    \"past\": \"BHISTcmip6\",\n",
    "    \"future\": \"BSSP370cmip6\",\n",
    "}\n",
    "yearbounds = {\n",
    "    \"past\": np.arange(1960, 2021, 10),\n",
    "    \"future\": np.arange(2045, 2106, 10),\n",
    "}\n",
    "yearbounds[\"past\"][-1] = yearbounds[\"past\"][-1] - 5\n",
    "yearbounds[\"future\"][-1] = yearbounds[\"future\"][-1] - 4\n",
    "timebounds = {key: [f\"{year1}0101-{year2 - 1}1231\" for year1, year2 in pairwise(val)] for key, val in yearbounds.items()}\n",
    "\n",
    "members = [f\"{year}.{str(number).zfill(3)}\" for year, number in zip(range(1001, 1201, 20), range(1, 11))]\n",
    "for startyear in [1231, 1251, 1281, 1301]:\n",
    "    members.extend(f\"{startyear}.{str(number).zfill(3)}\" for number in range(1, 11))\n",
    "    \n",
    "members2 = [f\"r{number}i{year}p1f1\" for year, number in zip(range(1001, 1201, 20), range(1, 11))]\n",
    "for startyear in [1231, 1251, 1281, 1301]:\n",
    "    members2.extend(f\"r{number}i{startyear}p1f1\" for number in range(1, 11))\n",
    "    \n",
    "season = None\n",
    "minlon = -180\n",
    "maxlon = 180\n",
    "minlat = 0\n",
    "maxlat = 90\n",
    "    \n",
    "    \n",
    "def get_url(varname: str, period: str, member: str, timebounds: str):\n",
    "    experiment = experiment_dict[period]\n",
    "    h = 6 if varname in [\"U\", \"V\", \"T\"] else 1\n",
    "\n",
    "    return fr\"https://tds.ucar.edu/thredds/fileServer/datazone/campaign/cgd/cesm/CESM2-LE/atm/proc/tseries/day_1/{varname}/b.e21.{experiment}.f09_g17.LE2-{member}.cam.h{h}.{varname}.{timebounds}.nc?api-token=ayhBFVYTOtGi2LM2cHDn6DjFCoKeCAqt69z8Ezt4#mode=bytes\"\n",
    "\n",
    "\n",
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "        \n",
    "        \n",
    "def download_url(url, output_path):\n",
    "    with DownloadProgressBar(unit='B', unit_scale=True, miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
    "\n",
    "\n",
    "basepath = Path(f\"{DATADIR}/CESM2/high_wind/ssp370\")\n",
    "scratchdir = Path(os.environ[\"TMPDIR\"], \"tmp_T_cesm_downloads_hbanderi\")\n",
    "scratchdir.mkdir(exist_ok=True)\n",
    "var = \"T\"\n",
    "period = \"future\"\n",
    "for member1, member2 in zip(members, members2):\n",
    "    opath = basepath.joinpath(f\"{member2}.nc\")\n",
    "    if opath.is_file():\n",
    "        print(member1, \"already there\")\n",
    "        continue\n",
    "    scratchpaths = []\n",
    "    for tb in timebounds[\"future\"]:\n",
    "        url = get_url(var, period, member1,tb)\n",
    "        scratchpath = scratchdir.joinpath(url.split(\"/\")[-1].split(\"?\")[0])\n",
    "        scratchpaths.append(scratchpath)\n",
    "        if scratchpath.is_file():\n",
    "            continue\n",
    "        download_url(url, scratchpath)\n",
    "    da = []\n",
    "    for scratchpath in scratchpaths:\n",
    "        da.append(\n",
    "            standardize(xr.open_dataset(\n",
    "                scratchpath,            \n",
    "                engine=\"h5netcdf\"\n",
    "            )[var])\n",
    "        )\n",
    "    da = xr.concat(da, \"time\")\n",
    "    ds = xr.open_mfdataset(basepath.glob(f\"{member2}-????.nc\"))\n",
    "\n",
    "    for coord in [\"lon\", \"lat\", \"lev\"]:\n",
    "        da[coord] = da[coord].astype(np.float32)\n",
    "\n",
    "    da[\"time\"] = da.indexes[\"time\"].to_datetimeindex(time_unit=\"us\") + datetime.timedelta(hours=12)\n",
    "    ds[\"time\"] = ds.indexes[\"time\"].to_datetimeindex(time_unit=\"us\")\n",
    "    da_ = da.sel(time=ds.time.values, lon=ds.lon.values, lat=ds.lat.values)\n",
    "    da_ = compute(da_.sel(lev=ds[\"lev\"]), progress_flag=True)\n",
    "    da_.to_netcdf(opath)\n",
    "    for scratchpath in scratchpaths:\n",
    "        os.remove(scratchpath)\n",
    "    print(member1, \"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/scratch/tmp_T_cesm_downloads/b.e21.BSSP370cmip6.f09_g17.LE2-1061.004.cam.h6.T.20450101-20541231.nc')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### new cesm zarrification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:03<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "basepath = Path(\"/storage/workspaces/giub_meteo_impacts/ci01/CESM2/high_wind/ssp370\")\n",
    "paths = list(basepath.glob(\"*.nc\"))\n",
    "names = [path.stem.split(\"-\") for path in paths]\n",
    "members = [name[0] for name in names]\n",
    "years = [name[1] for name in names]\n",
    "for i, member in enumerate(tqdm(np.unique(members))):\n",
    "    da = xr.open_mfdataset(basepath.joinpath(f\"{member}-*.nc\").as_posix())\n",
    "    kwargs = {\"mode\": \"w\"} if i == 0 else {\"mode\": \"a\", \"append_dim\": \"member\"}\n",
    "    da[\"member\"] = da[\"member\"].astype(\"<U15\")\n",
    "    da = da.expand_dims(\"member\").copy(deep=True)\n",
    "    break\n",
    "    # da.to_zarr(basepath.joinpath(\"ds.zarr\"), **kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env12",
   "language": "python",
   "name": "env12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
