{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetstream_hugo.definitions import *\n",
    "from jetstream_hugo.plots import *\n",
    "from jetstream_hugo.data import *\n",
    "# from jetstream_hugo.anyspell import *\n",
    "from jetstream_hugo.jet_finding import *\n",
    "from jetstream_hugo.clustering import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create jet relative climatologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = JetFindingExperiment(DataHandler(f\"{DATADIR}/ERA5/plev/high_wind/6H/results/1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_jet_relative_clim(exp, da):\n",
    "    all_jets_one_df = exp.find_jets()\n",
    "    jets = all_jets_one_df.with_columns(pl.col(\"time\").dt.round(\"1d\"))\n",
    "    jets = jets.with_columns(jets.group_by(\"time\", maintain_order=True).agg(pl.col(\"jet ID\").rle_id())[\"jet ID\"].explode())\n",
    "    indexer = iterate_over_year_maybe_member(jets, da)\n",
    "    to_average = []\n",
    "    for idx1, idx2 in tqdm(indexer, total=len(YEARS)):\n",
    "        jets_ = jets.filter(*idx1)\n",
    "        da_ = da.sel(**idx2)\n",
    "        try:\n",
    "            jets_with_interp = gather_normal_da_jets(jets_, da_, half_length=20)\n",
    "        except (KeyError, ValueError):\n",
    "            break\n",
    "        varname = da_.name + \"_interp\"\n",
    "        jets_with_interp = interp_jets_to_zero_one(jets_with_interp, [varname, \"is_polar\"])\n",
    "        jets_with_interp = jets_with_interp.group_by(\"time\", pl.col(\"is_polar\") > 0.5, \"norm_index\", \"n\", maintain_order=True).agg(pl.col(varname).mean() )\n",
    "        to_average.append(jets_with_interp)\n",
    "    to_average = pl.concat(to_average)\n",
    "    clim = to_average.group_by(pl.col(\"time\").dt.ordinal_day().alias(\"dayofyear\"), \"is_polar\", \"norm_index\", \"n\").agg(pl.col(varname).mean()).sort(\"dayofyear\", \"is_polar\", \"norm_index\", \"n\")\n",
    "    clim_ds = polars_to_xarray(clim, [\"dayofyear\", \"is_polar\", \"n\", \"norm_index\"])\n",
    "    clim_ds.to_netcdf(exp.path.joinpath(f\"{da.name}_relative_clim.nc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 41.57 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [01:39<00:00,  1.56s/it]\n"
     ]
    }
   ],
   "source": [
    "compute_all_smoothed_anomalies(\"ERA5\", \"thetalev\", \"apvs\", \"dailyany\", 'dayofyear', {'dayofyear': ('win', 15)}, None)\n",
    "compute_all_smoothed_anomalies(\"ERA5\", \"thetalev\", \"cpvs\", \"dailyany\", 'dayofyear', {'dayofyear': ('win', 15)}, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [18:19<00:00, 17.18s/it]\n"
     ]
    }
   ],
   "source": [
    "da_apvs = open_da(\n",
    "    \"ERA5\", \"thetalev\", \"apvs\", \"dailyany\", \"all\", None, -100, 60, 0, 90, \"all\",\n",
    ")\n",
    "da_apvs = compute(da_apvs)\n",
    "create_jet_relative_clim(exp, da_apvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [19:06<00:00, 17.91s/it]\n"
     ]
    }
   ],
   "source": [
    "da_cpvs = open_da(\n",
    "    \"ERA5\", \"thetalev\", \"cpvs\", \"dailyany\", \"all\", None, -100, 60, 0, 90, \"all\",\n",
    ")\n",
    "da_cpvs = compute(da_cpvs)\n",
    "create_jet_relative_clim(exp, da_cpvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [16:37<00:00, 15.59s/it]\n"
     ]
    }
   ],
   "source": [
    "da_t2m = open_da(\"ERA5\", \"surf\", \"t2m\", \"dailymean\", \"all\")\n",
    "create_jet_relative_clim(exp, da_t2m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [15:14<00:00, 14.28s/it]\n"
     ]
    }
   ],
   "source": [
    "da_tp = open_da(\"ERA5\", \"surf\", \"tp\", \"dailysum\", \"all\")\n",
    "create_jet_relative_clim(exp, da_tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [21:16<00:00, 19.94s/it]\n"
     ]
    }
   ],
   "source": [
    "da_apvs = open_da(\"ERA5\", \"thetalev\", \"apvs\", \"dailymean\", \"all\", levels=350)\n",
    "create_jet_relative_clim(exp, da_apvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [19:48<00:00, 18.56s/it]\n"
     ]
    }
   ],
   "source": [
    "da_cpvs = open_da(\"ERA5\", \"thetalev\", \"cpvs\", \"dailymean\", \"all\", levels=350)\n",
    "create_jet_relative_clim(exp, da_cpvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# arco-era5 tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_zarr(\n",
    "    \"gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3\",\n",
    "    chunks=None,\n",
    "    storage_options=dict(token=\"anon\"),\n",
    ")\n",
    "ar_full_37_1h = ds.sel(\n",
    "    time=slice(ds.attrs[\"valid_time_start\"], ds.attrs[\"valid_time_stop\"])\n",
    ")\n",
    "\n",
    "base_ds = standardize(ar_full_37_1h[\"total_precipitation\"].chunk(\"auto\"))\n",
    "base_ds = (\n",
    "    base_ds\n",
    "    .sel(\n",
    "        lat=base_ds.lat >= 0,\n",
    "        time=np.isin(base_ds.time.dt.year, YEARS)\n",
    "    )\n",
    "    .isel(lon=slice(None, None, 2), lat=slice(None, None, 2))\n",
    ")\n",
    "\n",
    "six_hourly = base_ds.resample(time=\"6h\").sum()\n",
    "daily = six_hourly.resample(time=\"1d\").sum()\n",
    "six_hourly = six_hourly * 4\n",
    "\n",
    "base_path_1 = Path(f\"{DATADIR}/ERA5/surf/tp/6H\")\n",
    "base_path_1.mkdir(exist_ok=True, parents=True)\n",
    "base_path_2 = Path(f\"{DATADIR}/ERA5/surf/tp/dailysum\")\n",
    "base_path_2.mkdir(exist_ok=True, parents=True)\n",
    "for year in YEARS:\n",
    "    opath_1 = base_path_1.joinpath(f\"{year}.nc\")\n",
    "    opath_2 = base_path_2.joinpath(f\"{year}.nc\")\n",
    "    if not opath_1.is_file():\n",
    "        six_hourly_ = compute(six_hourly.sel(time=six_hourly.time.dt.year == year), progress_flag=True)\n",
    "        six_hourly_.to_netcdf(opath_1)\n",
    "    if not opath_2.is_file():\n",
    "        daily_ = compute(daily.sel(time=daily.time.dt.year == year), progress_flag=True)\n",
    "        daily_.to_netcdf(opath_2)\n",
    "    print(f\"Completed {year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "six_hourly = base_ds.resample(time=\"6h\").sum()\n",
    "daily = six_hourly.resample(time=\"1d\").sum()\n",
    "six_hourly = six_hourly * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_zarr(\n",
    "    \"gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3\",\n",
    "    chunks=None,\n",
    "    storage_options=dict(token=\"anon\"),\n",
    ")\n",
    "ar_full_37_1h = ds.sel(\n",
    "    time=slice(ds.attrs[\"valid_time_start\"], ds.attrs[\"valid_time_stop\"])\n",
    ")\n",
    "\n",
    "temp_full = (\n",
    "    ar_full_37_1h[\"temperature\"]\n",
    "    .sel(\n",
    "        time=ar_full_37_1h.time.dt.hour % 6 == 0,\n",
    "        latitude=ar_full_37_1h.latitude >= 0,\n",
    "        level=[175, 200, 225, 250, 300, 350],\n",
    "    )\n",
    "    .isel(longitude=slice(None, None, 2), latitude=slice(None, None, 2))\n",
    ")\n",
    "\n",
    "temp_full = standardize(temp_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_zarr(\n",
    "    \"gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3\",\n",
    "    chunks=None,\n",
    "    storage_options=dict(token=\"anon\"),\n",
    ")\n",
    "ar_full_37_1h = ds.sel(\n",
    "    time=slice(ds.attrs[\"valid_time_start\"], ds.attrs[\"valid_time_stop\"])\n",
    ")\n",
    "\n",
    "temp_full = (\n",
    "    ar_full_37_1h[\"temperature\"]\n",
    "    .sel(\n",
    "        time=ar_full_37_1h.time.dt.hour % 6 == 0,\n",
    "        latitude=ar_full_37_1h.latitude >= 0,\n",
    "        level=[175, 200, 225, 250, 300, 350],\n",
    "    )\n",
    "    .isel(longitude=slice(None, None, 2), latitude=slice(None, None, 2))\n",
    ")\n",
    "\n",
    "temp_full = standardize(temp_full)\n",
    "\n",
    "orig_path = Path(f\"{DATADIR}/ERA5/plev/flat_wind/dailymean\")\n",
    "base_path = Path(f\"{DATADIR}/ERA5/plev/flat_wind/dailymean_2\")\n",
    "for year in tqdm(YEARS):\n",
    "    for month in trange(1, 13, leave=False):\n",
    "        month_str = str(month).zfill(2)\n",
    "        opath = base_path.joinpath(f\"{year}{month_str}.nc\")\n",
    "        if opath.is_file():\n",
    "            continue\n",
    "        ipath = orig_path.joinpath(f\"{year}{month_str}.nc\")\n",
    "        ds = xr.open_dataset(ipath)\n",
    "        this_temp = temp_full.sel(time=ds.time.values, lev=ds[\"lev\"])\n",
    "        this_temp = this_temp * (1000 / this_temp.lev) ** KAPPA\n",
    "        this_temp = this_temp.reset_coords(\"lev\", drop=True)\n",
    "        ds[\"theta\"] = compute(this_temp, progress_flag=True)\n",
    "        ds.to_netcdf(opath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new pvs das: any() over levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/64 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/jetstream_hugo/src/jetstream_hugo/definitions.py:643\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(obj, progress_flag, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 643\u001b[0m     \u001b[43mclient\u001b[49m  \u001b[38;5;66;03m# in globals # type: ignore # noqa: F821\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m      5\u001b[0m da \u001b[38;5;241m=\u001b[39m open_da(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERA5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthetalev\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapvs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m6H\u001b[39m\u001b[38;5;124m\"\u001b[39m, [year], \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint8)\u001b[38;5;241m.\u001b[39many(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlev\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mresample(time\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1D\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39many()\n\u001b[0;32m----> 6\u001b[0m da \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m to_netcdf(da, opath)\n",
      "File \u001b[0;32m~/jetstream_hugo/src/jetstream_hugo/definitions.py:650\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(obj, progress_flag, **kwargs)\u001b[0m\n\u001b[1;32m    648\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mcompute(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m    652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/miniforge3/envs/env12/lib/python3.12/site-packages/xarray/core/dataarray.py:1206\u001b[0m, in \u001b[0;36mDataArray.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Manually trigger loading of this array's data from disk or a\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;124;03mremote source into memory and return a new array.\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;124;03mdask.compute\u001b[39;00m\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1205\u001b[0m new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/env12/lib/python3.12/site-packages/xarray/core/dataarray.py:1174\u001b[0m, in \u001b[0;36mDataArray.load\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Manually trigger loading of this array's data from disk or a\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;124;03m    remote source into memory and return this array.\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_temp_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1175\u001b[0m     new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_from_temp_dataset(ds)\n\u001b[1;32m   1176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable \u001b[38;5;241m=\u001b[39m new\u001b[38;5;241m.\u001b[39m_variable\n",
      "File \u001b[0;32m~/miniforge3/envs/env12/lib/python3.12/site-packages/xarray/core/dataset.py:900\u001b[0m, in \u001b[0;36mDataset.load\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m chunkmanager \u001b[38;5;241m=\u001b[39m get_chunked_array_type(\u001b[38;5;241m*\u001b[39mlazy_data\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    899\u001b[0m \u001b[38;5;66;03m# evaluate all the chunked arrays simultaneously\u001b[39;00m\n\u001b[0;32m--> 900\u001b[0m evaluated_data: \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray[Any, Any], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mchunkmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlazy_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(lazy_data, evaluated_data, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables[k]\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m~/miniforge3/envs/env12/lib/python3.12/site-packages/xarray/namedarray/daskmanager.py:85\u001b[0m, in \u001b[0;36mDaskManager.compute\u001b[0;34m(self, *data, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute\u001b[39m(\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mdata: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m     82\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray[Any, _DType_co], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compute\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/env12/lib/python3.12/site-packages/dask/base.py:662\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    659\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 662\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/miniforge3/envs/env12/lib/python3.12/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/env12/lib/python3.12/threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for year in tqdm(YEARS):\n",
    "    opath = Path(\"/storage/workspaces/giub_meteo_impacts/ci01/ERA5/thetalev/apvs/dailyany\", f\"{year}.nc\")\n",
    "    if opath.is_file():\n",
    "        continue\n",
    "    da = open_da(\"ERA5\", \"thetalev\", \"apvs\", \"6H\", [year], None, None, None, None, None, \"all\").astype(np.int8).any(\"lev\").resample(time=\"1D\").any()\n",
    "    da = compute(da)\n",
    "    to_netcdf(da, opath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CESM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### new download with urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from itertools import pairwise\n",
    "from pathlib import Path\n",
    "from jetstream_hugo.definitions import DATADIR, compute\n",
    "from jetstream_hugo.data import standardize\n",
    "import numpy as np \n",
    "import xarray as xr\n",
    "\n",
    "experiment_dict = {\n",
    "    \"past\": \"BHISTcmip6\",\n",
    "    \"future\": \"BSSP370cmip6\",\n",
    "}\n",
    "yearbounds = {\n",
    "    \"past\": np.arange(1960, 2021, 10),\n",
    "    \"future\": np.arange(2045, 2106, 10),\n",
    "}\n",
    "yearbounds[\"past\"][-1] = yearbounds[\"past\"][-1] - 5\n",
    "yearbounds[\"future\"][-1] = yearbounds[\"future\"][-1] - 4\n",
    "timebounds = {key: [f\"{year1}0101-{year2 - 1}1231\" for year1, year2 in pairwise(val)] for key, val in yearbounds.items()}\n",
    "\n",
    "members = [f\"{year}.{str(number).zfill(3)}\" for year, number in zip(range(1001, 1201, 20), range(1, 11))]\n",
    "for startyear in [1231, 1251, 1281, 1301]:\n",
    "    members.extend(f\"{startyear}.{str(number).zfill(3)}\" for number in range(1, 11))\n",
    "    \n",
    "members2 = [f\"r{number}i{year}p1f1\" for year, number in zip(range(1001, 1201, 20), range(1, 11))]\n",
    "for startyear in [1231, 1251, 1281, 1301]:\n",
    "    members2.extend(f\"r{number}i{startyear}p1f1\" for number in range(1, 11))\n",
    "    \n",
    "season = None\n",
    "minlon = -180\n",
    "maxlon = 180\n",
    "minlat = 0\n",
    "maxlat = 90\n",
    "    \n",
    "    \n",
    "def get_url(varname: str, period: str, member: str, timebounds: str):\n",
    "    experiment = experiment_dict[period]\n",
    "    h = 6 if varname in [\"U\", \"V\", \"T\"] else 1\n",
    "\n",
    "    return fr\"https://tds.ucar.edu/thredds/fileServer/datazone/campaign/cgd/cesm/CESM2-LE/atm/proc/tseries/day_1/{varname}/b.e21.{experiment}.f09_g17.LE2-{member}.cam.h{h}.{varname}.{timebounds}.nc?api-token=ayhBFVYTOtGi2LM2cHDn6DjFCoKeCAqt69z8Ezt4#mode=bytes\"\n",
    "\n",
    "basepath = Path(f\"{DATADIR}/CESM2/high_wind/ssp370\")\n",
    "var = \"T\"\n",
    "period = \"future\"\n",
    "for member1, member2 in zip(members, members2):\n",
    "    opath = basepath.joinpath(f\"{member2}.nc\")\n",
    "    if opath.is_file():\n",
    "        print(member1, \"already there\")\n",
    "        continue\n",
    "    da = []\n",
    "    for tb in timebounds[\"future\"]:\n",
    "        da.append(\n",
    "            standardize(xr.open_dataset(\n",
    "                get_url(var, period, member1,tb),            \n",
    "                engine=\"h5netcdf\"\n",
    "            )[var])\n",
    "        )\n",
    "    da = xr.concat(da, \"time\")\n",
    "    ds = xr.open_mfdataset(basepath.glob(f\"{member2}-????.nc\"))\n",
    "\n",
    "    for coord in [\"lon\", \"lat\", \"lev\"]:\n",
    "        da[coord] = da[coord].astype(np.float32)\n",
    "\n",
    "    da[\"time\"] = da.indexes[\"time\"].to_datetimeindex(time_unit=\"us\") + datetime.timedelta(hours=12)\n",
    "    ds[\"time\"] = ds.indexes[\"time\"].to_datetimeindex(time_unit=\"us\")\n",
    "    da_ = da.sel(time=ds.time.values, lon=ds.lon.values, lat=ds.lat.values)\n",
    "    da_ = compute(da_.sel(lev=ds[\"lev\"]), progress_flag=True)\n",
    "    da_.to_netcdf(opath)\n",
    "    print(member1, \"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## newnew merger script: download then postprocess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/local/17548418'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"TMPDIR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "from itertools import pairwise\n",
    "from pathlib import Path\n",
    "from jetstream_hugo.definitions import DATADIR, compute\n",
    "from jetstream_hugo.data import standardize\n",
    "import numpy as np \n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "experiment_dict = {\n",
    "    \"past\": \"BHISTcmip6\",\n",
    "    \"future\": \"BSSP370cmip6\",\n",
    "}\n",
    "yearbounds = {\n",
    "    \"past\": np.arange(1960, 2021, 10),\n",
    "    \"future\": np.arange(2045, 2106, 10),\n",
    "}\n",
    "yearbounds[\"past\"][-1] = yearbounds[\"past\"][-1] - 5\n",
    "yearbounds[\"future\"][-1] = yearbounds[\"future\"][-1] - 4\n",
    "timebounds = {key: [f\"{year1}0101-{year2 - 1}1231\" for year1, year2 in pairwise(val)] for key, val in yearbounds.items()}\n",
    "\n",
    "members = [f\"{year}.{str(number).zfill(3)}\" for year, number in zip(range(1001, 1201, 20), range(1, 11))]\n",
    "for startyear in [1231, 1251, 1281, 1301]:\n",
    "    members.extend(f\"{startyear}.{str(number).zfill(3)}\" for number in range(1, 11))\n",
    "    \n",
    "members2 = [f\"r{number}i{year}p1f1\" for year, number in zip(range(1001, 1201, 20), range(1, 11))]\n",
    "for startyear in [1231, 1251, 1281, 1301]:\n",
    "    members2.extend(f\"r{number}i{startyear}p1f1\" for number in range(1, 11))\n",
    "    \n",
    "season = None\n",
    "minlon = -180\n",
    "maxlon = 180\n",
    "minlat = 0\n",
    "maxlat = 90\n",
    "    \n",
    "    \n",
    "def get_url(varname: str, period: str, member: str, timebounds: str):\n",
    "    experiment = experiment_dict[period]\n",
    "    h = 6 if varname in [\"U\", \"V\", \"T\"] else 1\n",
    "\n",
    "    return fr\"https://tds.ucar.edu/thredds/fileServer/datazone/campaign/cgd/cesm/CESM2-LE/atm/proc/tseries/day_1/{varname}/b.e21.{experiment}.f09_g17.LE2-{member}.cam.h{h}.{varname}.{timebounds}.nc?api-token=ayhBFVYTOtGi2LM2cHDn6DjFCoKeCAqt69z8Ezt4#mode=bytes\"\n",
    "\n",
    "\n",
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "        \n",
    "        \n",
    "def download_url(url, output_path):\n",
    "    with DownloadProgressBar(unit='B', unit_scale=True, miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
    "\n",
    "\n",
    "basepath = Path(f\"{DATADIR}/CESM2/high_wind/ssp370\")\n",
    "scratchdir = Path(os.environ[\"TMPDIR\"], \"tmp_T_cesm_downloads_hbanderi\")\n",
    "scratchdir.mkdir(exist_ok=True)\n",
    "var = \"T\"\n",
    "period = \"future\"\n",
    "for member1, member2 in zip(members, members2):\n",
    "    opath = basepath.joinpath(f\"{member2}.nc\")\n",
    "    if opath.is_file():\n",
    "        print(member1, \"already there\")\n",
    "        continue\n",
    "    scratchpaths = []\n",
    "    for tb in timebounds[\"future\"]:\n",
    "        url = get_url(var, period, member1,tb)\n",
    "        scratchpath = scratchdir.joinpath(url.split(\"/\")[-1].split(\"?\")[0])\n",
    "        scratchpaths.append(scratchpath)\n",
    "        if scratchpath.is_file():\n",
    "            continue\n",
    "        download_url(url, scratchpath)\n",
    "    da = []\n",
    "    for scratchpath in scratchpaths:\n",
    "        da.append(\n",
    "            standardize(xr.open_dataset(\n",
    "                scratchpath,            \n",
    "                engine=\"h5netcdf\"\n",
    "            )[var])\n",
    "        )\n",
    "    da = xr.concat(da, \"time\")\n",
    "    ds = xr.open_mfdataset(basepath.glob(f\"{member2}-????.nc\"))\n",
    "\n",
    "    for coord in [\"lon\", \"lat\", \"lev\"]:\n",
    "        da[coord] = da[coord].astype(np.float32)\n",
    "\n",
    "    da[\"time\"] = da.indexes[\"time\"].to_datetimeindex(time_unit=\"us\") + datetime.timedelta(hours=12)\n",
    "    ds[\"time\"] = ds.indexes[\"time\"].to_datetimeindex(time_unit=\"us\")\n",
    "    da_ = da.sel(time=ds.time.values, lon=ds.lon.values, lat=ds.lat.values)\n",
    "    da_ = compute(da_.sel(lev=ds[\"lev\"]), progress_flag=True)\n",
    "    da_.to_netcdf(opath)\n",
    "    for scratchpath in scratchpaths:\n",
    "        os.remove(scratchpath)\n",
    "    print(member1, \"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/scratch/tmp_T_cesm_downloads/b.e21.BSSP370cmip6.f09_g17.LE2-1061.004.cam.h6.T.20450101-20541231.nc')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### new cesm zarrification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:03<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "basepath = Path(\"/storage/workspaces/giub_meteo_impacts/ci01/CESM2/high_wind/ssp370\")\n",
    "paths = list(basepath.glob(\"*.nc\"))\n",
    "names = [path.stem.split(\"-\") for path in paths]\n",
    "members = [name[0] for name in names]\n",
    "years = [name[1] for name in names]\n",
    "for i, member in enumerate(tqdm(np.unique(members))):\n",
    "    da = xr.open_mfdataset(basepath.joinpath(f\"{member}-*.nc\").as_posix())\n",
    "    kwargs = {\"mode\": \"w\"} if i == 0 else {\"mode\": \"a\", \"append_dim\": \"member\"}\n",
    "    da[\"member\"] = da[\"member\"].astype(\"<U15\")\n",
    "    da = da.expand_dims(\"member\").copy(deep=True)\n",
    "    break\n",
    "    # da.to_zarr(basepath.joinpath(\"ds.zarr\"), **kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env12",
   "language": "python",
   "name": "env12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
